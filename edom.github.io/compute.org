#+TITLE: Computation
#+DATE: 2017-06-29 22:40 +0700
#+PERMALINK: /compute.html
#+MATHJAX: yes
* What is a computation?
** How do we answer this question?
We want to define the noun "computer", the noun "computation", and the verb "compute".
How do we do that?

We understand a word by scrutinizing its /origin/ (where it comes from, how it is formed) and /usage/ (how it is used).
We analyze the English suffixes -er and -ation.
We define the nouns "computer" and "computation" in terms of the verb "compute" and those suffixes.
Thus, the real problem is defining "compute", "-er", and "-ation".

We analyze the subject and object of the verb "compute".

We analyze what and how analog and digital computers compute.

We find out what is common among all computers.
We look at some history of computers.

We craft a definition of "compute" that satisfies all those computers.
** The meaning of the English suffix -er
"Computer" comes from "compute" and "-er".
Thus, a computer is a thing that computes.

Appending -er to a verb V forms the noun Ver which means "one who does V",
which we often stretch to mean "a thing that does V",
so that a Ver does not have to be a person.

The suffix -er works with both both [[https://en.wikipedia.org/wiki/List_of_Germanic_and_Latinate_equivalents_in_English][Germanic and Latinate]] verbs.
For example, a (Germanic) reckoner is one who reckons, and a (Latinate) computer is one who computes.

The suffix -er works with [[https://en.wikipedia.org/wiki/Neologism][neologisms]].
Once we accept that V is a verb, we readily accept that Ver is a noun that means one who does V.
For example, after we have accepted that "google" is a verb, we readily accept that a "googler" is one who "googles".
Conversely, once we have accepted that Ver is a noun, we readily accept that V is a verb that is done by a Ver.
For example, "burgle" is backformed from "burglar".[fn::https://en.wiktionary.org/wiki/burgle]

The suffix -er sometimes works as a backforming "anti-suffix": we can sometimes form a verb V from a noun Ver by /removing/ the -er.
For example, it is easy to imagine backforming "cadave" from "cadaver",
and it is not hard to imagine that "to cadave" means "to be a dead body" or "to behave like a dead body".

Indeed the suffix -er seems to works with all verbs except modals (may, might, can, could, shall, should).
Hence we say that the suffix -er is [[https://en.wikipedia.org/wiki/Productivity_(linguistics)][productive]].
** The meaning of the English suffix -ation
"Computation" comes from "compute" and "-ation".

The suffix -ation comes from -ate + -ion.[fn::https://www.etymonline.com/word/-ation]

The sentence S V O translates to the logic formula V(S,O).
Example: "John eats bread" translates to "eat(John,bread)", or, "john(x), bread(y), eat(x,y)".

S Vates O = S does a Vation to O.
Examples.
He is renovating (renewing) his house = he is doing a renovation to his house.
He is manducating the food = he is doing a manducation to the food.
He is computing the number = he is doing a computation to the number?
How can we do something to an abstract thing with no material existence?

Appending -ion to a Latinate verb Vate forms the noun Vation.

Germanic verbs take -ing instead of -ion.
Example: Germanic "eat" and Latinate "manducate"[fn::https://en.wiktionary.org/wiki/manducate],
and Germanic "eating" and Latinate "manducation"[fn::https://en.wiktionary.org/wiki/manducation].
Thus, in this case, Germanic -ing is Latinate -ion.

I think generally Latin -re → English -te → -tion.

This is not how those words were actually historically imported, but we can think of these words this way.

manducare → manducate[fn::https://en.wiktionary.org/wiki/manducate] → manducation

computare → computate[fn::https://en.oxforddictionaries.com/definition/computate][fn::https://en.wiktionary.org/wiki/computate] → computation

renovare → renovate[fn::https://en.wiktionary.org/wiki/renovate] → renovation

It seems that the historical path was longer:
Latin infinitive
→ first-person singular present subjective
→ past participle
→ nominalization -ionem / -ionis
→ English drops -em / -is
→ backform -ation to -ate.

Latin gestare → gesto → gestatio → English gestation → gestate

Letin renovare → renovo → renovation → English renovation → renovate

Latin computare → computo → computatio → English computation → computate

formare → formo → formatio → formation

Computation is a process, not a result.
This is supported by the usage "this computation does not terminate",
the usage "this computation produces 123",
and the non-usage "this computation is 123".

What is a process?
An action that takes some time?
What is the difference between an action and a process?

We don't say "the computation is 123"; thus computation is not a result.

English is a mess of [[https://en.wikipedia.org/wiki/Doublet_(linguistics)][doublets]].

Vation is the action of Vating, process of Vating, or the result of Vating?
Which one?

Etymonline entry of -ion: "word-forming element attached to verbs, making nouns of state, condition, or action,
from French -ion or directly from Latin -ionem (nominative -io, genitive -ionis),
common suffix forming abstract nouns from verbs."[fn::https://www.etymonline.com/word/-ion].
** What and how do analog and digital computers compute?
Let us compare an analog adder and a digital adder.

An analog inverting adder is modeled as a network of operational amplifiers and resistors.[fn::https://en.wikipedia.org/wiki/Operational_amplifier_applications#Summing_amplifier]
The output is inverted for practical engineering reasons[fn::https://electronics.stackexchange.com/questions/268547/inverting-summing-amplifier-vs-non-inverting-summing-amplfier],
but it is simple to chain an inverter to the adder's output.
A number is represented as a voltage.
The operation of the two-input inverting adder is modeled as:
\[
v_3 = - r_f \cdot \left( g_1 \cdot v_1 + g_2 \cdot v_2 \right)
\]

A digital adder is modeled as a network of logic gates.[fn::https://en.wikipedia.org/wiki/Adder_(electronics)]
A number is represented as a bit string.
The operation of a ripple adder is modeled as:
\begin{align*}
s_k &= (a_k \oplus b_k) \oplus c_{k-1}
\\ c_k &= a_k \wedge b_k \wedge c_{k-1}
\\ c_{-1} &= 0
\end{align*}

An analog computer is limited by noise.
A digital computer is limited by the number of bits.
** What is common to all computers?
In 1640, a /computer/ is a human calculator.[fn:eocomputer:https://www.etymonline.com/word/computer]
In 1897, a computer is a mechanical calculator.[fn:eocomputer]
In 1945, a computer is an electronic calculator.[fn:eocomputer]
All those computers ran approximation algorithms to generate look-up tables of values of transcendental functions.
There are also /analog computers/ made with operational amplifiers[fn::https://en.wikipedia.org/wiki/Operational_amplifier],
as opposed to /digital computers/ made with logic gates[fn::https://en.wikipedia.org/wiki/Logic_gate].
Both analog and digital computers are made with transistors,
but analog computers operate the transistors outside the saturated region,
whereas digital computers operate the transistors in the saturated region.
Analog to digital is knob to switch, that is, continuous to discrete.
Analog computers use transistors as amplifiers.
Digital computers use transistors as switches.
However, as we build stronger computers, we begin trying to simulate reality,
and we wonder whether the Universe is just an extremely powerful computer.
Some time in the 1970s, a computer is a desktop computer,
calculation gained a numerical connotation,
and a calculator is a computer specialized for numerical problems,
and thus calculation is numerical computation,
and a human calculator is a human who can mentally manipulate digits quickly and correctly.
The world progressed explosively,
despite being built on increasingly complex computer systems with ever-more undefined behaviors,
occasionally killing people.
However, modernization does not change the nature of computation:
finite, discrete, sequential, precise, algorithmic.
** What do adjectives tell us about computations?
Reversible computation relates erasure of information, entropy, and heat.
"Reversible" implies that there is a forward direction.

Is interactive computation computation[fn::https://en.wikipedia.org/wiki/Interactive_computation]?

Must algorithms describe only terminating computations?
An operating system describes an interactive computation.
Computers do not exist in a vacuum.
Computers interact with reality.
** The subject and object of the verb "compute"
The object of "compute" is a mathematical object with no material existence.
For example, it makes sense to compute a number, but it does not make sense to compute a chair.

The subject of "compute" is a physical object with material existence.
Examples of such subjects are some machines and some animals[fn::Some animals can count, and counting is a computation; thus some animals can compute.
http://www.bbc.com/future/story/20121128-animals-that-can-count].
A counterexample is an algorithm, which describes how to compute something but does not compute what is described,
because an algorithm is a mathematical object with no material existence.
An algorithm describing how to calculate a number does not itself calculate the number,
in the same way a recipe describing how to cook an egg does not itself cook the egg.
A recipe has no material existence; what has material existence is the physical medium (such as ink and paper)
that is used to describe that recipe in the symbols we agreed upon.
** To compute is to do mathematics?
The way we use the verb "compute" implies that computation is a model of how reality does some mathematics.

What do we mean by "doing mathematics"?

Something computes iff we think it does some mathematics.
/To compute is to do some mathematics./
Mathematics is not only arithmetics, but also logic, etc.

But what about analog computers, such as an operational amplifier that "adds two real numbers", or "integrate a real function"?

An analog computer can integrate a real function.
A digital computer cannot.

Are there programmable analog computers?

What does a programmable analog computer look like?

How would analog computers have conditionals, loops, and other constructs?
** Computation is a discrete model of reality
Problem: analog computers invalidate this definition.

Our knowledge of physics is a model of reality.

Computation is a circumstantial/occasional/special discretization of the laws of physics.
We can model an electronic computer containing 100 bipolar-junction transistors as 300 numbers, each representing voltage at each terminal of each transistor.
We can also model the same computer as 100 bits.

Computation is a discrete sequential logical model of reality.

A machine simply acts according to the laws of physics, but we interpret some of such acts as a computation.
This implies that a computation is what we think a physical system does, not what the system actually does.
Thus, /a computation is a discrete sequential model of what some physical systems do/.

Computation is our way of thinking about what the machine does.
We invent the concept of computation because we are eager to patterns everywhere.
Our understanding of computation enables us to manipulate machines into doing what we want.

Computation has no material existence.
What exist materially are machines acting according to the laws of physics.

A computation is an /explanation/ of what a machine does.
If machine A computes Y from X, and machine B computes Z from Y,
then the machine built from those machines computes Z from X.

However, if objective reality exists, then the machine will still compute,
regardless of whether we exist to describe what the machine does.
** Terminating and non-terminating computations
A machine "computes \(y\) from \(x\)" iff
the machine ends with a representation of \(y\) if the machine is started with a representation of \(x\).
Alas, this definition has two big problems:
- Must a computation be /started/ by something outside the computer?
- What is /representation/?

A computation may not end.
A Turing machine may compute without terminating.[fn::https://math.stackexchange.com/questions/1561293/must-an-algorithm-terminate]
 [fn::"An example of a non-terminating Turing machine program is a program that calculates sequentially each digit of the decimal representation of pi"
 http://www.alanturing.net/turing_archive/pages/reference%20articles/what%20is%20a%20turing%20machine.html]
For example, a machine may compute 2/3 (whose binary expansion 0.10... does not terminate) by repeatedly printing 10 forever.

(In this document, I always use "may" in the epistemic sense, and never in the deontic sense[fn::https://english.stackexchange.com/questions/189974/why-do-they-say-may-not-for-things-which-people-shouldnt-do].
Thus "may not" and "does not have to" have the same meaning.)
** What is to compute a function?
A machine "computes the function \(f:D\to C\)" iff, for each \(x\in D\), the machine computes \(f(x)\) from \(x\).
But a mathematical function may be infinite, whereas a machine is finite.
We often ignore ontology and say that a machine computes the function \(f\) to mean that the machine computes an interesting /finite subfunction/ of \(f\).
No machine can manipulate /every/ number, because there is always a number that is too big to physically represent.
It is physically impossible to manipulate extremely big natural numbers.
For example, no machine truly implements the addition of every possible two natural numbers, because it is physically impossible.
We can /describe/ an extremely large number, but we can only visually imagine five to nine things.

What is a function?

We must distinguish relations and expressions.
Which of these is a function: \(\{(0,1),(1,2),\ldots\}\) or \(x \mapsto x+1\)?
Neither.
A function \(f : D \to C\) is a /triple of sets/ \((D,C,F)\) where \(F \subseteq D \times C\),
and \(f(x)=y\) means \((x,y) \in F\),
and \(\forall x \forall y ( x = y \to f(x) = f(y) )\).
See also Rapaport 2005 \cite{rapaport2005philosophy}, section 7.3.1.3 ("Interlude: functions described as machines"), page 239.
** What is a function?
That question is difficult.
Consider this similar question:
What is an elephant?
There are several possible answers:
- An elephant E is an individual whose genetic code makes it belong to a species of the Elephantidae family, such that E can mate with other members of the species and produce fertile offsprings. But this begets another question: what is a member of the Elephantidae family?
- An elephant is a big gray mammal with proboscis.
But those are what /we think/ an elephant is, not what an elephant is.
What is the essence of an elephant?
What is true of elephants regardless whether there were humans to describe elephants?

An extensional description of a function is made by showing /each/ pairing in the function.
Thus we can only extensionally describe a /finite/ function,
because we do not have the time to write down each pairing in an infinite function.
The magic ellipsis is not an extensional description.
An example of such ellipsis is the triple dots "\(\ldots\)" in \(0,1,2,\ldots\).
Such ellipsis means "and so on".

See also Rapaport 2005 \cite{rapaport2005philosophy}, section 7.3.1 ("What is a function?") and its descendants, from page 236.
** What can be computed?
A machine "computes the set \(D\)" iff, for each \(x \in D\), the machine /can/ determine the truth of \(r(x) \in R(D)\),
where \(r\) is the computation's encoding scheme, and \(R(D) = \{ r(x) ~|~ x \in D \}\).

A machine "computes the (infinite) sequence \(x\)" iff the machine computes every finite prefix of \(x\).
That means: given ever-longer time to run, the machine computes an ever-longer prefix of the sequence.
Thus, a computation does not have to end; it may run forever.
The sequence \(x\) can be identified by the function \(f : \Nat \to A\), in the way \(x_k = f(k)\).

Turing 1937 \cite{turing1937computable} defines a computable number as a number whose digits can be generated by a machine.
Thus, to compute a number is to compute the sequence of its digits, using an algorithm (a finite description).

A machine that /generates/ a sequence computes something from /nothing/.

What does an operating system compute?

Piccinini distinguishes abstract computation and concrete computation \cite{sep-computation-physicalsystems}.

Defining computation as the execution of an algorithm raises difficult issues \cite{scheutz2006computation}.

Rapaport's very thick 2005 book \cite{rapaport2005philosophy} deals with things in the layer below the layer we work at.

Does a quantum computation consist of discrete steps?

Immerman 1999 \cite{Immerman99descriptivecomplexity}, in Definition 2.4 (page 25),
defines what it means for a Turing machine to compute a query.
** Undoing chronic ontological sloppiness
First, we undo the chronic ontologically-sloppy habit of conflating a thing and a representation of the thing.
"123" is not a number, but a /representation/ of a number.
We cannot manipulate numbers physically because they do not have material existence.
We can only manipulate the physical representations of those numbers.
When we "add two numbers", we are actually manipulating the representations of those numbers in a way that corresponds to adding those numbers.
Formally, if \(e : \Nat \to \{0,1\}^*\) is an encoding scheme, then
\( e(x+y) = e(x) +_e e(y) \), where \(+\) is the operation that we think we do, and \(+_e\) is the operation that we actually do.
We think we are adding numbers, but we are actually writing symbols on paper or juggling symbols in our mind.

Then, we un-conflate a program and a machine running the program.
A program does not /compute/; it is the machine that computes.
A program cannot do anything on its own; a machine has to run it.
When we say "a program computes a function",
we actually mean that running the program on the machine causes
the machine to compute that function.

Unfortunately, the ontologically correct thing is very wordy,
so I write in conflated manner.
For example, when I write "this program adds two numbers",
what I really mean is
"running the program causes the machine to manipulate two representations in a way that corresponds to adding two numbers".
Fortunately, the only time we have to care about this ontological issue is when we are talking about the foundations of computation.
* What is an algorithm?
** What is an algorithm?
In 1690, an /algorithm/ is an Arabic system of computation.[fn::https://www.etymonline.com/word/algorithm]
It is the historically-and-interculturally mangled name of Muhammad ibn Musa al-Khwarizmi[fn::https://en.wikipedia.org/wiki/Muhammad_ibn_Musa_al-Khwarizmi] who lived in the 8th century.
An /algorithm/ is a finite description of how a computer computes something.
In the medievals, an algorithm is a numerical approximation scheme to be run by humans.
Anyone who knows basic arithmetics can mindlessly carry out an algorithm
and produce a correct answer without any understanding of why or how the algorithm works.

An algorithm restates a function as a composition of /primitives/.

Some note about ontology:
The long addition algorithm does not describe how to add two numbers \(x\) and \(y\).
It describes how to manipulate two /representations/ \(e(x)\) and \(e(y)\) in order to produce a third representation \(e(x+y)\)
that represents the sum of \(x\) and \(y\).

An approximation scheme describes a number iff the sequence of approximations converges to the number.
The approximation may never reach the number, but it always gets closer.

An algorithm is a finite description.
Description implies language, presumably a formal language.
Language implies syntax and semantics.
Thus an algorithm is a string in a language.

There are many formal languages:
Turing, Post, primitive recursive arithmetics, lambda calculus, ML-family languages, computation models[fn::https://en.wikipedia.org/wiki/Model_of_computation], etc.
There are lots of computation models, each capturing different aspect, but most are equivalently powerful.

The language should have a sensible cost model so that we can define space complexity and time complexity.

Rapaport 2015 \cite{rapaport2005philosophy} p. 269 mentions Moschovakis's idea of algorithms as recursors.
See Vardi 2012 \cite{vardi2012algorithm}, Gurevich 2011 \cite{gurevich2011algorithm}, Moschovakis 2001 \cite{moschovakis2001algorithm}.
** Algorithm, problem
An algorithm describes /how/ to compute something.

A problem describes /what/ to compute.

See also Rapaport 2005 \cite{rapaport2005philosophy} page 242, about the difference between formulas and algorithms.
** Algorithm, machine, implementation, and computation
If algorithm A describes how to compute C, and machine M implements algorithm A,
then machine M computes C.

Are there undescribable computations?
* Digressions
** Genus-differentia definition of computation?
A computation is (what) that (what)?

Process? Activity? Mechanism?

A program describes the computation performed by a machine.
A program modulates the machine.
Manipulates computational resources to compute something.
** Computation as information transformation
Computation is answering a question.

What is the relationship between computation and answering questions?

A computer reduces information?
Transforms information?

Computation is transformation of information?
** Computation as model/concretion?
Computation is running a program on a machine.

It seems that the defining feature of computation is conditional and repetition.

Program is a model.
** Diving into philosophy of computation
Ian Horswill wrote an introductory article "What is computation?"[fn::http://www.cs.northwestern.edu/~ian/What%20is%20computation.pdf].
** Machine
A /machine/ is a tool that /computes/ what the machine is designed for.
A machine has material existence.
It is a physical implement.

Digression:
In [[file:philo.html]], I write that a machine is a tool, that is something that we use to extend our self (what we control).
** Even more historical?
Leibniz used the term "calculation"?
Turing used "effective calculability" to mean "algorithmic"?
Computation is calculation? It's just following rules?
** Analog vs digital
What does digital do better than analog?
Temperature affects analog computers more than it affects digital computers.
Digital signals are more immune to noises.
Digital computers have a wider operating temperature range.

What does analog do better than digital?
Analog computers degrade gracefully: computation gradually gets more and more wrong as the computer goes out of its designed operating conditions.
Digital computers degrade abruptly: computation suddenly gets chaotic as the computer approaches a limit of its designed operating conditions.
** Machine, automaton, robot
In 1540, a /machine/ is any structure or device.[fn:eomachine:https://www.etymonline.com/word/machine]
The word "machine" may have come from a Proto-Indo-European word that means "that which enables".[fn:eomachine]
Some machines are /programmable/.
Such machine implements several functions that can be chosen by a /program/ which is a part of the machine's input.
The program chooses which function the machine shall compute.

In 1610, an /automaton/ is a self-acting machine.[fn::https://www.etymonline.com/word/automaton]
Thus an automaton has an energy source or is connected to an energy source that enables the automaton to run with minimal human intervention.

In 1923, the English word "robot" came from the Czech word "robotnik" that means "forced worker".[fn::https://www.etymonline.com/word/robot]
** The implicit agency of -er, and how it might have impelled us to invent God
Why is -er so productive?
Because every sentence that has verb can always have a subject.
Because it is always possible, if not necessary, for a verb to have a doer.
Because every action has an agent,
because everything happens because an agent does it,
perhaps this is a tacit fundamental assumption of our logic,
or perhaps this reflects how the Universe works?
Our language implies that the subject causes the action or outcome described by the verb?
A Ver is one who Vs, that is, one who /causes/ a Ving to be done, and thus there is an implicit agency in each Ver.
Recall that an agency is an ability to cause.

There is a problem: if we assume that, then the sentence "X exists" implies that X causes its own existence,
but it seems problematic for something to cause itself[fn::https://en.wikipedia.org/wiki/Causa_sui]?
Did we invent God because we impose, through our languages, that everything has a cause, that every verb has a doer?
We invented God because we have evolved to crave explanations for everything, because craving for explanations promote survival?
We want to explain everything, but our finiteness precludes us from explaining everything.

It is curious that Christians call Jesus "Word (logos) of God"[fn::https://en.wikipedia.org/wiki/Logos_(Christianity)][fn::https://biblehub.com/sepd/genesis/1.htm],
and the Greek word "logos" also begets the English word "logic".

Spreading religion requires language, unless our ancestors were telepathic.

Aren't we rambling too much?
We are merely trying to define "compute", which requires us to traverse linguistics, and somehow we arrived at theology?
It is trivial to get lost in philosophy, as each question readily begets more questions.
How do we find the way out?
** What/where is God in Cantor's paradise?
If a god resides in a paradise,
and Cantor has made us a paradise[fn::https://en.wikipedia.org/wiki/Cantor%27s_paradise] (according to Hilbert),
then what/where is the god in Cantor's paradise?
** Computation and reckoning
The Germanic English of "compute" is "reckon" (German rechnen, Dutch rekenen).
Thus computation is reckoning.
* What is computation theory?
Computation theory spans philosophy, physics, and mathematics.
The mathematics part[fn::https://en.wikipedia.org/wiki/Theory_of_computation] studies logical models of computation, not computation itself.
Which part of computation theory are we interested in?
This document is mostly the mathematics part, because there is a one-million-dollar prize for solving the P vs NP problem.
See Piccinini 2017 \cite{sep-computation-physicalsystems} if you are interested in the philosophy and physics parts.

1999 Immerman \cite{Immerman99descriptivecomplexity},
2009 Arora & Barak \cite{Arora2009},
2009 Marek & Remmel \cite{Marek2009},
2002 Boolos, Burgess, & Jeffrey \cite{Boolos2002},
1987 Rogers \cite{Rogers1987}.

Where are the researchers?
There is ACM Special Interest Group on Logic and Computation (SIGLOG)[fn::https://siglog.acm.org/about/].
There is also Computational Complexity Conference[fn::http://www.computationalcomplexity.org/].

We can think of computation theory as refining these hierarchies:
automaton power hierarchy[fn::https://en.wikipedia.org/wiki/Automata_theory],
problem complexity hierarchy,
logic strength hierarchy,
Chomsky language hierarchy[fn::https://en.wikipedia.org/wiki/Chomsky_hierarchy],
arithmetical hierarchy[fn::https://en.wikipedia.org/wiki/Arithmetical_hierarchy],
formal system power hierarchy[fn::https://en.wikipedia.org/wiki/Reverse_mathematics#The_big_five_subsystems_of_second-order_arithmetic],
and so on.
They are related to each other.
We want to find out which feature gives which power.

What is the difference between descriptive complexity theory and implicit complexity theory[fn::http://www.cs.unibo.it/~martini/BISS/martini-1.pdf]?
* What is computer science?
Rapaport 2005 \cite{rapaport2005philosophy} surveys various definitions and their problems.
It summarizes the discussion in page 154 (3.15.4 Conclusion).

Computer science[fn::https://en.wikipedia.org/w/index.php?title=Computer_science&oldid=875563283#Etymology]
is not science (the application of the scientific method to make falsifiable theories).

Scott Schneider defines "computer science" as "everything to do with computation, both in the abstract and in the implementation".
 [fn::http://www.scott-a-s.com/cs-is-not-math/]
Is CS a branch of math?
 [fn::https://math.stackexchange.com/questions/649408/is-computer-science-a-branch-of-mathematics]

If science is simply a synonym of "knowledge", then computer science is a synonym of "computer knowledge".
* The mathematics part
There are many computation models[fn::https://en.wikipedia.org/wiki/Model_of_computation].
All of them imply some /operating conditions/:
there are no electrical disruptions, fires, cosmic rays, and so on.
All of them also imply a sequence of operations.

We often assume that the computation model is a Turing machine.
But, ontologically, a Turing machine is a computation model, not a machine,
and thus should be called a Turing model.

A /computation model/ is a formal system that represents the relevant aspects of the internal states of a computing machine.

Now we define "to compute the function \(f : D \to C\)" with respect to the computation model \((D,C,S,d,c,t)\) where
\(d : D \to S\), and
\(c : C \to S\), and
\(t\) has arity \((S,S)\).
The computation model is a three-sorted structure.
The functions \(d\) and \(c\) together bridge two things:
(1) our high-level thought of the machine computes, and
(2) the logical system that abstracts the machine's internal state and computation.
Let \(S\) be the computation model's domain of discourse, that is, the set of each mathematical object that is a simplified representation of a machine internal state.
Let \(t\) be a relation symbol of arity 2.
The relation \(t\) represents the state transition relation.
Define the transitive closure of \(t\) as \(T(x,y) = (TC(t))(x,y) = t(x,y) \vee \exists z (t(x,z) \wedge T(z,y))\)
where \(TC\) is the transitive-closure operator.

Machine \(M\) computes function \(f : D \to C\) according to computation model \((D,C,S,d,c,t)\) iff
\[
compute(M,f) = \forall x : T(d(x), c(f(x)))
\]

We can focus on the computation model, and focus on the substructure \((S,t)\) instead.

A machine /computes/ the function \(f : D \to C\) according to the computation model \((S,c,d,t)\), iff,
for all \(x \in D\), it is true that \(T(d(x),c(f(x)))\), that is, the machine starts at state \(d(x)\) and finishes at state \(c(f(x))\).

A /computation model/ is a logical system that has a domain of discourse representing machine internal state,
and has an arity-2 relation symbol \(t\) representing the state transition relation.

TODO \cite{vardi1998computational}
** Encoding scheme
Now we define encoding.

An encoding is a representation of something.
A representation is not the represented, but a representation behaves in the way the represented does.
Formally, an /encoding scheme/ is a computable bijective function \(e : D \to A^*\) where \(A\) is an alphabet.
Thus, an encoding scheme is an /algorithm/ that describes a bijective function.

If "algorithm" and "encoding scheme" depend on each other,
then there is only one logical conclusion:
/Algorithm and encoding-scheme are the same thing./
** Computable, algorithm, finite description
Function $f$ is /computable/ by formal system $S$ iff $S$ has a /finite description/ of $f$.

An /algorithm/ solves a /problem/.
A problem can be solved by many algorithms with different resource usage characteristics.

An algorithm is a finite description of what a machine is supposed to do.
** Is computation inherently sequential? Computation as sequence of steps
In a Turing machine, a step is a state transition
that consists of reading the tape cell,
writing the tape cell,
moving the tape head,
and changing the internal state.
In $\lambda$-calculus,
a step is a $\beta$-reduction
of an expression composed from more primitive subexpressions.
These examples suggest that we can define computation as a /sequence/ of steps.

Each of those models is a special case of deciders.
** Logic, model
See [[file:logic.html]].
** Problem, formula, input, output, model, relation
"Problem" comes from Greek "problema" which means "a task, that which is proposed, a question".[fn::https://www.etymonline.com/word/problem]
Therefore, a problem /is/ a question, or, formally, a /logical formula/.

/A problem is a formula./
For example, the problem "Given an \(x\), what is \(x+x\)?" is the formula
\( x+x = y \) in first-order logic with equality and some arithmetics.
Note that some logic is embedded in English.[fn::English is at least second-order, as demonstrated by the Geach--Kaplan sentence "Some critics admire only one another" https://en.wikipedia.org/wiki/Nonfirstorderizability].

#+CAPTION: Some common problem shapes
| name             | shape          | input | output |
|------------------+----------------+-------+--------|
| decision problem | \( p(x) \)     | \(x\) |        |
| search problem   | \( p(x) \)     |       | \(x\)  |
| function problem | \( f(x) = y \) | \(x\) | \(y\)  |

A problem may have /inputs/ and /outputs/.
An /input/ of a problem is a free variable in the formula.
An /output/ of a problem is a free variable in the formula.

Another example: the problem "Is the sum of two even numbers even?" is the formula \( E(x) \wedge E(y) \to E(x+y) \).

What does it mean to solve a problem (answer a question)?
Solving a problem is answering a question.
Answering a question corresponds to /proving a formula/.
Answering a question corresponds to /finding a model/ of a formula?

A /problem/ may be /modeled/ by a /relation/ between questions and answers.
For example, the problem \( \forall x \exists y : x+x = y \)
is modeled by the relation \( \{ (0,0), (1,2), (2,4), \ldots \} \)
and is also modeled by the relation \( \{ (\epsilon,\epsilon), (1,11), (11,1111), \ldots \} \).

Do not conflate a problem and a model of it.
A problem is a formula, /not/ a relation.

Compare various definitions of "problem"
 [fn::https://en.wikipedia.org/wiki/Computational_complexity_theory]
 [fn::https://plato.stanford.edu/entries/computational-complexity/].

A problem is \cite{sep-computational-complexity}

Problem can be /composed/ as formulas can be composed.
** Complexity
The worst-case time complexity[fn::https://en.wikipedia.org/wiki/Worst-case_complexity]
of machine $m$ for input $x$ is $t(m,x)$,
the number of steps $m$ makes between the beginning and the halting.
The /worst-case time complexity/ of $m$ for input /size/ $n$ is
$T(m,n) = \left\vert \max_{|x| = n} t(m,x) \right\vert$.
We can also write asymptotic statements such as $T(m,n) \in O(f(n))$.

An algorithm implies a machine.

The complexity class of a problem is the worst-case time complexity of the most efficient algorithm solving that problem.

A /machine/ $M$ is a /transition relation/ $T$
(an /acyclic/ binary relation).
$$
T(x,y) = \text{\(M\) can state-transition from \(x\) to \(y\).}
$$

$M$ /computes/ $P$ iff
a subgraph of the shortcut of $T$ is isomorphic to $P$.
(If $T$ were cyclic, this definition would fail.)

Related:
[[https://en.wikipedia.org/wiki/Graph_isomorphism][graph isomorphism]],
[[https://en.wikipedia.org/wiki/Subgraph_isomorphism_problem][subgraph isomorphism problem]].

/Deterministic/ machine equals /functional/ relation.

$G$ /accepts/ $v$ iff $F^\infty(\{v\}) = \emptyset$ where $F$ is the graph's fringe function.
The /language/ recognized by $G$ is the largest $L \subseteq V$ such that $F^\infty(L) = \emptyset$.

A Turing machine is $(C,I,f)$
where $C$ is countable
and $f$ is recursive.

https://en.wikipedia.org/wiki/Register_machine

Example: a state of a Turing machine is $(c,l,h,r)$
where $c$ is a configuration,
$l$ is the tape content to the left of the head,
$h$ is the tape content at the head,
and $r$ is the tape content to the right of the head.
** Problem, reduction
Sometimes we can /reduce/ a problem into another problem?
** Digressions
*** Pullback
We can model the apparent function computed by the machine as \(g : A^* \to A^*\) where \(g(e(x)) = e(f(x))\).
We then do some algebraic manipulation:
\begin{align*}
\\ g(e(x)) &= e(f(x))
\\ (g \circ e)(x) &= (e \circ f)(x)
\\ g \circ e &\equiv e \circ f
\end{align*}

An equation of the shape \(g \circ e \equiv e \circ f\) is a special case of pullbacks[fn::https://en.wikipedia.org/wiki/Pullback_(category_theory)] in category theory.
*** Cheating
"Cheating" with an unreasonable encoding is a common error in P vs NP "proofs".
** Encoding affects complexity
Encoding a natural number \(n\) in unary notation takes \(n\) symbols.
Encoding the same number in binary notation takes approximately \(\log_2(n)\) symbols.

Adding two natural numbers \(m\) and \(n\) takes \(m+n\) steps in unary notation,
but only approximately \(\log(\max(m,n))\) steps in positional notation.

Why don't encode a number as its prime factorization,
to simplify multiplication while complicating addition?

What do we formally mean by "reasonable encoding"?

Why do we assume that numbers are encoded in positional notation[fn::https://en.wikipedia.org/wiki/Positional_notation], not unary notation[fn::https://en.wikipedia.org/wiki/Unary_numeral_system]?

My guess:
What we mean by reasonable encoding is an /order-preserving homomorphism/:
\begin{align*}
a < b &\iff e(a) <_e e(b)
\\
a = b &\iff e(a) = e(b)
\end{align*}

A homomorphism preserves structure.
But which structure?

We may encode the natural numbers as the bitwise-negation of the base-2 representation: 1, 0, 11, 10, 01, 00, etc.
** What makes an encoding reasonable?
A /reasonable encoding/ is an encoding that is easy to compute and is easy to invert.

A reasonable encoding has a finite description.
** Rant: The sad state of computational complexity texts?
It is philosophically appaling that most computational complexity texts readily show what a problem is /represented/ as,
but never clearly and /formally define/ what a problem /is/.
It is appaling that they spend hundreds of pages discussing something undefined.
