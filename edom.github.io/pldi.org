#+TITLE: On programming language design and implementation
#+DATE: 2020-01-22 00:00:00 +0700
(Not to be confused with the ACM SIGPLAN conference of the same name.)
* Designing the language / a language
- Instead of designing a new language from scratch, improve and /unify/ existing languages (dialects of Lisp);
  otherwise we will end up reimplementing the same thing.
- Only MIT/BSD/Apache2/similar/public-domain; no GPL/AGPL/LGPL; no proprietary.
- Call-by-value (applicative-order evaluation) is OK; we can always make abstract interpreters later.
- Typing more should produce more return of effort:
  types are optional, but they help the computer help the programmer.
  Programmers should declare types (or use any feature of the language)
  because they /want/ to (as in Common Lisp or TypeScript), not because they /have/ to (as in Java).
- Design a language.
  - Design a language /top-down/ (from math down to reality).
    - [[file:pl-0.html][Sketch of PL-0 programming language]]
      - [[file:group.html][On grouping things]]: How should we group functions?
      - [[file:program-text.html][On models of text in programming]]
  - Design a language /bottom-up/ (from reality up to math).
    - [[file:pl-1.html][Sketch of PL-1 programming language]]
  - Synthesize the middle way.
  - [[file:software-system-model.html][Towards a comprehensive software system modeling language]]
* Instead of functions and macros, do partial evaluation?
- Problem:
  - Macros /are/ functions.
    - Macros are functions from /AST/ to AST.
    - Macros are functions that are called at /compile-time/; ordinary functions are called at run-time.
  - My biggest beefs with macros:
    - It is unclear which arguments are evaluated.
      Example: Why does =in-package= not evaluate its argument but =use-package= does?
    - Can't pass =and= to =zip= (although we can work around this in Racket with set!-transformers).
- Solution:
  - Partial evaluation cleanly /unifies/ macros and ordinary functions?
- for all x y z: (let ((x y)) z) = ((lambda (x) z) y)
  - Staging/compilation side-effects should not affect run-time.
  - Compile-time state and run-time state should be totally separate.
* Biggest problems in Lisp
- module/namespace
- macro
- quoting
  - I think quoting should satisfy these properties for all =x=:
    - =(equal (quote x) x)=
    - =(equal (unquote (quote x)) x)=
  - Important questions?
    - What is the difference between =unquote= and =eval=?
    - Is =quote= a function? If so, what is its domain and codomain?
- multiple return values?
* Which intermediate representation/language should we target?
- https://en.wikipedia.org/wiki/Intermediate_representation
  - Which intermediate representation (IR) is free and open?
  - ECL has an intermediate language? https://en.wikipedia.org/wiki/Embeddable_Common_Lisp
- Is the "Common Intermediate Language" really common?
- Why did Microsoft make MSIL and not just reuse or fork LLVM?
  Probably it was about political control, not technical superiority.
- Is there an IR that can be translated to CPU + GPU + all weird instructions?
  How big is that IR?
* Features for programming in the large
Module/namespace/name-management/what do you call it
** <2019-11-07> Distinguishing between modules, namespaces, and compilation units
A namespace is a mapping.

A compilation unit is the smallest thing that can be compiled.

They are often conflated into a module, in the sense that a module
often serves multiple functions like separate namespace, separate compilation, and dependency management,
but perhaps it is time that we distinguish between them.

Racket units are namespaces (in my sense) and Racket namespaces are namespaces (in my sense) too.
Confusing.
* Toward a curriculum for programming language design and implementation
Alternative title: "Exploring the programming language design space"
** What is a good language?
- A good language efficiently represents meanings.
** How do we design a programming language?
- Reuse Lisp syntax (S-expressions).
- Design the /semantics/ (the meaning).
  - Consider /implementability/.
  - Consider the future; consider probability/risk of changes.
** Implementation?
- Interpretation vs compilation
- Garbage collection
  - Tracing; not moveable; simple to implement
  - Copying; moveable; performant
** What?
We can begin designing from the semantics (our desires) or from the run-time system (our constraints).
All design is a compromise between the ideal and the practical.
But if the ideal needs to be compromised, is it truly ideal?

To consider:
- [[http://tomasp.net/blog/2017/design-side-of-pl/][Petricek 2017]] (about programming language design).
- \cite{ingalls1981design}
  [fn::<2019-12-11> [[https://www.cs.virginia.edu/~evans/cs655/readings/smalltalk.html][mirror 1 (html)]]]
  [fn::<2019-12-11> [[https://cs.pomona.edu/classes/cs131/readings/ingalls.pdf][mirror 2 (pdf)]]].
- \cite{coblenz2018interdisciplinary}
  [fn::<2019-12-11> [[http://www.cs.cmu.edu/~NatProg/papers/onward18essays-p7-p-682d101-38832-final.pdf][mirror (pdf)]]].
- [[https://leastfixedpoint.com/tonyg/kcbbs/projects/thing.html][Tony's Programming Language Experiments]].
- [[http://lisp-univ-etc.blogspot.com/2012/04/lisp-hackers-pascal-costanza.html][3-Lisp's tower of interpreters]].
- [[https://en.wikipedia.org/wiki/Subject-oriented_programming][Subject-Oriented Programming]]
  - \cite{harrison1993subject}?
** When should you make a DSL (domain-specific language)?
- When you will be writing many programs in the language so that it saves you effort.
  Example:
  - Write 100 similar systems in Java: 100 * 10,000 = 1,000,000 units of effort
  - Write 100 similar systems in DSL implemented in Lisp: 10,000 (translator) + 100 * 1,000 = 110,000 units of effort
  - Compare: writing math in math notation \(x^2+1=0\) vs writing math in spelled-out English ("a number such that its square plus one is zero").
- And when you can maintain the DSL.
- What? Mernik et al. 2005 \cite{mernik2005and}[fn::<2019-12-20> http://people.cs.ksu.edu/~schmidt/505f14/Lectures/WhenDSL.pdf]
** How do we specify semantics?
** What are some recent developments?
What are some recent innovations in programming languages?

What are some recent innovations in In programming-language semantics?

???
https://www.semanticscholar.org/paper/Practical-partial-evaluation-for-high-performance-W%C3%BCrthinger-Wimmer/1d4c0211549a8fe259a273da88c63e8f00fef463
** What we are going to do here
Here we design a [[https://en.wikipedia.org/wiki/Lisp_(programming_language)][Lisp]], its semantics, and its interpreter.
(Note that, in the 21st century, Lisp means a family of languages, not a particular language.)

We design a Lisp because we want to focus on semantics;
the syntax can always be improved later.

We name our language "PL-0" (not to be confused with [[https://en.wikipedia.org/wiki/PL/0][Wirth's PL/0]]).

We want a programming language for implementing programming languages.
We want to easily specify alternative semantics.
There are many prior arts, such as Racket;
Felleisen et al. 2008 \cite{felleisen2018programmable} discusses some problems of Racket.
** Should we make a Forth or a Lisp?
The question is wrong.
The answer is: we should begin from mathematics.

We did not make a stack-based language such as
[[https://en.wikipedia.org/wiki/Forth_(programming_language)][Forth]] or PostScript
because we did not know how to implement /optional arguments/ in a stack-based language
other than by explicitly passing a possibly empty list.

So we choose to make a Lisp instead.

But, why not both?
Why not make a Lisp interpreter on Forth?

The question of Forth vs Lisp is:
How hard should we allow the user to crash without foreign procedures:
just an exit due to an unhandled exception (Lisp), or an abort due to a segmentation fault (Forth)?
** Important psychological issues
- The language must come with a /benevolent dictator/.
- The language must come with  an /official formatter/ (like =gofmt=).
  Otherwise people will waste energy bickering about code style.
  With an official formatter, we can use authority to nip such wasteful debates in the bud.
  - The official formatter does not have to be the best, but should be /good enough/
    to prevent others from wasting energy writing competing formatters.
* Marketing
- Who decides what language is used?
  - Programmers in their hobby projects?
  - Managers in their companies?
* What?
On programming by examples.
Erik Meijer has an interesting presentation about machine learning, that is, programming computers by examples.
Can we create a programming language in which programming by examples is elegantly expressible?

If it is impossible to invent better primitives,
then the only way to make programming easier is to make computers program themselves more.

What is the significance of the [[https://blog.ballerina.io/posts/ballerina-a-cloud-native-programming-language/][Ballerina programming language]][fn::<2019-11-27>]?
* On the mathematics of programming languages
** Science?
A programming language should be /derived/ from observations and principles,
in the same way physical theories are derived from observations and principles?
** Programming languages are formal systems
(Hofstadter's [[https://en.wikipedia.org/wiki/BlooP_and_FlooP][BlooP and FlooP]]?)

A programming language is a formal system.

This is a point of view:

| prog. lang. theory   | formal logic                                                   |
|----------------------+----------------------------------------------------------------|
| syntax               | formal language                                                |
| program              | formula                                                        |
| primitive value      | axiom                                                          |
| primitive combinator | inference rule                                                 |
| programming language | [[https://en.wikipedia.org/wiki/Formal_system][formal system]] |

Another point of view is the [[https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence][Curry--Howard isomorphism]]:

| prog. lang. theory   | intuitionistic logic |
|----------------------+----------------------|
| program              | constructive proof   |
| type                 | proposition          |
| programming language | logic system         |

See also [[https://ncatlab.org/nlab/show/computational+trinitarianism][Curry--Howard--Lambek isomorphism]]
(called "computational trinitarianism" by Harper 2011).

Recall that:

#+BEGIN_EXAMPLE
formal system        = formal language + interpretation
programming language = syntax          + semantics
#+END_EXAMPLE
** The essence of syntax is finite describability
A formula does not have to be a string, but may be a /graph/, for example.
A language does not have to be /textual/, and even text does not have to be one-dimensional.
Syntax does not have to be one-dimensional;
the only restriction is that the syntax is /finitely describable/.

The semantics (the meaning) of a formula is a mathematical object, such as a number, a set, a state transformer (a special case of functions).

The semantics of a processor instruction is usually a state transformer.

We can map a state transformer to a number by /forgetting/ the parts of state that are preserved by the transformer. (Compare "forgetful functors" in category theory.)

A computation is /pure/ iff its state transformer semantics can be reduced to value semantics???
* Creating languages with the "tower of semantics" approach
** Semantics of values / arithmetic expressions
Let \(P\) be the set of programs.

An example value semantics (calculator): \(S : P \to \Nat\).
\[ S(a+b) = S(a)+S(b) \]
** Semantics of errors
We add /errors/.

Let \(V = \Nat \cup \Set{\bot}\).

\[ S(n/0) = \bot \]
** Semantics of bindings
Let \(N\) be the set of /names/.

Let \(V = \Nat\) be the set of /values/.

Let \(C = N \to V\) be the set of /contexts/.

We add /binders/ (basic math expression with eager dynamic binding): \(S : C \times P \to \Nat\).

\[ S(c,\text{var}(v)) = c(v) \]
\[ S(c,\text{let}(n,e,b)) = S(c + \Set{(n,S(n,e))},b) \]

What use is mathematics, if the mathematics ends up looking like a Lisp/Prolog/Haskell program!?

The argument for eager "let": If you don't use a variable, simply don't define it.

The argument for lazy "let": Immutable cyclic data cannot be done with eager "let".
** Semantics of application?
\[ S(f(x)) = [S(f)](S(x)) \]
** Semantics of state / mutation?
We add mutable binders.

An example state-transformer semantics: \(M : (P \times E \to \Nat) \to (P \times E \to \Nat)\).
** Garbage collection simplifies semantics?
It is surprising that programming languages that have higher implementation complexity can have simpler semantics.

It is surprising that higher-level languages are more complex to implement but simpler to reason about.

If all we have is machine code (that is, if we are writing the /first/ electronic computer program ever),
is it harder to implement a minimal assembler or a minimal Lisp interpreter?
* Informal program complexity
The complexity of a program is the difficulty of describing what it does.

The complexity of a program is the length of the description of what it does.
* Programs that write programs
Programs that write abstract syntax trees?

#+BEGIN_EXAMPLE
(defmacro (make-prin1 x)
  "A trivial example."
  `(progn
    (prin1 ,x)
    (terpri)))
#+END_EXAMPLE

Programs that write concrete syntax trees?

- Instead of =read=, there should be =read-cst= and =cst->ast=.
  Preserve source location for formatter and refactoring tools.

It should be possible to parse programs /without executing them/?
* Specification and complexity
** Motivation
We want to define a complexity measure of specifications.

We want to be able to determine whether specification X is easier to implement than specification Y.

But isn't this NP-hard?
What are our chances?
If it is impossible to determine whether specification X is easier to implement than specification Y,
then is "software engineering" possible at all?
** The syntax of specifications
A /specification/ is a positive first-order logic formula.

Informally, a positive formula is a formula that does not contain negation.

In the rest of this document,
by "formula" we usually mean "positive first-order logic formula".

The abstract syntax of formulas (expressions) is:
- Each /atom/ is an expression.
  An atom is an identifier such as \(p\),  =name_123=, \(\wedge\), \(\forall\), etc.
- If each of \(\beta,\alpha_1,\ldots,\alpha_n\) is an expression,
  then the /application/ \(\beta(\alpha_1,\ldots,\alpha_n)\) is an expression.

Should we have these?
- An atom is a zero-arity functor: \( \alpha = \alpha() \).
- Currying: \( [\alpha(\beta_1,\ldots,\beta_n)](\gamma,\ldots) = \alpha(\beta_1,\ldots,\beta_n,\gamma,\ldots) \).
** Syntactic complexity partial order
What do we mean by "X is less complex than Y"?

The key idea of the /syntactic-complexity partial-order/ \(\le\) is
"If \(\alpha\) occurs in \(\beta\), then \(\alpha \le \beta\)":
\begin{align*}
\alpha &\le \beta(\ldots, \alpha, \ldots)
\\ \alpha &\le \alpha(\ldots)
\end{align*}

Examples:
\begin{align*}
A &\le \wedge(A,B)
\\ B &\le \wedge(A,B)
\\ A &\le f(A)
\\ A &\le [A(B)](C)
\\ x &\le +(x,y)
\\ z &\le \wedge(x,\wedge(y,z))
\\ \forall &\le \forall(x,y)
\\ A(x) &\le \forall(x,A(x))
\end{align*}
** Syntactic complexity measure
The /syntactic complexity/ of a formula \(x\) is a natural number \(c(x)\) where:
\begin{align*}
c(x) &= 1 & \text{if \(x\) is an atom}
\\ c(\beta(\alpha_1,\ldots,\alpha_n)) &= c(\beta) + c(\alpha_1) + \ldots + c(\alpha_n)
\end{align*}

Numbers are encoded as Peano numerals like \(z,s(z),s(s(z))\), etc.,
because we want bigger numbers to have bigger syntactic complexity.
** Semantic complexity measure
The /semantic complexity/ of a formula \(x\) is \(C(x) = c(y)\)
where \(y\) is the simplest (the least syntactically complex) formula that is /equivalent/ to \(x\).
\[ C(\alpha) = \min_{\alpha \equiv \beta} c(\beta) \]

Iff \(\alpha \equiv \beta\) then \(C(\alpha) = C(\beta)\).

Example: In classical logic, \(C(\wedge(\alpha,\alpha)) = C(\alpha)\).

In classical logic, \(C\) is unmonotonic, for example: \(C(\neg\neg\alpha) \le C(\neg\alpha)\).

The semantic complexity of a formula is the syntactic complexity of its simplest equivalent formula.

? "essential complexity" = "irreducible complexity" = "semantic complexity"

? "accidental complexity" = "reducible complexity" = "syntactic complexity"

(Brooks et al.?)
** Specification complexity
Let x, y be positive literals.

It is easier to do less:

c(x) leq c(x wedge y)
c(y) ...

c(y) leq c(x to y)

c(x) leq c(x to y)

The complexity of a formula is its proof complexity?

A specification is a logical formula in conjunctive normal form in which negation is only allowed in the antecedent of implications.
(Conjunction of Horn clauses?)

Example of specification of state/memory using discrete-time logic?
This assumes a temporal ordering of commands.

Forall t in nat: if command(t) = increment then state(t+1) = state(t) + 1
** Problems
It does not make sense that \(\forall(x,p(x))\) and \(q(x,y,z)\) have the same semantic complexity?

Similar issue: [[https://cs.stackexchange.com/questions/48329/size-of-propositional-formula][CS SE 48329]].
** What?
1982
A measure of logical complexity of programs
Iyengar et al.
https://www.sciencedirect.com/science/article/abs/pii/0096055182900030

Some second-order logic can be embedded into first-order logic with this trick:
p(x, ...) can always be rewritten to apply(p, x, ...)

Rijnders 2008
First-Order Logic as a Lightweight Software Specification Language
https://homepages.cwi.nl/~paulk/thesesMasterSoftwareEngineering/2008/MichelRijnders.pdf
