#+TITLE: Sketch of PL-0 programming language
#+DATE: 2019-02-07 00:00:00 +0700
* <2019-11-27> Thought
It is easy to process a byte list into a token list.

The question is:
How should we interpret that token list?
How should we ascribe meaning to that token list?
How should we map tokens to values?

The lowest layer is more like a library for manipulating tokens than a language.

A stream of bytes is translated into a stream of tokens.
A token is either /white/ or /black/.
A token has /location/.
A token list has /location/.

I want to use the same name "append" for appending lists and appending strings;
I don't want "list-append" and "string-append".
We can implement this with types or namespaces.
I'm fine with explicitly-prefixed namespaces like this:
#+BEGIN_EXAMPLE
(define (example)
  (import list)
  (import string)
  (list:append '(1) '(2))
  (string:append "a" "b"))
#+END_EXAMPLE
* The goals
PL-0 programs should be translatable to native code.

Foreign function interface should be easy.
* The lazy implementor's language
PL-0 (not to be confused with Wirth's PL/0[fn::<2019-11-18> https://en.wikipedia.org/wiki/PL/0]) is a lazy implementor's programming language,
where the language designers do not strive to please language users.

The focus is on implementability, not usability.

The implementation tries to be stupid, simple, predictable, understandable, not smart.

Our motto is "we only do what we understand" (read: "we only do easy things").
Here are examples of our incompetence:
- We don't implement many modern language features like closures, continuations, proper tail calls, and type checking,
  because we think they are hard to implement.
  We have not tried to actually implement them; we are spooked by the mere thought of having to implement them.
- Parsing is hard, so we only do lexical analysis.
  The token list becomes the degenerate concrete syntax tree.
- Writing a compiler is hard, so we write an interpreter.

We think we will end up reimplementing Forth[fn::<2019-11-18> https://en.wikipedia.org/wiki/Forth_(programming_language)] or PostScript
or something very similar to them.

PL-0 is a stack-based virtual machine.

PL-0 enables you to write unsafe programs that can crash.
(This is not something to brag about?)

PL-0 is like an interpreted assembly language, a scripting assembly language.
* Guide for embedding PL-0 in C++ programs
** PL-0 C++ conventions
The C++ namespace is =stc_pl_0=.
** Creating a virtual machine
Each instance of the =Machine= class is a virtual machine with operand stack, dictionary stack, return stack, and heap.
The size of each memory area is fixed when the =Machine= is instantiated.

#+BEGIN_EXAMPLE
Machine machine;
#+END_EXAMPLE
** Executing programs
A /program/ is a sequence of tokens.
For example,
"1" is a program that pushes the word 1 to the stack.
The following is a program that consists of /six/ tokens (1, space, 2, space, add, newline):
#+BEGIN_EXAMPLE
1 2 add
#+END_EXAMPLE

#+BEGIN_EXAMPLE
void            Machine::push_source (Token_Iterator&)
Token_Iterator& Machine::pop_source ()
#+END_EXAMPLE

A /token iterator/ can be created from an in-memory token list or an in-disk source file.
A file-based token-iterator maintains a location (path, line, column, byte offset).

A /token/ is a byte string with location information (to keep track of its provenance).

Typically, =Machine::step= is called in a loop.
An iteration in the execution loop goes like this, if we ignore errors:
- read token
- determine the executable of that token
- execute that executable (a primitive, a value, a token, or a token list)

/The =step= method executes at most one token./
If the meaning of the token is a token list,
then =step= creates a call frame and arranges the next =step= call to execute the first token of the subroutine.

The machine reads the current program from a token iterator.
** Creating primitives
A /primitive/ is a foreign procedure that may mutate the machine state.

#+BEGIN_EXAMPLE
using Prim = void (Machine&);
#+END_EXAMPLE

A primitive must not throw any C++ exceptions.
** Quoting
The program =quote W B= pushes =B= to the operand stack where =W= is expected to be a white token.
** Macros
A macro is a procedure that transforms a prefix of the remaining program token stream.

A macro transforms a concrete syntax tree.

Important: Whitespaces are tokens too.

Macro : Cst -> Cst
** What?
#+BEGIN_EXAMPLE
% A B C muladd -> A*B+C

quote muladd { mul add } def

define (muladd x y z)
  x y mul z add
end
#+END_EXAMPLE

Curly braces delimit a token list?

Macros are ordinary functions.

=quote= reads the token right after the token currently being interpreted but does not execute it.

#+BEGIN_EXAMPLE
1 2 quote add -> 1 2 add
1 2 add -> 3
#+END_EXAMPLE

Type information can be attached to value (Scheme), variable (C++), or function (Assembly).
If we want function polymorphism (Scheme display), then we must choose to attach type information at either value or variable.

Why choose?
Why not attach type information everywhere (to values, variables, and functions)?

If we want =read= to produce a value (not a type-value pair), then values must carry type information.

In mathematics, it is natural to overload functions (such as +). Otherwise we would have +N, +Q, +R, etc. which is ugly.
Do we care about what something is, or about what can we do with it?

PostScript enables the programmer to choose between early binding and late binding.
* <2019-11-28> The problem is not binding; the problem is closures
If we don't have closures, then it does not matter whether we use static (lexical) or dynamic binding; the result will be the same.

The problem is not static vs dynamic binding.
The problem is: Should we have closures or not?

Why do we bother having closures if programmers can do explicit closure conversion?
For example:
#+BEGIN_EXAMPLE
f x = \ y -> x + y
-- gets closure-converted to
f x = (\ x y -> x + y) x
#+END_EXAMPLE
* Syntax and parsing
** Reversibility, information-preservation
I insist that the parser be reversible, because I want traceability and debuggability.

Each stage must be reversible:
it must either be a bijection or preserve enough information from the previous stage.

The first stage is character + location (defined later).

The next stage is tokenization.

A token has type and a list of characters.

The next stage is concrete syntax tree (CST).

The concrete syntax tree is required for formatting and refactoring, because those activities should preserve comments.

In Lisp syntax, a token coincides with an AST node.

The next stage is abstract syntax tree.

An AST node has a "main" CST node.

An AST node has a "preceding-whites" (a list of whitespace CST nodes that precede that AST node)
so that the AST node can be turned back into CST node (and so on until we reach the original substring that constitutes the CST node).

The parser is a recursive descent parser because I don't know how to parse.
** Locations
A /location/ is a tuple of path, line (0-based), column (0-based), byte-offset.
This is like Racket srcloc.

=current-location= parameter

=read= from current location

=raise-parse-error= at current location
** Macro, reflection, reification, quoting
The language should be a model of itself.

The language should be able to describe itself.

Does that cause a paradox?
** Annotations: user-defined metadata attached to concrete syntax tree nodes
(Is this a good idea?)

We add these expression syntax rules:

- If M is an expression and E is an expression, then =E : M= (read: data E annotated with metadata M) is an /annotated expression/.
  - Alternative syntax: =E : M= can also be written =meta M E=.

This generalizes type systems.
With type systems, you annotate an expression with a type expression.
With general annotations, you annotate an expression with another expression (some of which are type expressions).

We assume that the outermost metadata update wins:

- meta M (meta N E) = meta M E

We add metadata extraction function symbol =meta-of=.

We add these beta-reduction rules:

- reduce (meta M E) = reduce E
- reduce (meta-of (meta M E)) = reduce M
- reduce (meta-of E) = #<empty-record> (for expressions without metadata)

This is like Java/C# annotation but more principled?

Annotations are not types.

This is an example of type annotation that our annotation above can't handle: =\ (x : T) -> y=,
because =x= is not an expression.
* Bottom-up design?
** Example
- Example of bottom-up language design and how each level reduces cognitive load:
  - Begin with machine code.
  - Provide mnemonics for instructions.
  - Provide the illusion of infinite custom-named registers and orthogonal operands.
  - Provide macros subroutines as extensible instructions.
  - Provide the illusion of infinite custom-named registers and orthogonal operands.
  - Provide macros and subroutines as extensible instructions.
  - Provide named locations.
  - Provide the illusion of infinite memory.
  - Abstract away processor registers.
  - Abstract away pointers.
  - Expression.
  - Infix expression syntax.
  - First-class functions.
  - The program itself is a procedural program that tells the interpreter what code to generate.
  - End up with something like Randall Hyde's High Level Assembly?
** Starting with assembly
PL-0 is slightly more abstract than typed assembly languages (TALs).

We may begin from x86 assembly.

First we abstract away locations, registers, memory,
so that we can write something like this:
#+BEGIN_EXAMPLE
mov dword ptr [var_1], [var_2]
#+END_EXAMPLE

Macro Assembler (MASM)?
TASM, NASM, what?

There does not exist a computer with infinite memory.
Why do we pretend, with garbage collection, that the computer had infinite memory?
Because it simplifies most problems?

What is the problem with these:
High-Level Assembly,
typed assembly languages such as TALx86 \cite{crary1999talx86}[fn::<2019-11-04> https://www.cis.upenn.edu/~stevez/papers/MCGG99.pdf],
LLVM IR,
MSIL,
JVM bytecodes?

We can add a type system to assembly language to enforce constraints like these:
- "Add-integer" takes two integers.
- "Add-pointer" takes a pointer of alignment N and an integer that is an integral multiple of N.
- It is illegal to add two pointers.

For example, a type may be:
- =Integer N= where N is 1, 2, 4, or 8
- =Pointer A= where A is the alignment (1, 2, 4, or 8)

One difficulty is that the same register may sometimes contain an integer and sometimes contain a pointer.

We can "solve" that with Static Single Assignment (SSA) Form and automatic register allocation.

But perhaps the bigger issue is to abstract away the difference between processors;
why should we care if it is an Intel processor, a Motorola processor, a Symbolics Lisp machine, or something else?

Even though the machine does not know about subroutines,
we organize our programs into subroutines;
we find it more convenient to work with subroutines than to work with instructions.
We feel that the instructions are too finely-grained, unnecessarily detailed.
* Bibliography
