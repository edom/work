#+TITLE: Sketch of PL-0 programming language
#+DATE: 2019-02-07 00:00:00 +0700
* Overview
Alternative title: "Exploring the programming language design space"
** What we are going to do here
Here we design a [[https://en.wikipedia.org/wiki/Lisp_(programming_language)][Lisp]], its semantics, and its interpreter.
(Note that, in the 21st century, Lisp means a family of languages, not a particular language.)

We design a Lisp because we want to focus on semantics;
we can always dress up the syntax later.

We name our language "PL-0" (not to be confused with [[https://en.wikipedia.org/wiki/PL/0][Wirth's PL/0]]).

We want a programming language for implementing programming languages.

We choose C++ as the implementation language because we don't know any better.
We considered Rust and Go but we could not make up our minds.
We refuse C because we want namespaces; we refuse to manually prefix every procedure name.

To consider: [[http://tomasp.net/blog/2017/design-side-of-pl/][Petricek 2017]] (about programming language design).
\cite{ingalls1981design}
 [fn::<2019-12-11> [[https://www.cs.virginia.edu/~evans/cs655/readings/smalltalk.html][mirror 1 (html)]]]
 [fn::<2019-12-11> [[https://cs.pomona.edu/classes/cs131/readings/ingalls.pdf][mirror 2 (pdf)]]].
\cite{coblenz2018interdisciplinary}
 [fn::<2019-12-11> [[http://www.cs.cmu.edu/~NatProg/papers/onward18essays-p7-p-682d101-38832-final.pdf][mirror (pdf)]]].
[[https://leastfixedpoint.com/tonyg/kcbbs/projects/thing.html][Tony's Programming Language Experiments]].
[[http://lisp-univ-etc.blogspot.com/2012/04/lisp-hackers-pascal-costanza.html][3-Lisp's tower of interpreters]].

We can begin designing from the semantics (our desire) or from the run-time system (our constraints).
All design is a compromise between the ideal and the practical.
** The lazy implementor's language
PL-0 is a lazy implementor's programming language,
where the language designers do not strive to please language users.

The focus is on implementability, not usability.

The implementation tries to be stupid, simple, predictable, understandable, not smart.

Our motto is "we only do what we understand" (read: "we only do easy things").

Here are examples of our incompetence:
- We don't implement many modern language features like closures, continuations, proper tail calls, and type checking,
  because we think they are hard to implement.
  We have not tried to actually implement them; we are spooked by the mere thought of having to implement them.
- Parsing is hard, so we only do lexical analysis.
  The token list becomes the degenerate concrete syntax tree.
- Writing a compiler is hard, so we write an interpreter.

We thought of making a stack-based language such as
Forth[fn::<2019-11-18> https://en.wikipedia.org/wiki/Forth_(programming_language)] or PostScript,
but we don't know how to implement /implicit arguments/ (optional arguments) in a stack-based language,
other than by explicitly passing a possibly empty list, so we choose to make a Lisp instead.

The question of Forth vs Lisp is:
How hard do we allow the user to crash without foreign procedures:
just an exit due to an unhandled exception (Lisp), or an abort due to a segmentation fault (Forth)?
* Run-time system
[[https://drops.dagstuhl.de/opus/volltexte/2015/5475/pdf/4.pdf][Belikov 2015]] \cite{belikov2015language}
** Memory management
We use a /garbage collector/ because we believe that that garbage collection greatly simplifies the language semantics.
Also, we don't know how to implement a Lisp without garbage collection
like [[https://github.com/wolfgangj/bone-lisp/][Bone Lisp]], Pre-Scheme, Carp, newLISP, Linear Lisp, and ThinLisp.

We use a /copying garbage collector/
because we are convinced by
Appel 1987 \cite{appel1987garbage}
 [fn::via [[https://softwareengineering.stackexchange.com/questions/364371/type-based-memory-safety-without-manual-memory-manage-or-runtime-garbage-collect][Basile Starynkevitch]]]
that "[naÃ¯ve copying] garbage collection can be faster than stack allocation".

The drawbacks of our simple choices are:
- We have to overprovision physical memory if we want our programs to run at a reasonable speed.
- We lose real-time guarantee; the program may pause for an unpredictable duration at inopportune times.

We may wish to do these later:
- Improve the garbage collector to be generational and concurrent.
  Currently we stop the world while we collect garbage because we don't know how to do it concurrently.
- Implement alternative garbage collectors and let the programmer choose.
- Write a compiler for, say, PL-1, a language with manual memory management, and probably also static typing, on top of PL-0.
  Thus the real-time part of the program can be written in PL-1 while seamlessly interoperating with PL-0.
** Converting C types
void, uintN_t, intN_t, intptr_t, for N in {8,16,32,64}.
** Foreign interface, mostly C
We do not expect users to use this directly.
The ideal thing for user is to make PL-0 understand C header files.
That is, PL-0 should come with a C parser and preprocessor
that translate signatures to PL-0 bridges.
Compare: [[http://www.swig.org/][SWIG]].
(But why stop there; why not go all the way and write a C interpreter/compiler in PL-0?)

We should use [[https://sourceware.org/libffi/][libffi]] for portability.

Compare: [[https://docs.racket-lang.org/foreign/index.html][Racket Foreign Interface]].

Suppose there is a C procedure whose declaration is
#+BEGIN_EXAMPLE
Ret proc(Arg-1, ..., Arg-n)
#+END_EXAMPLE
and we want to call it from PL-0.

With power comes responsibility:
The foreign interface enables users to crash the program.

We must represent the /type/ and construct the /reference/.

A =Type= is any of these:
#+BEGIN_EXAMPLE
char
int
(unsigned int)
int32_t
uint32_t
(procedure Type (Type-1 ... Type-n))
(struct (Field-1 ... Field-k))
    where each Field-k is a list [name Type]
(union (Type-1 ... Type-n))
#+END_EXAMPLE

Reference constructors:
#+BEGIN_EXAMPLE
(ref Type Address)
#+END_EXAMPLE

Actions:
#+BEGIN_EXAMPLE
(read Ref) -> Val
(write Ref) -> Val
(call Ref) -> Val
#+END_EXAMPLE

We can obtain symbol addresses with =dlsym=.
** Values
What should the set of values (the irreducible meanings) in a programming language be?

Perhaps we all agree that the set of values must include at least some integers.

A /value/ (an /object/) is any of these:
- a representation of a mathematical object:
  - an /integer/ (of arbitrary precision)
  - a /pair/ (a /cons cell/)
  - a /unit/ (like C void)
  - a /boolean/ (false or true)
  - a /byte string/
- a generic data structure:
  - a /list/
  - a /vector/ (a /heterogenous array/)
  - a /hash table/
- a structure used by the interpreter:
  - a /namespace/
  - an /environment/ (a /context/)
  - a /rule/, function, macro, AST transformer
  - a /type/
- a structure used by the parser:
  - a /location/
  - a /concrete syntax tree/ (CST)
  - an /abstract syntax tree/ (AST)
- a structure used by the C interface
  - a C type representation
  - a C reference (a type and an address)

There are so many values; are we sure that all of them should be primitives?

Difference from common Lisps:
- In PL-0, lists and pairs are different things.
- PL-0 does not have /nil/.
** Do we need generic functions? The case of "append"
I want to write just =append= instead of =list-append=, =vector-append=, =bytestring-append=, etc.
In other words, want =append= to be /polymorphic/.

What are my choices?

I can define =append= with =cond=.

But what if users also want to customize =append=?

They can define their own =append= using =cond= in their own namespaces and fall-back to the standard =append=.

Or I can define =append= to be a generic function.

But generic function becomes extremely tricky with subtyping.
Julia solves this with a complete lattice of types.
But do we have to deal with the unholy interaction between generics/polymorphism/multiple-dispatch and subtyping?

A combination of namespaces and =cond= is simpler than generic functions, and achieves closed ad-hoc polymorphism, but is it better?
** Representation of values
=read-cst= is similar to Racket's =read-syntax=,
but =read-cst= reads comments, and the result of =read-cst= can be turned back to source code (textual representation).

=read= is implemented by calling =read-cst= and recursively discarding location information and comment nodes.

Unlike in other Lisps, in PL-0, the external representation of a pair is =#pair(head tail)=, not =(head . tail)=.
* Semantics
What should a symbol mean?
It usually means a hash-table lookup,
where the symbol is the key and the environment is the hash table.
But is there a better semantics?

In human languages, the meaning of a symbol is usually determined by agreement/consensus between the users of the symbol.
For example, I can define "foobar" to mean "table" in a document,
and the readers will be able to understand the document if they play along.

The meaning of a symbol may be defined in terms of the meaning of other symbols.
For example, "/chair/" may be defined as a "/seat/ with /back rest/".

In human languages, the irreducible meanings are the /direct experiences/ (such as the concepts represented by "red", "sweet", "happy").
For other examples of irreducible meanings,
see [[https://en.wikipedia.org/wiki/Semantic_primes][Semantic primitives]]
and [[https://en.wikipedia.org/wiki/Natural_semantic_metalanguage][Natural semantic metalanguage]].

In Assembly, the irreducible meanings are the meaning of the execution of an instruction;
such meanings can be formalized as state transformers.
For example, the meaning of executing =inc rax= is to mutate the machine state such that =rax= now contains the previous value of =rax= incremented by one, modulo \( 2^{64} \).

Perhaps we want something like [[https://en.wikipedia.org/wiki/Refal][Refal]] but in Lisp syntax?

How do we build meaning in mathematics?
We may start from logic, axioms, natural numbers.

[[https://en.wikipedia.org/wiki/Jakobson%27s_functions_of_language][Jakobson's functions of language]]

In Lisps, the irreducible meanings are the meaning of the values, including the side-effects.

How do we distinguish between "Print 2 + 3" and "Print /the result of calculating/ 2 + 3"?
We use [[https://en.wikipedia.org/wiki/Use%E2%80%93mention_distinction][Use-mention distinction]].
See also B. C. Smith's PhD thesis.

A quoted word means itself.

Therefore, to design a programming language is to decide /how to build meanings from a finite set of irreducible meanings/.
That is, how to build values.

However, meanings are inseparable from pragmatics.
For example, the expected answer to "Can you pass me the salt?" is not the /utterance/ "Yes", but the /action/ of passing the salt.

#+BEGIN_EXAMPLE
interpret : Internal-Form -> Meaning
interpret : Abstract-Syntax -> Semantics
#+END_EXAMPLE

We must distinguish between an /internal form/ and its /external representation/.

The =read= function transforms an external representation into an internal form?

#+BEGIN_EXAMPLE
(calculate (+ 1 2)) -> 3
(calculate (+ 1 2) into x) ???
#+END_EXAMPLE

A procedure can be thought of as a term rewriting rule (a reduction rule).

Should a =define= be interpreted as a =hash-set!= or as a /rule definition/?

Which syntax should we use to define a rule?

#+BEGIN_EXAMPLE
(rewrite x 1)

(rewrite (x) (f x) (+ x x))

(define-rewrite (forall (x) [(f x) (+ x x)]))

(rewrite (f :lit x :var) :to (+ x x))

(with-variables (x)
  (with-literals (f +)
    (with-undefined-symbols-as-literals
      (with-numeric-symbols-as-numbers
        (defrule (f 0) 1)
        (defrule (f x) (* x (f (- x 1))))
      ))))

(define-function (f x) (+ x x))
#+END_EXAMPLE

Should /numeric symbols/ (symbols that look like numbers: symbols that consist of only digits) be treated as /numbers/?
I think yes, because we have the vertical-bar syntax like =|123|= to mean arbitrarily named symbols, including non-number numeric symbols.

The meaning of a /rule/ \( A \to B \) is to /replace/ every /matching/ occurrence of \(A\) with \(B\) in the /current expression/.

A /function/ can be thought of as a rewriting rule;
the function name matches literally;
the function arguments match everything (are wildcards).

A symbol may be treated as a /literal/ or a /variable/.

For example, in =(define-function (f x) ...)=, the symbol =f= is a literal, and =x= is a variable.

In a function header, the pattern =(head arg1 ... argN)= matches every list that:

1. has length N+1, and
2. begins with something that has the same binding as =head=.

What should a list such as =(x)= mean?

What should a list such as =(x y)= mean?
** Term rewriting semantics?
For efficiency, we require that the head of a rule begins with a /literal/,
so that we can /index/ the rules for fast matching/retrieval.

The programmers are responsible for ensuring confluence by avoiding ambiguous/overlapping rules.
** Graph reduction semantics?
Should the semantics be formulated in terms of expression graph reductions/transformations?

An S-expression can be thought of representing a /tree/ (or, more precisely, a /graph/).

A value can be thought of as an irreducible one-vertex graph.
* Syntax and parsing
We use a recursive descent parser because we don't know any better.
** Reversibility, information-preservation
I insist that the parser be reversible, because I want traceability and debuggability.

Each stage must be reversible:
it must either be a bijection or preserve enough information from the previous stage.

The first stage is character + location (defined later).

The next stage is tokenization.

A token has type and a list of characters.

The next stage is concrete syntax tree (CST).

The concrete syntax tree is required for formatting and refactoring, because those activities should preserve comments.

In Lisp syntax, a token coincides with an AST node.

The next stage is abstract syntax tree.

An AST node has a "main" CST node.

An AST node has a "preceding-whites" (a list of whitespace CST nodes that precede that AST node)
so that the AST node can be turned back into CST node (and so on until we reach the original substring that constitutes the CST node).

The parser is a recursive descent parser because I don't know how to parse.
** Locations
A /location/ is a tuple of path, line (0-based), column (0-based), byte-offset.
This is like Racket srcloc.

=current-location= parameter

=read= from current location

=raise-parse-error= at current location
** Macro, reflection, reification, quoting
The language should be a model of itself.

The language should be able to describe itself.

Does that cause a paradox?
** Annotations: user-defined metadata attached to concrete syntax tree nodes
(Is this a good idea?)

We add these expression syntax rules:

- If M is an expression and E is an expression, then =E : M= (read: data E annotated with metadata M) is an /annotated expression/.
  - Alternative syntax: =E : M= can also be written =meta M E=.

This generalizes type systems.
With type systems, you annotate an expression with a type expression.
With general annotations, you annotate an expression with another expression (some of which are type expressions).

We assume that the outermost metadata update wins:

- meta M (meta N E) = meta M E

We add metadata extraction function symbol =meta-of=.

We add these beta-reduction rules:

- reduce (meta M E) = reduce E
- reduce (meta-of (meta M E)) = reduce M
- reduce (meta-of E) = #<empty-record> (for expressions without metadata)

This is like Java/C# annotation but more principled?

Annotations are not types.

This is an example of type annotation that our annotation above can't handle: =\ (x : T) -> y=,
because =x= is not an expression.
* <2019-11-27> Thought
It is easy to process a byte list into a token list.

The question is:
How should we interpret that token list?
How should we ascribe meaning to that token list?
How should we map tokens to values?

The lowest layer is more like a library for manipulating tokens than a language.

A stream of bytes is translated into a stream of tokens.
A token is either /white/ or /black/.
A token has /location/.
A token list has /location/.

I want to use the same name "append" for appending lists and appending strings;
I don't want "list-append" and "string-append".
We can implement this with types or namespaces.
I'm fine with explicitly-prefixed namespaces like this:
#+BEGIN_EXAMPLE
(define (example)
  (import list)
  (import string)
  (list:append '(1) '(2))
  (string:append "a" "b"))
#+END_EXAMPLE

Peter Van Roy's "Programming Paradigms for Dummies: What Every Programmer Should Know"
https://www.info.ucl.ac.be/~pvr/VanRoyChapter.pdf
* Guide for embedding PL-0 in C++ programs
** PL-0 C++ conventions
The C++ namespace is =stc_pl_0=.
** Creating a virtual machine
Each instance of the =Machine= class is a virtual machine with operand stack, dictionary stack, return stack, and heap.
The size of each memory area is fixed when the =Machine= is instantiated.

#+BEGIN_EXAMPLE
Machine machine;
#+END_EXAMPLE
** Executing programs
A /program/ is a sequence of tokens.
For example,
"1" is a program that pushes the word 1 to the stack.
The following is a program that consists of /six/ tokens (1, space, 2, space, add, newline):
#+BEGIN_EXAMPLE
1 2 add
#+END_EXAMPLE

#+BEGIN_EXAMPLE
void            Machine::push_source (Token_Iterator&)
Token_Iterator& Machine::pop_source ()
#+END_EXAMPLE

A /token iterator/ can be created from an in-memory token list or an in-disk source file.
A file-based token-iterator maintains a location (path, line, column, byte offset).

A /token/ is a byte string with location information (to keep track of its provenance).

Typically, =Machine::step= is called in a loop.
An iteration in the execution loop goes like this, if we ignore errors:
- read token
- determine the executable of that token
- execute that executable (a primitive, a value, a token, or a token list)

/The =step= method executes at most one token./
If the meaning of the token is a token list,
then =step= creates a call frame and arranges the next =step= call to execute the first token of the subroutine.

The machine reads the current program from a token iterator.
** Creating primitives
A /primitive/ is a foreign procedure that may mutate the machine state.

#+BEGIN_EXAMPLE
using Prim = void (Machine&);
#+END_EXAMPLE

A primitive must not throw any C++ exceptions.
** Quoting
The program =quote W B= pushes =B= to the operand stack where =W= is expected to be a white token.
** Macros
A macro is a procedure that transforms a prefix of the remaining program token stream.

A macro transforms a concrete syntax tree.

Important: Whitespaces are tokens too.

Macro : Cst -> Cst
** What?
#+BEGIN_EXAMPLE
% A B C muladd -> A*B+C

quote muladd { mul add } def

define (muladd x y z)
  x y mul z add
end
#+END_EXAMPLE

Curly braces delimit a token list?

Macros are ordinary functions.

=quote= reads the token right after the token currently being interpreted but does not execute it.

#+BEGIN_EXAMPLE
1 2 quote add -> 1 2 add
1 2 add -> 3
#+END_EXAMPLE

Type information can be attached to value (Scheme), variable (C++), or function (Assembly).
If we want function polymorphism (Scheme display), then we must choose to attach type information at either value or variable.

Why choose?
Why not attach type information everywhere (to values, variables, and functions)?

If we want =read= to produce a value (not a type-value pair), then values must carry type information.

In mathematics, it is natural to overload functions (such as +). Otherwise we would have +N, +Q, +R, etc. which is ugly.
Do we care about what something is, or about what can we do with it?

PostScript enables the programmer to choose between early binding and late binding.
* <2019-11-28> The problem is not binding; the problem is closures
If we don't have closures, then it does not matter whether we use static (lexical) or dynamic binding; the result will be the same.

The problem is not static vs dynamic binding.
The problem is: Should we have closures or not?

Why do we bother having closures if programmers can do explicit closure conversion?
For example:
#+BEGIN_EXAMPLE
f x = \ y -> x + y
-- gets closure-converted to
f x = (\ x y -> x + y) x
#+END_EXAMPLE
* Bottom-up design?
** Example
- Example of bottom-up language design and how each level reduces cognitive load:
  - Begin with machine code.
  - Provide mnemonics for instructions.
  - Provide the illusion of infinite custom-named registers and orthogonal operands.
  - Provide macros subroutines as extensible instructions.
  - Provide the illusion of infinite custom-named registers and orthogonal operands.
  - Provide macros and subroutines as extensible instructions.
  - Provide named locations.
  - Provide the illusion of infinite memory.
  - Abstract away processor registers.
  - Abstract away pointers.
  - Expression.
  - Infix expression syntax.
  - First-class functions.
  - The program itself is a procedural program that tells the interpreter what code to generate.
  - End up with something like Randall Hyde's High Level Assembly?
** Starting with assembly
PL-0 is slightly more abstract than typed assembly languages (TALs).

We may begin from x86 assembly.

First we abstract away locations, registers, memory,
so that we can write something like this:
#+BEGIN_EXAMPLE
mov dword ptr [var_1], [var_2]
#+END_EXAMPLE

Macro Assembler (MASM)?
TASM, NASM, what?

There does not exist a computer with infinite memory.
Why do we pretend, with garbage collection, that the computer had infinite memory?
Because it simplifies most problems?

What is the problem with these:
High-Level Assembly,
typed assembly languages such as TALx86 \cite{crary1999talx86}[fn::<2019-11-04> https://www.cis.upenn.edu/~stevez/papers/MCGG99.pdf],
LLVM IR,
MSIL,
JVM bytecodes?

We can add a type system to assembly language to enforce constraints like these:
- "Add-integer" takes two integers.
- "Add-pointer" takes a pointer of alignment N and an integer that is an integral multiple of N.
- It is illegal to add two pointers.

For example, a type may be:
- =Integer N= where N is 1, 2, 4, or 8
- =Pointer A= where A is the alignment (1, 2, 4, or 8)

One difficulty is that the same register may sometimes contain an integer and sometimes contain a pointer.

We can "solve" that with Static Single Assignment (SSA) Form and automatic register allocation.

But perhaps the bigger issue is to abstract away the difference between processors;
why should we care if it is an Intel processor, a Motorola processor, a Symbolics Lisp machine, or something else?

Even though the machine does not know about subroutines,
we organize our programs into subroutines;
we find it more convenient to work with subroutines than to work with instructions.
We feel that the instructions are too finely-grained, unnecessarily detailed.
* Bibliography
