#+TITLE: Parsing Research Group
#+DATE: 2018-04-11 00:00 +0700
#+OPTIONS: ^:nil toc:nil
#+PERMALINK: /parse.html
* What is parsing?
In 1550, "to parse" is "to state the parts of speech in a sentence".[fn::https://www.etymonline.com/word/parse]
** What is the problem?
The problem is to design a language that describes languages.
To design a language specification language.
To design a meta-language, for describing grammars, with practical operational semantics.

The problem is "we donâ€™t know how to /specify/ language syntax"[fn::http://trevorjim.com/parsing-not-solved/].

The language should facilitate these:
- Specify the grammar of Haskell off-side rule.
- Specify the grammar of C.
- Specify the grammar of C++.

Are composable grammars and parsers possible?[fn::https://tratt.net/laurie/blog/entries/parsing_the_solved_problem_that_isnt.html]

Prolog definite-clause grammar (DCG) formalism is almost a dream.
It is declarative.
It handles context-sensitive grammars.
Its only weakness is its non-handling of left recursion.
But it seems that every left-recursive grammar can be algorithmically transformed into an equivalent non-left-recursive grammar, so what's the problem?
It shouldn't be too hard to implement a left recursion elimination algorithm for Prolog DCGs.
** Parsing with Brzozowski quotients
Parsing with Brzozowski quotients
 [fn::http://matt.might.net/articles/parsing-with-derivatives/]
 [fn::https://github.com/webyrd/relational-parsing-with-derivatives/blob/master/README.md]
 \cite{might2011parsing}.
Does the general parser community understand that?
How do we implement it in Prolog?

Some things I find interesting from \cite{might2011parsing}:
- Kleene fixed-point theorem has a practical application.

Equational theories?
Now that's a principled parsing.

The result of left-dividing a language \(L\) by a string \(c\) is
\(
c \backslash L = \SetBuilder{w}{cw \in L}
\).
\cite{brzozowski1964derivatives}
\cite{might2011parsing}

Atoms[fn::https://blog.github.com/2018-10-31-atoms-new-parsing-system/]

Differentiating Parsers[fn::http://lambda-the-ultimate.org/node/3704]

Might et al.'s 2011 pearl \cite{might2011parsing} is not in Grune & Jacobs's 2008 book \cite{grune2008parsing}, but the book does mention Brzozowski.
Brzozowski's idea goes back to his 1964 paper \cite{brzozowski1964derivatives}.
But who would have thought of adding laziness and memoization on top of it, and generalize it to context-free grammars?
(Formal definition?)
*** What?
- Differentiating Parsers
  Automatic derivation of incremental parser from a grammar or a non-incremental parser?
  Like automatic differentiation but generalized to any program?
  http://lambda-the-ultimate.org/node/3704
- http://matt.might.net/articles/implementation-of-regular-expression-matching-in-scheme-with-derivatives/
- http://okmij.org/ftp/continuations/differentiating-parsers.html
- Parsing with derivatives?

  - https://hackage.haskell.org/package/derp
  - https://arxiv.org/abs/1010.5023
  - http://matt.might.net/articles/parsing-with-derivatives/ "Yacc is dead"

- Brzozowski quotients.

  - [[https://arxiv.org/abs/1010.5023][Yacc is dead]]
  - "Parsing with derivatives"

- 2017, [[https://www.cl.cam.ac.uk/~nk480/parsing.pdf]["A Typed, Algebraic Approach to Parsing"]]

  - "[...] we extend the notion of Brzozowski derivative from regular expressions to the typed context-free expressions."
*** Digression: How is the Brzozowski derivative a derivative?
Why does Brzozowski 1964 \cite{brzozowski1964derivatives} calls it derivative if it is actually a quotient?
The article contains has no explanation, so here goes our guess.

The /Brzozowski derivative of language \(R\) with respect to string \(s\)/
is written \(D_s R\) and is \(\SetBuilder{t}{st \in R}\) \cite{brzozowski1964derivatives}.
The \(D_a\) in equation 3.7 in that article would indeed be a differential-algebraic /derivation/[fn::https://en.wikipedia.org/wiki/Derivation_(differential_algebra)]
(generalized product rule) if it lost the \(\delta(P)\) term:
\[
D_a(PQ) = (D_a P) Q + \delta(P) D_a Q
\]
But it is a derivative (a "derivation" /in spirit/),
and there are other things called derivatives that do not exactly satisfy the product rule either.
It makes sense for regular expressions to have derivatives,
because regular expressions can be studied from abstract-algebra perspective,
because regular expressions form an algebra as defined in Brzozowski 1964 section 2 (although he does not explicitly mention that the structure is indeed an algebra).

A corollary: what is the integral?
Smith & Yau 1972 \cite{smith1972generation} defines an integral counterpart to Brzozowski derivatives.
But is that integral really an anti-derivative?

Another corollary:
Under what conditions do quotients form derivations/derivatives?
** What
The /multiplication/ of two strings $x$ and $y$ is the concatenation $x \cdot y = x y$.

Multiplication is associative: $(xy)z = x(yz)$.

The /inverse/ of a string $x$ is written $x^{-1}$.
It's hypothetical.
It's pure symbolic manipulation.
Don't imagine what it looks like.
Do care about its properties:

- We define $x^{-1} x = \epsilon$.
- We define $x x^{-1} = \epsilon$.
- We define $(x y)^{-1} = x^{-1} y^{-1}$.

The /left division/ of a string $x$ by divisor $y$ is $y^{-1} x$.

The /right division/ of a string $x$ by divisor $y$ is $x y^{-1}$.

How do we define quotient and remainder?

The Brzozowski derivative is a quotient[fn::https://en.wikipedia.org/wiki/Quotient_of_a_formal_language],
because it is the result of dividing a language (a set of strings) by a string.
** The semantics of grammar expressions
Consider the expression X,Y.
Declaratively it means X followed by Y.
Operationally it means match X /then/ match Y.
** Possible-prefix incremental parsing?
Given a string S, find all rules that /may/ match a string that begins with S.
** A language can be thought as a possibly infinite set of strings
** Grammar and parsing
Grammar is the what.
Parsing is the how.

We say that a parser /implements/ a grammar.
** Techniques?
In /recursive descent parsing/[fn::https://en.wikipedia.org/wiki/Recursive_descent_parser], the program procedures mirror the grammar rules.
Backtracking /unreads/ the input (places the input back into a queue).

"How should I specify a grammar for a parser?"[fn::https://softwareengineering.stackexchange.com/questions/107266/how-should-i-specify-a-grammar-for-a-parser]

Naive parser with memoization.

TODO sample Leiss's book "Language equations"
** Regular expressions can be extended to context-free expressions by adding a fixed-point expression involving a binder
\( \mu a . b \).
** String and tree
A /string/ is a homogeneous sequence.

A /tree/ may be represented by a list of lists.

Parsing is relating strings and trees.
Parsing is creating a tree from a string.

What is an alphabet?
It may be the set of Unicode character code points.
It may be the set of the tokens that a /lexical analyzer/ may produce.

A parser /implements/ a grammar, as a machine /implements/ an algorithm.

A /lexer/ is a degenerate[fn::https://en.wikipedia.org/wiki/Degeneracy_(mathematics)] parser whose codomain is a list (which is a degenerate tree).

The parser is /parallelizable/ if there exists a relatively fast function \(combine\) such that for all \(x,y \in C^*\):
\[
P(xy) = combine(P(x), P(y))
\]
* Left-recursion elimination
It is possible to /manually/ eliminate left recursion by rewriting all rules of the form
\(A \to AB | C\) to \(A \to C B^*\)
where each of \(A\) and \(B\) is an expression that does not begin with \(A\).
I have an example in =parse_manual.pro=.

Are we OK with manual transformations?
There are not many left-recursive rules in practice.

For the computer, our manual transformation is a perfectly fine solution.

The grammar must not have a nullable left-recursive rule like \( A \to A \) or \( A \to \epsilon^* \).
Otherwise a computer running a naive top-down left-to-right parsing algorithm is doomed into infinite loop.
But we can argue that the only \(A\) satisfying \(A \to A\) is \(epsilon\),
and that \( \epsilon^* = \epsilon \).

Two problems arise:
- What about the parse tree?
  We want a parser, not a matcher.
- Can it be automated?

Why do we care about left recursion?
Grune & Jacobs 2008 sums it up:
"Basically almost all parsing is done by top-down search with left-recursion protection"\cite[p. vii]{grune2008parsing}.

We are interested in eliminating left recursion from Prolog definite-clause grammars (DCGs).

to-do: summarize:
- https://en.wikipedia.org/wiki/Left_recursion
- https://www.microsoft.com/en-us/research/publication/removing-left-recursion-from-context-free-grammars/

I got this idea for left-recursion elimination on <2019-02-20>, but this may be well-known.
** What is left recursion?
This is a grammar with three left-recursive non-terminals.
\begin{align*}
A &\to B | C
\\ B &\to Ab | b
\\ C &\to Bc | c
\end{align*}

We say that \(A\) /left-calls/ \(B\) iff there exists a reduction \(A \to B C\).

A non-terminal \(A\) is /left-recursive/ iff it may reduce to something beginning with itself.
For example, the following rule \(A\) is left-recursive.
\begin{align*}
A &\to B
\\ B &\to \epsilon | AC
\end{align*}

The left-call graph.
Each vertex represents a non-terminal.
An edge \((A,B)\) represents that \(A\) left-calls \(B\).

If the left-call graph is cyclic, then a top-down parser may not work.

Left-recursion elimination is about breaking cycles in the left-call graph.

How do we delete the minimum number of edges from a graph to make it acyclic?
Is this problem NP-hard?
** Semiring of languages
We care about algebra because it guides us to /correct/ algorithms.

A /semiring/ is---roughly---an additive group, a multiplicative group, and an interaction between addition and multiplication.

The alphabet is \(A\).
It is a finite set.

The semiring's underlying set is \(A^*\).

The languages of the same alphabet form a semiring.

0 is the empty set.

1 is \(\Set{\epsilon}\), the language that consists of the empty string only.

Addition is set union.

Multiplication is language concatenation: \(AB = \SetBuilder{ab}{a \in A, b \in B}\).
** Production rule, language endofunction, and least fixed point
We can think of a production rule as a /language endofunction/.
For example, we can think of the rule \(A \to \epsilon | a A\) as the function \(A \mapsto 1 + \Set{a} A\).
Then, we can think of the language described by the rule as the /least fixed point/ of the corresponding function,
that is, the smallest set such that \(A = 1 + \Set{a} A\).

If a rule is non-recursive, then the corresponding language endofunction
is a constant function that does not depend on the parameter.
** Factoring finite left-recursion
Conjecture:
Every finite left-recursive rule can be factored into the form \(A \to AB | C\)
such that the rule \(A \to C\) would not be left-recursive.

Example of /infinite/ left recursion:
\(A \to Aa\).
It matches an infinite string of \(a\).
** Left-recursive language
Because every rule can be factored as above,
it suffices us to consider the least fixed point of the function \( A \mapsto AB + C \).

We obtain the least fixed point by inferring the pattern formed by repeatedly replacing \(A = AB+C\) and manipulating the equation.
\begin{align*}
A &= AB+C
\\ A &= (AB+C)B + C
\\ A &= ABB + CB + C
\\ A &= (AB+C)BB + CB + C
\\ A &= ABBB + CBB + CB + C
\\ A &= \ldots + CB^3 + CB^2 + CB^1 + CB^0
\\ A &= \sum_{k\in\Nat} CB^k
\\ A &= C \sum_{k\in\Nat} B^k
\\ A &= C B^*
\end{align*}
It turns out that \( lfp(A \mapsto AB + C) = C B^* \).

Because we are not using extended context-free grammar (which would have regular expressions and the Kleene star),
we have to introduce an auxiliary non-terminal \(A'\) for representing \(B^*\):
\begin{align*}
A &= C A'
\\ A' &= 1 + BA'
\end{align*}

Observe that \(A' = B^*\).
\begin{align*}
A' &= 1 + BA'
\\ A' &= 1 + B(1 + BA')
\\ A' &= 1 + B(1 + B(1 + BA'))
\\ A' &= \sum_{k\in\Nat} B^k
\end{align*}
** Left-recursion elimination algorithm
The algebra leads us to this left-recursion elimination algorithm:
1. Remove the original rule for the left-recursive non-terminal \(A\) from the grammar.
1. Factor that original rule into the form \(A \to AB | C\) such that \(A \to C\) would not be left-recursive and would not be empty.
   If this is impossible, tell the user about the infinite left recursion.
   Do not add \(A \to AB | C\) to the grammar; this rule is only an intermediate product.
1. Add these two rules to the grammar: \(A \to C A'\) and \(A' \to \epsilon | B A'\).

We have just eliminated left-recursion in a principled way, in a provably language-preserving way, guided by algebra.
Now we understand why it works.
If we forget the algorithm, we can always derive it from the algebra.

Example:
#+BEGIN_EXAMPLE
Original left-recursive rule:
exp :- num ; "(", exp, ")" ; exp, "*", exp ; exp, "+", exp

After factoring (A :- ...) into (A :- A,B ; C):
exp :- exp, ("*", exp ; "+", exp) ; (num ; "(", exp, ")")

After replacement:
exp :- (num ; "(", exp, ")"), exp0
exp0 :- "" ; ("*", exp ; "+", exp), exp0
#+END_EXAMPLE
** Inlining the auxiliary rule's parse tree
Two grammars describing the same language may produce different parse trees.

Unfortunately left-recursion elimination changes the syntax tree.
How do we unchange it?
** TODO Prolog implementation
Write a Prolog program to eliminate left recursion from definite-clause grammars.

The logical meaning of the Prolog DCG rule \(A(x) \to B_1(x), \ldots, B_n(x)\) is the predicate \(A\)
where \(A(x,s_1,s_{n+1}) \leftarrow ( B_1(x,s_1,s_2) \wedge \ldots \wedge B_n(x,s_n,s_{n+1}) )\).
** Reverse parsing
parse((A,B),C) iff parse(r((A,B)),r(C)).

where r((A,B)) = r(B),r(A).

Reversing the parser makes it right-to-left top-down parser.
It can now handle left-recursion, but it can now not handle right-recursion.
* Language-oriented approach
The language-oriented approach to parsing is to make a language for expressing a relation between strings and trees.

The structure of the concrete syntax tree reflects the structure of the grammar production rules.

Example: a regular expression is a DSL for string matching / pattern matching / parsing.
* What is the inverse of parsing?
The inverse of parsing is /unparsing/ (tree linearization).

A reverse of parsing is /grammar inference/, that is to find a grammar that produces a given set of sentences \cite[p. 1]{grune2008parsing}.

Parsing is the treeization (delinearization, deserialization) of a line.
Unparsing is the linearization (serialization) of a tree.

Parsing is String -> Maybe Tree.
Unparsing is Tree -> String.

Can we make parsing truly one-to-one?
String -> Tree.
CST = AST.
Very rigid syntax.
Forbid whitespace freedom.

Another possibility: Inverse of parsing is anti-parsing (generation)?
From grammar, generate all possible strings and their syntax trees.

Inverse of analytical grammar is generative grammar?

- https://en.wikipedia.org/wiki/Generative_grammar
- https://en.wikipedia.org/wiki/Formal_grammar#Analytic_grammars

Parser is syntax analyzer.
Analysis is the opposite of synthesis?
What is syntax synthesizer?

Inverse of parsing is pretty-printing?

If matching is analogous to subtraction, then what is analogous to multiplication?
Generation?

- algebra of pretty-printing

  - 1995, Hughes, "The design of a pretty-printing library"
  - 1998, Wadler, "A prettier printer"
  - Hughes, Peyton-Jones, et al., http://hackage.haskell.org/package/pretty-1.1.3.6/docs/Text-PrettyPrint-HughesPJ.html

- [[https://www.cs.kent.ac.uk/people/staff/oc/pretty.html][Efficient simple pretty printing combinators]]
* Relational parsing
** What?
Recall that a /relation/ is a triple that consists of domain, codomain, and pairing.

A grammar \(G\) can be thought as a relation between the set \(F\) of forms and the set \(M\) of meanings: \(G \subseteq F \times M\).

In computer-language parsing, usually the form set \(F = C^*\) is the set of character strings,
and the meaning set \(M\) is the set of syntax tree nodes.

Viewing grammar as /relation/ leads to writing parsers as /logic programs/, which are almost synonymous with /relational programs/.

Shieber, Schabes, & Pereira 1995 \cite{shieber1995principles} sees parsing as deduction.
It sees parsing from proof-theory point of view.
It presents a proof-theoretic framework that unifies several parsing algorithms (CYK, Earley, etc.).
It implies that we can use a theorem prover for parsing.
But should we?

The correspondence: one Chomsky production rule corresponds to one Horn clause with two parameters (input and rest/unparsed).
P(A,B) means that the rule P matches the prefix of A that B lacks.

A DCG predicate can be thought as a relation between two strings.
\( P \subseteq C^* \times C^* \).

A /grammar relation/ is a relation \(G \subseteq C^* \times T\).
The set \(C\) is the /alphabet/.
The set \(C^*\) is the /Kleene closure/ of \(C\).
The set \(T\) is the set of /syntax trees/.

Let \(G\) be a grammar.

We say that a string \(S\) is /grammatical/ with respect to \(G\) iff there exists a tree \(T\) such that \(G(S,T)\).
We may omit "with respect to \(G\)" if it is clear from context that there is only one grammar.

Iff the grammar relation is a function, then we say that the grammar is /unambiguous/.
** History of DCG?
DCG evolved from Colmerauer's "metamorphosis grammar"?
* How do we parse? How should we?
** How?
Zaytsev & Bagge 2014 \cite{zaytsev2014parsing} survey

\cite{Mu2004AnIL}

\cite{alimarine2005there}

Kourzanov 2014 \cite{kourzanov2014bidirectional} bidirectional parsing

\cite{caballero1999functional}

somewhat unrelated \cite{Tan2016BidirectionalGF}

\cite{Matsuda2013FliPprAP}

Parsing is also called "syntax analysis" (analysis = breakdown, syntax = put together).

Parsing is the act of modifying the /state/ of the parser.
This is the operational view.

Parsing is converting a sequence to a tree.
This is the data view.

What is the difference between syntax and grammar?

We /lex/ (perform lexical analysis / tokenization) to clean up the grammar (no need to mention whitespaces in the grammar).

Lexing simplifies grammars.

With lexing:
#+BEGIN_EXAMPLE
    exp ::= exp PLUS exp
#+END_EXAMPLE

Without lexing:
#+BEGIN_EXAMPLE
    white ::= ...
    exp ::= exp white "+" white exp
#+END_EXAMPLE

"Strictly speaking, tokenization may be handled by the parser.
The reason why we tend to bother with tokenising in practice is that it makes the parser simpler,
and decouples it from the character encoding used for the source code."
([[https://en.wikibooks.org/wiki/Compiler_Construction/Lexical_analysis][Wikibooks:Compiler construction]])

- [[https://jeffreykegler.github.io/personal/timeline_v3][Parsing: a timeline -- V3.0]]: 2012 article about a history of parsing.
  - [[https://www.reddit.com/r/ProgrammingLanguages/comments/8cz97n/parsing_a_timeline_hopefully_this_puts_parsing_is/][Parsing: a timeline. Hopefully this puts "Parsing is a solved problem" to rest. : ProgrammingLanguages]]
  - [[http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2018/05/knuth_1965.html][Why is parsing considered solved?]]


Parsing is transforming a list into a tree.

Stand on the shoulders of giants.
2012 timeline of parsing.
https://jeffreykegler.github.io/personal/timeline_v3

partial parsing; wrong formatting
http://www.vinartus.net/spa/94j.pdf

Deep:
"Partial evaluation can turn a general parser into a parser generator."
"The Essence of LR Parsing"
Sperber_Thiemann_The_essence_of_LR_parsing.pdf


See the forest, not only the trees.

Some parsing techniques:
- recursive descent parser (writing a parser manually)
- parser generators: Happy (Haskell), Bison (with Yacc)
- parser combinators: Parsec (Haskell)
- PEG (parsing expression grammar)
- Brzozowski quotient
- binary-parser description languages: ASN.1, Google Protobuf, Apache Thrift, Apache Avro
- invertible parsing?
- https://en.wikipedia.org/wiki/Chart_parser
- Parsing Expression Grammar (PEG)
  - https://github.com/harc/ohm/
    - https://ohmlang.github.io/
      - https://harc.ycr.org/project/ohm/
  - Packrat
- 2015, [[https://arxiv.org/abs/1511.08307][Nez: practical open grammar language]]
- Earley parser
  - https://en.wikipedia.org/wiki/Earley_parser
  - https://hackage.haskell.org/package/Earley
- https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#parsing--pretty-printing
  - https://hackage.haskell.org/package/trifecta
  - https://hackage.haskell.org/package/parsers
- Parsing in Lisp and Racket https://stackoverflow.com/questions/21185879/writing-a-formal-language-parser-with-lisp
** Incremental/online parsing
How do IDEs not have to reparse the entire document when the user presses one keystroke?

Incremental parsing is parsing as input becomes available (without waiting for the whole input to become available).

- Type-directed automatic incrementalization

  - http://www.cs.cmu.edu/~joshuad/papers/incr/

- https://en.wikipedia.org/wiki/Incremental_computing

  - https://inc-lc.github.io/

- https://hackage.haskell.org/package/incremental-parser
- [[https://yi-editor.github.io/posts/2014-09-04-incremental-parsing/][incremental/online parsing]]

An /incremental/ parser is a relation \(step \subseteq C \times T \times T\).

The idea is to output to all possible continuations?
\(incrementalize : (C^* \to T) \to (C^* \to T^*)\)?
** How should we generate parsers and unparsers from grammars?
What we are interested in is how to specify grammar, and how to derive a parser and unparser from grammar specificiation.

I expect the computer to infer a parser and a pretty-printer from the same grammar.
Parser generators only give half of what I want.

I expect the computer to work with non-ambiguous left-recursive grammars.

How should parsing be done?
From grammar description, the machine should generate both a parser and a pretty-printer.

Given grammar, generate both parser and unparser/pretty-printer.
- http://www.semdesigns.com/Products/DMS/DMSPrettyPrinters.html?Home=DMSToolkit
- https://hackage.haskell.org/package/invertible-syntax-0.2.1/src/Example.lhs
- https://hackage.haskell.org/package/invertible-syntax
- [[http://www.informatik.uni-marburg.de/~rendel/unparse/rendel10invertible.pdf][Tillmann Rendel and Klaus Ostermann. "Invertible Syntax Descriptions: Unifying Parsing and Pretty Printing". In Proc. of Haskell Symposium, 2010.]]
- http://jssst.or.jp/files/user/taikai/2016/PPL/ppl1-1.pdf
- [[http://lambda-the-ultimate.org/node/4191][LTU: Invertible Syntax Descriptions: Unifying Parsing and Pretty Printing]]
- [[http://www.informatik.uni-marburg.de/~rendel/unparse/rendel10invertible.pdf][Invertible Syntax Descriptions: Unifying Parsing and Pretty Printing]]
** What parsing techniques/formalisms are there?
There are many techniques/formalisms:
- Prolog definite-clause grammar (DCG) rules
- Haskell parser combinators
- continuation-based parsing
- parser generators

Prolog DCG is interesting because it is often /reversible/: the same code often gives us both a parser and an unparser.

Logically, a production (a syntax rule) is a predicate (relation) of arity 2.
That is, the rule ~Exp ::= Num Op Num~ is logically the Horn-clause =exp(A,D) :- num(A,B), op(B,C), num(C,D)=.

The application of a rule to an input-list produces a syntax object and a remaining-list.
A syntax object contains the name of the rule that produces it, the part of the input that matches it, the input position, and so on.
We can make this with SWI-Prolog dicts.

We can use Scheme continuation for backtracking like Prolog.
*** Syntax objects?
The application of a rule to an input-list produces a syntax object and a remaining-list.
A syntax object contains the name of the rule that produces it, the part of the input that matches it, the input position, and so on.
We can make this with SWI-Prolog dicts.
*** Reversible programming? Bidirectional programming?
Example: If \(T\) is a terminal, then the nonterminal \(N \to T\) is invertible.
To parse, remove the prefix matching T from the input list.
To unparse, prepend T to the input list.

If the rules \(A\) and \(B\) are invertible, then the concatenation nonterminal \(N \to AB\) is invertible.

Thus we say the relation =cons/3= is invertible: =cons(H,T,[H|T])=.

We want something similar to Rendell & Ostermann 2010 \cite{rendel2010invertible}, but in Prolog instead of Haskell.

Given view : D -> V and modv : V -> V, the interpreter should be able to infer modd : D -> D.

modd = through view modv

Boomerang language?

Benjamin C. Pierce 2006 "The Weird World of Bi-Directional Programming"[fn::https://www.cis.upenn.edu/~bcpierce/papers/lenses-etapsslides.pdf]

Wikipedia[fn::https://en.wikipedia.org/wiki/Bidirectional_transformation]

Janus
 [fn::https://topps.diku.dk/pirc/?id=janus]
 [fn::https://en.wikipedia.org/wiki/Janus_(time-reversible_computing_programming_language)]
** How do we relate CST and AST without clutter?
Big problems in parsing: lossless clutterless relation between CST and AST.
** <2018-11-02> Direct left-recursive parsers in Prolog
The key: unify terminals before recursing into nonterminals.
#+BEGIN_EXAMPLE
% S is a list of character codes.
binary_operator([0'+]).
binary_operator([0'*]).

digit(C) :- code_type(C, digit).

number(S) :-
    digit([S])
;   append([[A], B], S), digit(A), number(B);

expression(S) :-
    number(S)
;   binary_operator(B), append([A, B, C], S), expression(A), expression(C).
#+END_EXAMPLE
** Relational parsing; parsing with Prolog
Parsing is turning a list into a tree.
*** Approaches
- 2002 course notes http://www.cs.sfu.ca/~cameron/Teaching/383/DCG.html
- 1987 article "Parsing and compiling using Prolog" http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.9739&rep=rep1&type=pdf
- relational approach
  - recognizer: =digit(Input)=
    - recognizer with explicit search strategy
  - prefix remover: =digit(Input, Unparsed_suffix)=
    - This is the approach used by Prolog DCG (definite clause grammar).
  - prefix extractor: =digit(Input, Parsed_prefix, Unparsed_suffix)=
    - This enables us to get the parsed input without =append/3=.
  - concrete syntax tree parser: =digit(Input, Parsed, Unparsed)= where =Parsed = number(Children)=.
    - An example of =Parsed= is =number(digit(1), number(digit(2)))=.
  - interpreter
- functional approach
  - parser combinator
- generator approach
  - parser generator
  - parsing expression grammar
- procedural approach
  - recursive-descent
- https://en.wikipedia.org/wiki/Garden-path_sentence
*** Determining the groundness of the length of the lists involved in append/3 and append/2
**** Why do we care?
Because we want to write naive parsers that terminate.
**** What?
From the source code of SWI-Prolog, with some modifications:
- http://www.swi-prolog.org/pldoc/doc/_SWI_/library/lists.pl?show=src#append/3

"Ground" here is an adjective, not a noun.
A term is /ground/ iff it has no variables.
A term is non-ground otherwise.

#+BEGIN_SRC prolog
append([], L, L).
append([H|T], L, [H|R]) :-
    append(T, L, R).

append([], []).
append([L|Ls], As) :-
    append(L, Ws, As),
    append(Ls, Ws).
#+END_SRC

We say that a list is /length-ground/ iff its length is ground, and /length-unground/ otherwise.
The elements don't have to be ground.
- The empty list is length-ground.
- A list [_|T] is length-ground iff T is length-ground.
- If a variable gets unified with a length-ground list, then the variable is length-ground.

To analyze length-groundedness, we "reverse" the program.

#+BEGIN_EXAMPLE
% append(T, L, R)
append([], L, L).
append(T, L, R) => append([H|T], L, [H|R]).
#+END_EXAMPLE

(Length-ground = proper list?)

Now we can infer these about append(T, L, R):
- If T = [], then L and R have the same length-groundness.
- The recursive case:
  - Iff T is length-ground, then [H|T] is length-ground.
  - Iff R is length-ground, then [H|R] is length-ground.
- If we want L to be length-ground, then R has to be length-ground.
- Thus we can infer that L and R have the same length-groundness regardless of the length-groundness of T.

If append(A, B, C) succeeds, then:
- If A = [], then B and C have the same length-groundness.
- If two of A, B, C are length-ground, then the other one is length-ground?
- If two of A, B, C are length-unground, then the other one is length-unground?

What?
- 2002 article "Efficient Groundness Analysis in Prolog" https://arxiv.org/abs/cs/0201012
  - https://github.com/pschachte/groundness
**** How do we generate a long list in Prolog, for testing?
***** How do we say "A is a list of 100 equal elements" in Prolog?
*** Naive approach with recognizer / membership predicate
A /recognizer/ is a unary predicate that takes a list of character codes.

Another possible names for recognizer are /acceptor/, /determiner/, /decider/, /membership predicate/.

Example: The following =digit= predicate recognizes ASCII decimal digits.
#+BEGIN_SRC prolog
digit([D]) :- code_type(D, digit).
#+END_SRC

We can build recognizers on other recognizers.
For example, here we use =digit= to define =number_=:
#+BEGIN_SRC prolog
% We append underscore because =number= is a reserved Prolog predicate.
number_([H]) :- digit([H]).
number_([H|T]) :- digit([H]), number_(T).
#+END_SRC

That Prolog knowledge base corresponds to this context-free grammar:
#+BEGIN_SRC
digit ::= <a digit character as defined by Unicode>
number ::= digit | digit number
#+END_SRC

Exercise:
- Here you will compare depth-first search and iterative deepening search, and understand search completeness.
- Try the query =number_(S)=.
- Try the query =length(S,_), number_(S)=.
- If you keep pressing semicolon in the first query, will you ever encounter =S = [48,49]=?
**** A cool thing: recognizers are generators.
The predicate =number_= can be used not only to recognize strings, but also to /generate/ all such strings.
#+BEGIN_SRC prolog
% Press ; to generate the next possibility.
% Press . to stop.
?- length(S,_), number_(S).
#+END_SRC

To understand how that works, we have to understand Prolog backtracking.
**** Left recursion thwarts the naive approach.
Problem:
The following =expression= doesn't terminate.
#+BEGIN_SRC prolog
operator([A]) :- string_codes("+", Ops), member(A, Ops).

expression(E) :- number_(E).
expression(E) :- true
    , append([A, B, C], E)
    , expression(A)
    , operator(B)
    , expression(C)
    .
#+END_SRC

The corresponding context-free grammar is left-recursive:
#+BEGIN_SRC
expression ::= number | expression operator expression
#+END_SRC

We don't want to sacrifice the elegance of the description.
**** Can memoization (tabling) help speed up the naive approach?
No.
**** Another naive approach that works.
This one works.

The key is:
- Put grounding goals first.
  A grounding goal is a goal that grounds its variables.
- Be careful with the pattern =g, u= where =g= generates ungrounded terms and =u= fails,
  because it may cause infinite loop when Prolog backtracks,
  because Prolog continues to generate fresh variables.
  For example, this doesn't terminate:
  #+BEGIN_SRC prolog
  ?- length(L, N), fail.
  #+END_SRC
  - If =p= may generate infinite choice points, then =p, fail= doesn't terminate.

#+BEGIN_SRC prolog
digit([C]) :- code_type(C, digit).

number_([H]) :- digit([H]).
number_([H|T]) :- digit([H]), number_(T).

operator([0'+]).

% expression(Meaning,Codes) may not work if Codes is ungrounded.
expression(number(E), E) :- number_(E).
expression(plus(MA,MC), E) :- true
    , operator(EB) % Put grounding goals first.
    , append([EA,EB,EC], E) % Thus B is grounded.
    , expression(MA,EA)
    , expression(MC,EC)
    .
#+END_SRC
*** Prefix remover / difference-list recognizer / list partitioner
We can turn the naive recognizer =digit/1= into difference-list recognizer =digit/2=.
#+BEGIN_SRC prolog
digit([D]) :- code_type(D, digit).
#+END_SRC

- The first parameter is the input string, say Input.
- The second parameter is the recognized prefix of Input.
- The third parameter is the unrecognized suffix of Input.

In the following, P stands for Parsed, and U stands for Unparsed.

We can turn the recognizer into:
#+BEGIN_SRC prolog
% Prefix remover.
digit([P|U], U) :- code_type(P, digit).

% List partitioner.
digit([P|U], [P], U) :- code_type(P, digit).

% The list partitioner can be derived from the prefix remover:
% digit(U0, P0, U1) :- digit(U0, U1), append(P0, U1, U0).

number_(U0, U1) :- digit(U0, U1).

number_(U0, P0, U1) :- digit(U0, P0, U1).
number_(U0, P2, U2) :- true
    , digit(U0, P0, U1)
    , number_(U1, P1, U2)
    , append(P0, P1, P2)
    .
#+END_SRC

The meaning of =number_(U0, P0, U1)= is:
- P0 is a number.
- P0 is a prefix of U0.
- U0 is the concatenation of P0 and U1.

Observe how we "thread" the state.
The calls in the body follow the pattern =something(U<n>, P<n>, U<n+1>)=.

We can translate a recognizer into a difference-list recognizer.

The cool thing is that each parameter works both ways.
- The query =string_codes("123", A), number_(A, A, [])= asks Prolog to find out whether "123" parses as a number.
- The query =length(A, _), number_(A, A, []).= asks Prolog to find a string that parse as a number.
  You can keep pressing =;= to generate the next strings.
#+BEGIN_SRC prolog
operator([P|U], [P], U) :- string_codes("+", Codes), member(P, Codes).

expression(U0, P0, U1) :- number_(U0, P0, U1).
expression(U0, P0, U1) :- true
    , expression(U0, P0, U1)
    , operator(U1, P1, U2)
    , expression(U2, P2, U3)
    .
#+END_SRC
*** Definite clause grammars
- The DCG clause =left --> right= desugars/expands/translates into the definite clause =left(U0, U1) :- ...= where:
  - U0 is the input.
  - U1 is the suffix of U0 that is not recognized by the DCG clause.
  - The string recognized by the clause is the difference between U0 and U1.
    That string is the P such that U0 = P + U1 where + denotes list concatenation.
- "Interesting Things about Prolog" https://gist.github.com/CMCDragonkai/89a6c502ca7272e5e7464c0fc8667f4d
  - "Definite clause grammars (DCG) make the difference list pattern into a first class primitive with the =-->= operator."
**** Why does this naive DCG fail?
#+BEGIN_SRC prolog
digit --> [Code], {code_type(Code, digit)}.

number -->
    digit, number
;   digit
.

operator --> "+".

expression -->
    number
;   expression, operator, expression
.
#+END_SRC
*** Context-sensitive grammars?
We can add context by adding parameter.
*** Libraries?
- https://github.com/cbaziotis/prolog-cfg-parser
- This isn't Prolog, but this looks awesome https://github.com/Engelberg/instaparse/blob/master/README.md
*** Left recursion
Mathematics handles left recursion just fine.
Computers should too.
We shouldn't chicken out.
We shouldn't compromise by working around our grammar descriptions.
*** Precedence parsing?
- 1996 article "An Operator Precedence Parser for Standard Prolog Text" https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-024X%28199607%2926%3A7%3C763%3A%3AAID-SPE33%3E3.0.CO%3B2-L
** Metainterpreter for left-recursive parsing?
"Parsing with left-recursive grammars"
https://www.metalevel.at/acomip/
** What is left-recursion, and how should we handle it?
*** Should we blame left-recursion on naive operational semantics?
Mathematics has no problem with left-recursion.
Why should computers have problem with left-recursion?
*** Handling left-recursion
Laurent and Mens 2016 \cite{laurent2016taming} (some emphasis ours):
"When a parser invokes itself (either directly or indirectly through intermediate parsers) without intervening state changes, the result is an infinite loop of parser invocations.
This is a well-known problem of top-down recursive parsers, called /left-recursion/.
Fortunately, it can be /mitigated/ as follows:
start by running the left-recursive parser /while failing all left-recursive invocations/, then re-run it, using the result of the initial parse as the result of all left-recursive invocations."

Avoiding left-recursion means always consuming something before recursing.
*** Left-recursive parsing
2009
Direct Left-Recursive Parsing Expressing Grammars
https://www.semanticscholar.org/paper/Direct-Left-Recursive-Parsing-Expressing-Grammars-Tratt/b1e8309db5537fb15f51071fcdc39e139659ed15

2008
Packrat Parsers Can Support Left Recursion

Naive recognizer + memoization

list_not_empty

#+BEGIN_SRC prolog
exp(S) :- is_list(S), append([A,[0'+],C],S), exp(A), exp(C).
#+END_SRC

Consume before recursing?

We can't piggyback Prolog's unification for lambda calculus substitution,
because Prolog unifies same-named variables while lambda-calculus shadows same-named variables.

If the recursive call has smaller arguments than the parent call does, then the predicate should terminate.
** Inconclusive
1997 inconclusive discussion "Prolog Parser in Prolog"
https://dtai.cs.kuleuven.be/projects/ALP/newsletter/archive_93_96/net/grammars/parser.html
** Parsing
"Parsing in Prolog"
http://www.cs.sfu.ca/~cameron/Teaching/383/DCG.html

"Jacc's LR-Parsing with Dynamic Operators"
"This part of the Jacc documentation explains the modifications we can make to a basic table-driven LR parser generator Ã  la yacc to accommodate support for Prolog's dynamic operators."
http://www.hassan-ait-kaci.net/hlt/doc/hlt/jaccdoc/dynamicLR.html
* Conferences
ACM SIGPLAN SLE http://www.sleconf.org/blog/11-20-2013-parsing-at-sle-2013
* Why can't top-down parsers (Prolog DCG) handle left recursion?
Can we fix it by prescribing a different operational semantics?

Should we just use bottom-up parsers?
* Can we extend Brzozowski derivatives to context-sensitive expressions?
Context-free expression is regular expression plus fixed points.

A context-sensitive rule has a left-hand side that may contain more than one non-terminal.
An example of such rule is \(AB \to C\).
* Politics of parsing
This patent (US patent 6449589, "Elimination of left recursion from context-free grammars")[fn::http://www.freepatentsonline.com/6449589.html][fn::https://patents.google.com/patent/US6449589B1/] should not exist?
* Declarative Programming Research Group
** Transitive closure
There are several ways of thinking about transitivity:
relation, logic, graph, fixed-point, and limit.

A relation \(R\) is /transitive/ iff \(\forall x \forall y \forall z [(R(x,y) \wedge R(y,z)) \to R(x,z)]\).

The transitive closure of a relation \(R\) is the smallest transitive superrelation of \(R\).
Such closure is obtained by adding the fewest number of edges to make \(R\) transitive.

The /transitive closure of an arity-2 predicate \(P\)/ is \(T(P)\) where \(T(P,x,y) = P(x,y) \vee \exists i (P(x,i) \wedge T(P,i,y)) \).
The transitive closure of a first-order logic predicate is a first-order logic predicate.
But the transitive closure /operator/ \(T\) is a second-order logic predicate.
Fagin 1974 proves that transitive closure makes first-order logic more expressive.[fn::https://en.wikipedia.org/wiki/Transitive_closure#In_logic_and_computational_complexity]

The /transitive closure of an arity-2 relation \(R\)/ is \(R \cup R_2 \cup R_3 \cup \ldots = \bigcup_{k \in \Nat \ge 1} R_k\) where \(R_k = \underbrace{R \circ \ldots \circ R}_k\).
But this assumes that the relation is countable.
If the relation is finite and its domain has \(n\) elements, then \(k\) does not need to go higher than \(n-1\),
because the shortest path between two connected vertices in that graph will have at most \(n-1\) edges.
Thus \(T(R)\) is the smallest set that satisfies the equation \(T(R) = T(R) \cup R\).
Thus \(T(R)\) is the least fixed point of the function \(A \mapsto (A \cup R)\).

We can also think of transitive closure of \(R\) as the limit \(\lim_{n\to\infty} S_n\)
But this also assumes that the relation is countable.
where \(S_1 = R\) and \(S_{n+1} = (S_n \circ R) \cup R\) and \((B,C,S) \circ (A,B,R) = (A,C,\SetBuilder{(x,z)}{\exists x (R(x,y) \wedge S(y,z))})\).
We can think of \(S_n\) as the set of paths whose length does not exceed \(n\).
** Transitive closure in Prolog
This naÃ¯ve Prolog predicate =t/2= may not terminate if the graph represented by =edge= is cyclic.
Direct translation of the logical formula does not work.
#+BEGIN_EXAMPLE
t(A,B) :- edge(A,B).
t(A,C) :- edge(A,B), t(B,C).
#+END_EXAMPLE

How do we make Prolog smarter so that the above predicate =t/2= terminates?
* Bibliography
