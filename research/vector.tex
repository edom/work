\chapter{Vector}

\section{Vector space}

\index{vector space}%
\index{space!vector}%
A \emph{vector space} over a field \(F\) is a set \(V\) and the
\index{vector space axioms}%
\emph{vector space axioms} \cite{wpvectorspace}\cite{roman2005advanced}:
For all \(a, b \in F\) and \(x, y \in V\):
\begin{enumerate*}[label={(\arabic*)}]
    \item \(V\) forms an additive group,
    \item \(1 x = x\) where \(1\) is the multiplicative identity of \(F\),
    \item \((ab)x = a(bx)\),
    \item \(a(x+y) = ax+ay\),
    \item \((a+b)x = ax+by\).
\end{enumerate*}
Therefore, a vector space \(V\) over a field \(F\) is
an additive group \(V\) and a \emph{scalar multiplication} \(F \to V \to V\).

\index{vector}%
A \emph{vector} is an element of a vector space.

\index{vector!concatenation}%
\index{concatenation!vector}%
\index{vector concatenation}%
The \emph{vector concatenation} of \(a : \Real^m\) and \(b : \Real^n\)
is \(a|b : \Real^p\)
where \(p = m + n\),
\(a|b = (a_1 , \ldots , a_m , b_1 , \ldots , b_n)\),
and a scalar is treated as a vector of length 1.

\index{vector!column}%
\index{column vector}%
A \emph{column vector} of length \(n\) is a \(n \times 1\) matrix.

\index{vectors!orthogonal}%
\index{orthogonal vectors}%
Two vectors \(x\) and \(y\) are \emph{orthogonal} iff \(x \cdot y = 0\).
\index{vectors!parallel}%
\index{parallel vectors}%
Two vectors \(x\) and \(y\) are \emph{parallel} iff \(x \cdot y = \norm{x} \cdot \norm{y}\).

\index{dot product}%
\index{vectors!dot product}%
Relationship between length and dot product: \(\norm{x}^2 = x \cdot x\).
Dot product distributes addition: \(x \cdot (y+z) = x \cdot y + x \cdot z\).
Geometric interpretation of dot product: \(a \cdot b = \norm{a} \cdot \norm{b} \cdot \cos \theta\).

\index{unit vector}%
\index{vectors!unit}%
A \emph{unit vector} is a vector whose length is 1.

\index{vector projection}%
\index{vectors!projection}%
\index{projection!of a vector to another vector}%
The \emph{projection} of \(a\) to \(b\) is \((a \cdot b) \cdot b / |b|^2\).

\section{Matrix}

\index{scalar-matrix multiplication}%
\emph{Scalar-matrix multiplication} is \((ka)_{ij} = k \cdot a_{ij}\).
\index{matrix!addition}%
\index{addition!matrix}%
\emph{Matrix addition} is \((a + b)_{ij} = a_{ij} + b_{ij}\).
\index{matrix!multiplication}%
\index{multiplication!matrix}%
\emph{Matrix multiplication} is \((ab)_{ij} = \sum_{k=1}^n a_{ik} b_{kj}\) where
\(a : R^{m \times n}, b : R^{n \times p}, c : R^{m \times p}\).

\index{coefficient matrix}%
\index{matrix!coefficient}%
\index{system of linear equations}%
\index{unknown}%
A \emph{system of linear equations} is a matrix equation \(A x = b\)
where \(A\) is the \emph{coefficient matrix} and \(x\) is the \emph{unknown}.
\index{overdetermined system of linear equations}%
\index{system of linear equations!overdetermined}%
That system is \emph{overdetermined} iff \(A\) has more rows than columns.

\section{Least-squares solution}

\index{least-squares!solution of an overdetermined system of linear equations}%
\index{system of linear equations!overdetermined!least-squares solution}%
\index{system of linear equations!least-squares solution}%
If \(A : R^{m \times n}\) and \(b : R^m\),
then the \emph{least-squares solution} of \(A x = b\)
is the \(y\) that minimizes \(\norm{A y - b}^2\).
\index{normal equation}%
That \(y\) is also the solution of the corresponding \emph{normal equation}
\((A^T A) y = A^T b\).

\section{Hyperplane}

\index{hyperplane}%
\index{hyperplane!below}%
\index{hyperplane!below-or-on}%
\index{hyperplane!on}%
\index{hyperplane!above}%
\index{hyperplane!above-or-on}%
A \emph{hyperplane}
\(h : \Real^\infty \to \Real\)
is \(h~x = n \cdot (x - p)\)
where \(n\) is the \emph{normal} of \(h\)
and \(p\) is a point on \(h\).
The point \(x\)
is \emph{below} \(h\) iff \( h~x < 0 \),
is \emph{below-or-on} \(h\) iff \( h~x \le 0 \),
is \emph{on} \(h\) iff \( h~x = 0 \),
is \emph{above} \(h\) iff \( h~x > 0 \),
and
is \emph{above-or-on} \(h\) iff \( h~x \ge 0 \).

\index{hyperplane equation!matrix form}%
\index{matrix form of hyperplane equation}%
The \emph{matrix form} of the hyperplane equation \(f~x = a \cdot x + b\)
is \(f~x = (a|b)^T (x|1)\).

The \emph{distance} of a point \(x\) to hyperplane \(h = n \cdot (x - p)\)
is the length of the projection of \(x-p\) to \(n\).

\section{Matrix unary operations}

The
\index{transpose}%
\index{transpose!of matrix}%
\index{matrix!transpose}%
\emph{transpose} of \(M\) is \((M^T)_{ij} = M_{ji}\).

The
\index{conjugate transpose}%
\index{conjugate transpose!of matrix}%
\index{transpose!conjugate}%
\index{matrix!conjugate transpose}%
\emph{conjugate transpose} of \(M\) is \((M^*)_{ij} = (M_{ji})^*\).

\section{Special matrices}

\index{identity matrix}%
\index{matrix!identity}%
A matrix \(I : \Real^{n \times n}\) is \emph{identity} iff
\(\forall (A : \Real^{n \times n}) (IA = AI = A)\).
The \emph{\(n\times n\) identity matrix} is
\((I_n)_{ij} = \delta_{ij}\) where \(\delta\) is the
\index{Kronecker delta}%
Kronecker delta \(\delta_{ij} = [i=j]\).

A matrix \(M\) is
\index{unitary matrix}%
\index{matrix!unitary}%
\emph{unitary} iff \(M^*M = MM^* = I\)
where \(I\) is an identity matrix.

\section{Singular value decomposition}

The
\index{singular value decomposition}%
\index{matrix decomposition!singular value}%
\emph{singular value decomposition} of \(M\) is \(U S V^* = M\) where \(V^*\) is the conjugate transpose of \(V\).

\section{QR decomposition}

\(M\) is an
\index{triangular matrix}%
\index{matrix!triangular}%
\emph{upper triangular matrix} iff ...

\(M\) is an
\index{orthogonal matrix}%
\index{matrix!orthogonal}%
\emph{orthogonal matrix} iff ...

The
\index{QR decomposition}%
\index{matrix decomposition!QR}%
\emph{QR decomposition} of \(M\) is \(M = QR\) where
\(Q\) is an orthogonal matrix and
\(R\) is an upper triangular matrix.
