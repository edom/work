\chapter{Vector}

\section{Vector space as Cartesian power of ring}

A ring \(R\) give rises to an
\index{vector space}%
\emph{\(n\)-dimensional vector space} \(R^n\) and its vector operations.
Each element of \(R^n\) is a
\index{vector}%
\emph{vector} \((x_1,\ldots,x_n)\)
where each \(x_k \in R\).
We say that \(R^n\) is a vector space \emph{over} ring \(R\).

\paragraph{Operations}
Let \(a \in R\) and \(x,y\in R^n\).
The
\index{scalar multiplication}%
\index{multiplication!scalar and vector}%
\index{vector!scalar multiplication}%
\emph{scalar multiplication} of \(a\) and \(x\) is \(ax \in R^n\) where \((ax)_k = a x_k\).
The
\index{vector!addition}%
\index{addition!vector}%
\emph{vector addition} of \(x\) and \(y\) is \((x+y) \in R^n\) where \((x+y)_k = x_k+y_k\).
The
\index{inner product!vector}%
\index{vector!inner product}%
\emph{inner product} of \(x\) and \(y\) is \(\langle x,y \rangle = x \cdot y = \sum_{k=1}^n x_k y_k\).

The \emph{\(i\)th standard basis} is \(e_i\) where \((e_i)_k = \delta_{ik}\)
where \(\delta_{ik} = 1_R\) iff \(i=k\) and \(0_R\) otherwise.
\(0_R\) is the additive identity of \(R\)
and \(1_R\) multiplicative identity of \(R\).

% FIXME use standard terms
Such vector space is also an inner product space.
Such vector space is the \emph{natural \(n\)-vectorization of \(R\)}.
The \emph{component ring} of that vector space is \(R\).

\section{Dual vector space}

Let \(V\) be a vector space over ring \(R\).

The
\index{dual!vector space}%
\index{vector space!dual}%
\emph{dual} of \(V\) is \(V^* = V \to R\).
A
\index{covector}%
\emph{covector} is an element of \(V^*\).
Another names for covector are
\emph{linear form}, \emph{linear functional}, and \emph{one-form}.

\(V^*\) is also a vector space
where scalar multiplication is
\((cf)(x) = c \cdot f(x)\)
and vector addition is
\((f+g)(x) = f(x)+g(x)\)
where \(c \in R\) and \(f,g \in V^*\).
\(f(x) = \langle a,x\rangle\) defines a bijection involving \(f:V^*\) and \(a:V\).

A \emph{linear map} is a function \(f\) such that \(f(x+y) = f(x) + f(y)\)
and \(f(c \cdot x) = c \cdot f(x)\) where \(c\) is a scalar.
A linear map \(T : R^m \to R^n\) can be thought
as a vector of covectors \(t : (R^m \to R)^n\) where
\(
(T(x))_i = t_i(x) = \langle a_i, x \rangle
\).
There is a bijection between matrices and linear maps:
\(Mx = T(x)\) where
\[
M = \begin{bmatrix}\tp(a_1)\\\vdots\\\tp(a_m)\end{bmatrix}
= \begin{bmatrix}a_{11} & \ldots & a_{1m} \\ \vdots & \ddots & \vdots \\ a_{m1} & \ldots & a_{mm}\end{bmatrix}
\]
where \(\tp(x)\) is the transpose of \(x\).
Iff $m = n$, then \(T\) is an \emph{operator}.

The linear combination of \(n\) vectors in the same \(n\)-dimensional vector space:
\(x\) is a linear combination of \(e_1,\ldots,e_n\)
iff there exists \(a = (a_1,\ldots,a_n)\)
such that \(x = a_1 \cdot e_1 + \ldots + a_n \cdot e_n\)
where each \(a_k\) is a scalar and each \(e_k\) is a vector.
We can also write the latter equation as
\[
x = E~a = a_1 \cdot e_1 + \ldots + a_n \cdot e_n,
\]
which we can also spell out into $n$ equations, each like this:
\[
x_i = E_i~a = a_1 \cdot (e_1)_i + \ldots + a_n \cdot (e_n)_i,
\]
which suggests that each $E_i$ is a covector
(you should be able to show that $E_i$ is linear).
We can also write the equation using matrices:
\[
x =
\begin{bmatrix}
e_1 & \ldots & e_n
\end{bmatrix}
\begin{bmatrix}
a_1 \\ \vdots \\ a_n
\end{bmatrix}
\]
which we can write out in full as
\[
x =
\begin{bmatrix}
e_{11} & \ldots & e_{n1}
\\ \vdots & \ddots & \vdots
\\ e_{1n} & \ldots & e_{nn}
\end{bmatrix}
\begin{bmatrix}
a_1 \\ \vdots \\ a_n
\end{bmatrix}
\]
and thus $E~a = ea$,
where $e_k$ becomes the $k$th column of $e$.
We say that $e$ is the matrix corresponding to $E$.
This is an example of using matrix multiplication to specify
a linear combination of a set of vectors;
this matrix equation summarizes $n$ equations into one equation.
Thus there is an isomorphism between a linear endofunction in a vector space
and multiplication of a matrix and a column vector;
every such endofunction has a corresponding matrix.

\section{Vector space over a field}

\index{vector space}%
\index{space!vector}%
A \emph{vector space} over a field \(F\) is a set \(V\) and the
\index{vector space axioms}%
\emph{vector space axioms} \cite{wpvectorspace}\cite{roman2005advanced}:
For all \(a, b \in F\) and \(x, y \in V\):
\begin{enumerate*}[label={(\arabic*)}]
    \item \(V\) forms an additive group,
    \item \(1 x = x\) where \(1\) is the multiplicative identity of \(F\),
    \item \((ab)x = a(bx)\),
    \item \(a(x+y) = ax+ay\),
    \item \((a+b)x = ax+by\).
\end{enumerate*}
Therefore, a vector space \(V\) over a field \(F\) is
an additive group \(V\) and a \emph{scalar multiplication} \(F \to V \to V\).

\index{vector}%
A \emph{vector} is an element of a vector space.

\index{vector!concatenation}%
\index{concatenation!vector}%
\index{vector concatenation}%
The \emph{vector concatenation} of \(a : \Real^m\) and \(b : \Real^n\)
is \(a|b : \Real^p\)
where \(p = m + n\),
\(a|b = (a_1 , \ldots , a_m , b_1 , \ldots , b_n)\),
and a scalar is treated as a vector of length 1.

\index{vector!column}%
\index{column vector}%
A \emph{column vector} of length \(n\) is a \(n \times 1\) matrix.

\index{vectors!orthogonal}%
\index{orthogonal vectors}%
Two vectors \(x\) and \(y\) are \emph{orthogonal} iff \(x \cdot y = 0\).
\index{vectors!parallel}%
\index{parallel vectors}%
Two vectors \(x\) and \(y\) are \emph{parallel} iff \(x \cdot y = \norm{x} \cdot \norm{y}\).

\index{dot product}%
\index{vectors!dot product}%
Relationship between length and dot product: \(\norm{x}^2 = x \cdot x\).
Dot product distributes addition: \(x \cdot (y+z) = x \cdot y + x \cdot z\).
Geometric interpretation of dot product: \(a \cdot b = \norm{a} \cdot \norm{b} \cdot \cos \theta\).

\index{unit vector}%
\index{vectors!unit}%
A \emph{unit vector} is a vector whose length is 1.

\index{vector projection}%
\index{vectors!projection}%
\index{projection!of a vector to another vector}%
The \emph{projection} of \(a\) to \(b\) is \((a \cdot b) \cdot b / |b|^2\).

\section{Matrix}

\index{scalar-matrix multiplication}%
\emph{Scalar-matrix multiplication} is \((ka)_{ij} = k \cdot a_{ij}\).
\index{matrix!addition}%
\index{addition!matrix}%
\emph{Matrix addition} is \((a + b)_{ij} = a_{ij} + b_{ij}\).
\index{matrix!multiplication}%
\index{multiplication!matrix}%
\emph{Matrix multiplication} is \((ab)_{ij} = \sum_{k=1}^n a_{ik} b_{kj}\) where
\(a : R^{m \times n}, b : R^{n \times p}, c : R^{m \times p}\).

\index{coefficient matrix}%
\index{matrix!coefficient}%
\index{system of linear equations}%
\index{unknown}%
A \emph{system of linear equations} is a matrix equation \(A x = b\)
where \(A\) is the \emph{coefficient matrix} and \(x\) is the \emph{unknown}.
\index{overdetermined system of linear equations}%
\index{system of linear equations!overdetermined}%
That system is \emph{overdetermined} iff \(A\) has more rows than columns.

\section{Basis}

A
\index{basis!of vector space}%
\index{vector space!basis}%
\emph{basis} of an \(n\)-dimensional vector space \(V\) is a set of vectors \(\{e_1,\ldots,e_n\}\) where each \(e_k \in V\)
such that for every \(v \in V\) there is \(a \in V\) such that \(v = \sum_{k=1}^n a_k e_k\).

The
\index{standard basis}%
\index{natural basis}%
\emph{standard basis} for \(R^n\) is \(\{ e_1, \ldots, e_n\}\) where
\( (e_k)_i = [ k = i ] \).
It is also called \emph{natural basis}.

\section{Matrix as linear operator}

The
\index{span (of a matrix)}%
\emph{span} of \(A \in \Real^{n\times n}\) is $\fspan(A) = \{ Ax ~|~ x \in \Real^n \}$.
If we treat \(A\) as a linear functional,
then \(\fspan(A)\) is the range of \(A\).

\section{Least-squares solution}

\index{least-squares!solution of an overdetermined system of linear equations}%
\index{system of linear equations!overdetermined!least-squares solution}%
\index{system of linear equations!least-squares solution}%
If \(A : R^{m \times n}\) and \(b : R^m\),
then the \emph{least-squares solution} of \(A x = b\)
is the \(y\) that minimizes \(\norm{A y - b}^2\).
\index{normal equation}%
That \(y\) is also the solution of the corresponding \emph{normal equation}
\((A^T A) y = A^T b\).

\section{Hyperplane}

\index{hyperplane}%
\index{hyperplane!below}%
\index{hyperplane!below-or-on}%
\index{hyperplane!on}%
\index{hyperplane!above}%
\index{hyperplane!above-or-on}%
A \emph{hyperplane}
\(h : \Real^\infty \to \Real\)
is \(h~x = n \cdot (x - p)\)
where \(n\) is the \emph{normal} of \(h\)
and \(p\) is a point on \(h\).
The point \(x\)
is \emph{below} \(h\) iff \( h~x < 0 \),
is \emph{below-or-on} \(h\) iff \( h~x \le 0 \),
is \emph{on} \(h\) iff \( h~x = 0 \),
is \emph{above} \(h\) iff \( h~x > 0 \),
and
is \emph{above-or-on} \(h\) iff \( h~x \ge 0 \).

\index{hyperplane equation!matrix form}%
\index{matrix form of hyperplane equation}%
The \emph{matrix form} of the hyperplane equation \(f~x = a \cdot x + b\)
is \(f~x = (a|b)^T (x|1)\).

The \emph{distance} of a point \(x\) to hyperplane \(h = n \cdot (x - p)\)
is the length of the projection of \(x-p\) to \(n\).

\section{Matrix unary operations}

The
\index{transpose}%
\index{transpose!of matrix}%
\index{matrix!transpose}%
\emph{transpose} of \(M\) is \((M^T)_{ij} = M_{ji}\).

The
\index{conjugate transpose}%
\index{conjugate transpose!of matrix}%
\index{transpose!conjugate}%
\index{matrix!conjugate transpose}%
\emph{conjugate transpose} of \(M\) is \((M^*)_{ij} = (M_{ji})^*\).

\section{Special matrices}

\index{identity matrix}%
\index{matrix!identity}%
A matrix \(I : \Real^{n \times n}\) is \emph{identity} iff
\(\forall (A : \Real^{n \times n}) (IA = AI = A)\).
The \emph{\(n\times n\) identity matrix} is
\((I_n)_{ij} = \delta_{ij}\) where \(\delta\) is the
\index{Kronecker delta}%
Kronecker delta \(\delta_{ij} = [i=j]\).

A matrix \(M\) is
\index{unitary matrix}%
\index{matrix!unitary}%
\emph{unitary} iff \(M^*M = MM^* = I\)
where \(I\) is an identity matrix.

\section{Singular value decomposition}

The
\index{singular value decomposition}%
\index{matrix decomposition!singular value}%
\emph{singular value decomposition} of \(M\) is \(U S V^* = M\) where \(V^*\) is the conjugate transpose of \(V\).

\section{QR decomposition}

\(M\) is an
\index{triangular matrix}%
\index{matrix!triangular}%
\emph{upper triangular matrix} iff ...

\(M\) is an
\index{orthogonal matrix}%
\index{matrix!orthogonal}%
\emph{orthogonal matrix} iff ...

The
\index{QR decomposition}%
\index{matrix decomposition!QR}%
\emph{QR decomposition} of \(M\) is \(M = QR\) where
\(Q\) is an orthogonal matrix and
\(R\) is an upper triangular matrix.
