\chapter{Old computation book draft}

\theoremstyle{definition}
\newcounter{thmctr}
\newtheorem{mdef}[thmctr]{Definition}
\newtheorem{mque}[thmctr]{Question}
\newtheorem{mcon}[thmctr]{Conjecture}
\newtheorem{msco}[thmctr]{Strong Conjecture}
\newtheorem{mcor}[thmctr]{Corollary}
\newtheorem{mlem}[thmctr]{Lemma}
\newtheorem{mthm}[thmctr]{Theorem}
\newcommand\sno{\ensuremath{\mathcal N}}
\newcommand\syes{\ensuremath{\mathcal Y}}
\newcommand\Fin{\ensuremath{\operatorname{Fin}}}
\newcommand\powerset{\ensuremath{\mathcal{P}}}
\newcommand\fa[1]{\forall#1\,}
\newcommand\Fa[1]{\forall#1~}
\newcommand\FA[1]{\forall#1~~}
\newcommand\sfT{\mathsf{T}}
\newcommand\Typ{\mathsf{Typ}}
\newcommand\Either[2]{\mathsf{Either}~#1~#2}
\newcommand\Left[1]{\mathsf{Left}~#1}
\newcommand\Right[1]{\mathsf{Right}~#1}
\newcommand\Void{\mathsf{Void}}
\newcommand\Pair[2]{\mathsf{Pair}~#1~#2}
\newcommand\UDMach[1]{\mathsf{UDMach}~#1}
\newcommand\DMach[1]{\mathsf{DMach}~#1}
\newcommand\NMach[1]{\mathsf{NMach}~#1}
\newcommand\Mach[2]{\mathsf{Mach}~#1~#2}
\newcommand\Macha[1]{\mathsf{Mach}~#1}
\newcommand\RecRel{\mathsf{RecRel}}
\newcommand\Kleene[1]{\mathsf{Kleene}~#1}
\newcommand\RecFun{\mathsf{RecFun}}
\newcommand\DTM{\mathsf{DTM}}
\newcommand\NTM{\mathsf{NTM}}
\newcommand\sfDMach{\mathsf{DMach}}
\newcommand\sfNMach{\mathsf{NMach}}
\newcommand\sfFun{\mathsf{Fun}}
\newcommand\sfSet{\mathsf{Set}}
\newcommand\sfRel{\mathsf{Rel}}
\newcommand\sfVec{\mathsf{Vec}}
\newcommand\Vect[2]{\sfVec~#1~#2}
\newcommand\Relab[2]{\sfRel~#1~#2}
\newcommand\Fun[2]{\sfFun~#1~#2}
\newcommand\sfPred{\mathsf{Pred}}
\newcommand\Pred[1]{\sfPred~#1}
\newcommand\setB{\mathbb B}
\newcommand\decset{\mathcal D}
\newcommand\langset{\mathcal L}
% time complexity
\newcommand\TC{\mathcal{T}}
\newcommand\leTC{\ensuremath{\le_\TC}}
% time equivalence class
\newcommand\Teq{\mathbb{T}}
\newcommand\Teqsum{\mathbf{T}}
% space equivalence class
\newcommand\Seq{\mathbb{S}}
\newcommand\Seqsum{\mathbf{S}}
\newcommand\SC{\mathcal{S}}
% \newcommand\hom{\operatorname{hom}}
\newcommand\fite[3]{\text{if}~#1~\text{then}~#2~\text{else}~#3}
\newcommand\id{\ensuremath{\operatorname{id}}}
\newcommand\mE{\ensuremath{\mathcal E}}
\newcommand\mP{\ensuremath{\mathcal P}}
\newcommand\mM{\ensuremath{\mathcal M}}
\newcommand\mL{\ensuremath{\mathcal L}}
\newcommand\bB{\ensuremath{\mathbb B}}
\newcommand\amb{\operatorname{amb}}
\newcommand\ambc{\operatorname{ambc}}
\newcommand\fhead{\ensuremath{\operatorname{head}}}
\newcommand\ftail{\ensuremath{\operatorname{tail}}}
\newcommand\fcons{\ensuremath{\operatorname{cons}}}
\newcommand\aTIME{\operatorname{\alpha-TIME}}
\newcommand\idTIME{\operatorname{id-TIME}}
\newcommand\mPTIME{\operatorname{\mP-TIME}}
\newcommand\TIME{\operatorname{\mathsf{TIME}}}
\newcommand\DTIME{\operatorname{\mathsf{DTIME}}}
\newcommand\NTIME{\operatorname{\mathsf{NTIME}}}
\newcommand\EXPTIME{\ensuremath{\mathsf{EXP}}}
\newcommand\PTIME{\ensuremath{\mathsf{P}}}
\newcommand\NPTIME{\ensuremath{\mathsf{NP}}}
\newcommand\NSPACE{\operatorname{NSPACE}}
\newcommand\DSPACE{\operatorname{DSPACE}}
\newcommand\SDP{\ensuremath{\operatorname{SD}}}

\section{TODO}

cite CMI problem description;
understand Baker-Gill-Solovay (?) theorem and relativization;
read Rogers \cite{Rogers1987};
read Arora and Barak \cite{Arora2009};
read Marek and Remmel \cite{Marek2009};
read Boolos, Burgess, and Jeffrey \cite{Boolos2002}

\section{Diary}

In this document, I use the term \emph{predicate} to always mean
a total function that takes a bit string and gives a bit.
Furthermore, if $f$ is a predicate, then
I may write ``$x$ \emph{satisfies} $f$'' to mean $f(x) = 1$, and
I may write ``$f$ is \emph{satisfiable}'' to mean
that there exists $x$ that satisfies $f$.

\newcommand\encoding[1]{\langle#1\rangle}

I write $\encoding{x}$ to mean a binary encoding of $x$.
I write $\encoding{x,y}$ to mean a binary encoding of $x$
concatenated with a binary encoding of $y$.

The ATMB problem is ``Given a machine $m$, a string $x$,
and a number $n$,
does $m$ accept $x$ in $n$ steps or less?''
This problem is exptime-complete with respect to the length of
$\encoding{m,x,n}$. (cite?)

The bounded halting problem is:
``Given a machine $m$ and a number $n$,
does $m$ accept any string that is at most $n$ bits long?''
This is np-complete? (cite?)
% http://www.ics.uci.edu/~eppstein/161/960312.html
% sat-t-icalp.ps

\begin{enumerate}
\item Show that it is in exptime.
\item Show that it is in exptime-hard.
(This implies that it is not in p.
Show that it is polynomial-time many-one reducible to ATMB?
Use diagonalization perhaps?)
\item Show that it is in np.
\item Congratulations.
\end{enumerate}

\section{Defining computation}

Difficulty is because the tape head may move in two directions.
Graph reduction always proceeds in one direction.
Post-Turing machine is simpler than Turing machine.
Wang B-machine is even simpler.

Here I introduce the macroscopic model of computation:
a computation is a repeated application of a function $f$
until the output does not change anymore.
The computation begins with the input $x$ already in place.
I write that such computation produces the output $y$
in $n$ steps iff $n$ is the smallest number such that $f^{n-1}(x) = f^n(x) = y$
where $f^n(x)$ means $n$ times applying $f$ to $x$.
This function can be thought a transition function that describes
how the entire system (machine state and tape content) changes.
A machine can be seen as a function.

This is a special case of fractal? Chaotic function?

The zero function $f(x) = 0$ corresponds to a machine
that always accepts its input.

The function $f(x) = 2 \cdot x$ corresponds to a machine
that appends a zero bit to the memory, one by one.

One-instruction-set computer.
Subleq.

What is the essence of computation?
Is it recursion?

Using graph reduction as computational model
may make the problem easier to solve.

This has to do with time-constructibility?

\section{Finding the problem}

Now I try to craft a problem that nondeterministic machines
can solve exponentially faster than deterministic machines can best do.
I am looking for a problem whose best deterministic solution
is an exhaustive search.
Formally, I am looking for a problem that is dexptime-hard but also in np.

I feel that this problem is promising:
``Find a string that satisfies a given satisfiable ptime-predicate,''
where I use the term \emph{ptime-predicate} to mean that
a machine can compute the predicate in an amount of time
that is a polynomial of the number of bits in the input string.

However, there is also an exptime-complete problem:
given a machine and a number $k$, determine if the machine
halts in at most $k$ steps.

What if I combine those problems into this problem:
``Given a ptime-predicate $p$ and a natural number $n$,
determine whether there is an $n$-bit string satisfying $p$?''
Showing that this problem is bounded below by dexptime
and bounded above by np would prove that p and np are not equal.

The input is $(p,n)$.
The output is a bit.

If the problem is to be in ptime,
the length of the string representation of $p$
must be a polynomial of $n$.

Suppose there exists $k$ such that the length of the input is in $\Theta(n^k)$.

Let the input be $(k,p,n)$?

The following question is a generalization of the p vs np problem:
Given the same amount of resources (space and time),
what is the maximum ratio of the number of functions that NTMs can compute
to the number of functions than DTMs can compute?
Informally, what is the maximum speedup
that nondeterminism can give a TM?
The answer of this question will answer the p vs np question.

How do I prove that a problem doesn't have better algorithm than brute force?
I have to see the paper that proves something is dexptime-complete.
Pebble game problem? Kolaitis and Panttaja?
Does that a problem can only be brute-forced at best
depend on the model of computation?
Show that the existence of faster algorithm raises a contradiction?

Should I switch from DTM and NTM to ATM (alternating Turing machine)?
Should I abandon TMs and use pebble games instead?
How about Boolean circuits?

The following is a nondeterminstic algorithm that
builds the string that will satisfy such predicate.
\begin{enumerate}
\item Let the current string be empty.
\item Either skip this step,
or pick a bit and append it to the current string.
\item If the current string satisfies the predicate, halt.
\item Otherwise go to step 2.
\end{enumerate}
The running time of that algorithm is a function of the length
of the shortest string that satisfies the predicate.
Let $s~p$ be the length of the shortest string that satisfies the predicate $p$.
We have just shown that the problem is in $\NTIME(O(s(p)))$.

Now, if we can show that for that problem
there is no better deterministic algorithm than the exhaustive search
(that is if the problem is exptime-hard),
then we prove that P and NP are not equal.

The size of the search space of all deterministic algorithms
that run in $n$ steps is smaller than...
$\DTIME(2^{s(p)})$.

Can the algorithm use any information from
the string representation of the predicate?

Suppose that there is a deterministic algorithm that can
always find an answer in polynomial time.

How many satisfiable predicates are there?
Let $c(n)$ be the number of predicates that can be satisfied
by an input that is not longer than $n$ bits.
\begin{align}
    c(0) &= 2^{2^0} - 1 = 1
    \\
    c(1) &= 2^{2^0 + 2^1} - 1 = 7
    \\
    c(2) &= 2^{2^0 + 2^1 + 2^2} - 1 = 127
    \\
    c(3) &= 2^{2^0 + 2^1 + 2^2 + 2^3} - 1 = 32,767
    \\
    c(n) &= 2^{2^{n+1}-1} - 1
\end{align}

I use the term \emph{$n$-least predicate} to mean that the predicate
can be satisfied by a string that is no longer than $n$ bits.

There exists an $n$-least predicate.

If two predicates differ, then their encodings will also differ.

A predicate can be encoded as a base-2 representation real number
between 0 inclusive and 1 exclusive.
The uncountability of real number implies that there is no algorithm
for determining whether any arbitrary predicate is satisfiable.

However, a predicate can also be encoded as a natural number
(by G\"odel numbering for example).
Natural numbers are countable.

\section{Introduction}

What does it mean for a function to be \emph{computable} or \emph{recursive}?

Constant function $\forall x ~ f(x) = a$.

Roughly speaking, we say that a function is computable
if and only if it can be stated as a finite arrangement
of certain primitive operations we allow.

For example, if we only allow the constant function,
then the machine will either accept all input or reject all input.

\section{First attempt}

A \emph{recursive predicate} is a function having type $\mathbb N \to \{0,1\}$.
There is a bijection between $\{0,1\}^*$ and $\mathbb N$
so every function having type $\{0,1\}^* \to \{0,1\}$ can also be considPred{a} recursive predicate.
We say that $x$ \emph{satisfies} $p$ iff $px = 1$.
We say that $p$ is \emph{satisfiable} iff there exists $x$ that satisfies $p$.

The \emph{$p$-satisfaction problem}
and asks for the smallest $x$ that satisfies a given predicate $p$.
A related decision problem is the \emph{$p$-satisfiability problem} that
asks whether a given predicate $p$ is satisfiable.
This satisfiability problem allows us to show that $\PTIME \neq \NPTIME$
by constructing a satisfiable recursive predicate
$p$ in $\DTIME(\Theta n)$ such that
the corresponding $p$-satisfiability is in both $\NTIME(On)$ and $\DTIME(\Omega 2^n)$
where $n$ is the length of the shortest string that satisfies $p$.

An \emph{alphabet} is a finite countable non-empty set.

Many things can be \emph{recursive} or \emph{computable}: sets, functions, languages.

Blum \cite{Blum1967} defined a machine-independent complexity measure?
Blum speedup theorem implies $\EXPTIME = \PTIME$?

Rabin \cite{Rabin1977}?

Chow \cite{Chow1976} introduces the theory concisely.

Who? shows that partial computable functions are isomorphic to natural numbers.
This allows category theory to be used on computability theory.
An algorithm is a natural number.
An algorithm for simulating an algorithm: $\mathbb N \to \mathbb N$.
G\"odel numbering of partial recursive functions.
Formal systems.
Relates completeness, consistency, computability, decidability.

We use typed lambda calculus as our model of computation.

Informally, we construct a function $f$ such that $f$ is easy but $\SDP f$ is hard
such that there is no faster deterministic algorithm
than trying every possible subsequence.
A list of length $n$ has at most $2^n$ subsequences.

There is a bijection between $X^*$ and $\mathbb N$.
the empty string, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, 100, 101, 110, 111
\begin{align}
    \varphi [] &= 0
  \\ \varphi x &= \sum_{k=0}^{\mu x - 1} 2^k + \sum_{k=0}^{\mu x - 1} x_k \cdot 2^k
            \\ &= 2^{\mu x} - 1 + \sum_{k=0}^{\mu x - 1} x_k \cdot 2^k
\end{align}

Time hierarchy theorem?

The deterministic algorithm can use iterative deepening depth-first search:
\begin{align}
    sf &= \text{any $($map $f$ $\xi)$}
    \\
    \xi &= \text{fix} n []
    \\
    nx &= \text{map} (0:) x ++ \text{map} (1:) x
    \\
\end{align}
The nondeterministic algorithm:
\begin{align}
    sf &= gf[]
    \\
    gfx &= fx \vee \ambc (gf(0:x)) (gf(1:x))
\end{align}

\section{Type theory}

\section{Motivation}

The type theory described here is inspired by
Martin-L\"of type theory
and the Haskell programming language.
The type theory is constructive (intuitionistic) and dependent.
This theory shall help us elucidate ideas that
would be otherwise cumbersome using set theory.
Perhaps this theory can translate the $\PTIME$ vs $\NPTIME$ problem
into something easier.
This theory will be the foundation
of the next chapters.

\section{Notation}

Iff \(a\) is a type then \(|a|\) is the \emph{cardinality} of \(a\).
It is the number of inhabitants of that type.

\section{Common types}

\begin{itemize}
    \item
        $\Void$ is the empty type.
        It has no inhabitant.
    \item
        $\Bool$ is the type of boolean values.
        This type has two inhabitants: $\true$ and $\false$.
    \item
        $\Bit$ has two inhabitants: $0$ and $1$.
    \item
        $\Fun{a}{b}$ is the type of functions from $a$ to $b$.
        The function can be partial.
        This type is also be written $a \to b$.
    \item
        $\Pred{a}$ is the type of
        logical predicates about objects of type $a$.
        We define
        \[ \Pred{a} = \Fun{a}{\Bool}. \]
    \item
        $\Nat$ is the type of natural numbers.
        This type is defined using Peano axioms.
    \item
        $\Set{a}$ is the type of the set of elements of type $a$.
    \item $\List{a} = \Kleene{a}$ is the Kleene closure of $a$.
        A list of $a$ is an ordered collection of elements of type $a$ with duplicates allowed.
        so we can write $[x,y,z] 1 = y$.
    \item $\Bits$ is the type of bitstrings.
        \[
            \Bits = \Kleene{\Bit}
        \]
    \item $\Either{a}{b}$ is the sum type or the union type
        that consists of $\Left{x}$ for all $x : a$
        and $\Right{y}$ for all $y : b$.
    \item $\Pair{a}{b}$ is the product type
        that consists of $(x,y)$ for all $x : a$ and $y : b$.
    \item
        $\Vect{a}{n}$ is the type of vectors
        whose element type is $a$ and whose length is $n : \Nat$:
        \[ \Vect{a}{n} = \Fun{(I n)}{a} \]
        where
        \[ I n = \{ 0, 1, 2, \ldots, n - 1 \} \]
        or alternatively using predicate logic
        \[ \Fa{x} (x : \Nat \wedge x < n \iff x : I n) \]
    \item $\Typ$ is the type of types.
        This implies
        \[ \Typ : \Typ \]
        (Does $\Typ : \Typ$ imply inconsistency?
        Russell's paradox?
        Unrestricted comprehension?)

        This means that $\sfSet$ and $\sfRel$ can be thought as the \emph{type functions}
        \begin{align*}
            \sfSet &: \Typ \to \Typ,
            \\
            \sfRel &: \Typ \to \Typ \to \Typ.
        \end{align*}
\end{itemize}

The cardinality of $\Nat$ is $\aleph_0$ (aleph-null).

\section{Cardinality of types}

$|\List{a}| = |\Fun{\Nat}{a}|$.

\(|a \to b| = |b|^{|a|}\)

If a type has finitely many inhabitants,
then the cardinality of that type is the number of its inhabitants.

If there is a bijection between two types,
then those types have the same cardinality.

Type-theoretic restatement of Cantor's theorem?
There is no bijection between $a$ and $\Set{a}$.
$|a| < |\Set{a}|$.

Kind of ordering on cardinalities:
\begin{itemize}
    \item Iff there is an injection from $a$ to $b$, then $|a| \le |b|$.
    \item Iff there is an surjection from $a$ to $b$, then $|a| \ge |b|$.
    \item Iff there is a bijection between $a$ and $b$, then $|a| = |b|$.
\end{itemize}

Two sets are equinumerous (have the same cardinality) if and only if there is a bijection between them.

$|\sfT a| = |\sfT (\Set{a})|$?

\section{Cardinality theorems}

\begin{mthm}
    \[
        |a| \lneq |\Set{a}|
    \]
\begin{proof}
    Has been proved by Georg Cantor using Cantor's diagonal argument
    that the cardinality of a set is strictly less than its power set.
    Beth numbers.
    $\beth_n \lneq \beth_{n+1}$ for each natural number $n$.
\end{proof}
\end{mthm}

\begin{mthm}[Equinumerosity among one-parameter types]
    For each $a$, all these types have the same cardinality:
    $\Pred{a}$, $\Set{a}$.
    \begin{proof}
        Let $p : \Pred{a}$ be a predicate and $s : \Set{a}$ be a set.
        We define $p$ and $s$ such that each object that satisfies the predicate $p$ is an element of the set $s$
        and also such that each element of the set $s$ satisfies the predicate $p$.
        \begin{align*}
            F p &= \{ x \,|\, p x \}
            \\
            G s &= \lambda x \to x \in s
        \end{align*}
        The relationship is
        \[ \FA{x} (p x \iff x \in s) \]

        But what if $p x = x \not\in S$.
        Or what if $p x = \neg\exists S ~ x \in S$?
        Or what if $p x = \Fa{S} x \in S$?
        Or what if $p x = x \in x$?
        What if $p x = \neg (p x)$?
        Isn't this prone to Russell's paradox?
        Unrestricted comprehension?
        FIXME?

        Or is this not prone?
        $p$ cannot refer to $s$?
        Can it?
    \end{proof}
\end{mthm}

Thus a predicate is a set and a set is a predicate.
It turns out that there is a name for this concept:
that set is the \emph{extension} of that predicate.
If $p$ is a predicate, then $p$ is also a set,
so we can write $x \in p$ to mean that $p x$ is true.
What if we assume that a predicate is equal to its own extension?
Now we make a bold but reasonable claim:
a predicate \emph{is} a set and a set \emph{is} a predicate.
This has some interesting consequences.

If we assume the equality, then $p$ becomes a fixed point of $\phi \mu$.
To see this, we have to define several functions.
Let $\phi$ be the flip combinator, that is $\phi f x y = f y x$.
Let $\mu$ be the set membership function, that is $\mu x y = x \in y$.
Recall that the $\eta$-reduction transforms $p x = q x$ to $p = q$.
\begin{align*}
    p x &= x \in s
    \\
    &= \mu x s
    \\
    &= \phi \mu s x
    \\
    p &= \phi \mu s
    \\
    p &= s
    \\
    p &= \phi \mu p
    \\
    p &= \phi \mu (\phi \mu p)
    \\
    &= \phi \mu (\phi \mu (\phi \mu p))
    \\
    &= \ldots
\end{align*}
That implies that we can write strange but provable things like these:
\begin{align*}
    1 \in \{0,1,2\} &= \{0,1,2\} 1 = \true
    \\
    3 \in \{0,1,2\} &= \{0,1,2\} 3 = \false
    \\
    (\lambda x \to x = 1) 1 &= 1 \in (\lambda x \to x = 1) = \true
\end{align*}
but this can be confusing at first.
Should we distinguish predicate and set?
Should we treat them as the same thing?
The membership operator $\in$ becomes swapped function application.

We can even generalize the notation $f x = x \in f$ to every function $f : a \to b$, not just predicates.
Let $f x = x + 1$. Then $f 0 = 0 \in f = 1$.
This may need some effort and time to get accustomed to,
but once you master it, you will be another mathematician.

\begin{mthm}[Equinumerosity among two-parameter types]
    All these types have the same cardinality:
    \begin{itemize}
        \item $\Relab{a}{b}$, $\Relab{b}{a}$
        \item $\Pred(a,b)$, $\Pred(b,a)$
        \item $\Set{(a,b)}$, $\Set{(b,a)}$
        \item $\Fun{a}{(\Set{b})}$, $\Fun{(\Set{a})}{b}$
    \end{itemize}
    \begin{proof}
        Proving $|\Relab{a}{b}| = |\Relab{b}{a}|$ is simple.

        Proving $|\Pred(a,b)| = |\Pred(b,a)|$ is simple.

        $r : \Relab{a}{b}$ and $p : \Pred(a,b)$ and $f : a \to b \to \Bool$.
        $r$ relates $x$ to $y$ if and only if $p(x,y)$ is true.
        \begin{align*}
            p z &= z \in r
             \\ &= \mu z r
             \\ &= \phi \mu r z
            \\
            p &= \phi \mu r
        \end{align*}
        Then let $p = r$.

        Since there is a bijection between $\Pred{(a,b)}$ and $\Relab{a}{b}$
        and between $\Pred{a}$ and $\Set{a}$,
        there is a bijection between $\Relab{a}{b}$ and $\Set{(a,b)}$.

        To prove that there is a bijection between $\Relab{a}{b}$ and $\Fun{a}{(\Set{b})}$,
        we choose any $r : \Relab{a}{b}$ that is a relation
        from objects of type $a$ to objects of type $b$.
        Define the \emph{image of $x$ in $r$} as
        $i r x = \{ y \,|\, \text{$r$ relates $x$ to $y$} \}$
        where the type of $i$ is $\Relab{a}{b} \to a \to \Set{b}$.
        We define the \emph{relation functionization} function $F$ as
        \[ F r = \{ (x,Y) \,|\, i r x = Y \} \]
        we capitalize $Y$ to highlight the fact that it is a set.
        $G : \Fun{a}{(\Set{b})} \to \Relab{a}{b}$ is the \emph{function relationization} function.
        \[ G f = \{ (x,y) \,|\, y \in f x \} \]
        $G f$ relates $x$ to $y$ iff $y \in f x$.
        We can see that $F(G f) = f$ and $G(F r) = r$.
        Thus $F \circ G$ is the identity of $\Fun{a}{(\Set{b})}$
        and $G \circ F$ is the identity of $\Relab{a}{b}$.
        Thus $F$ and $G$ are inverses of each other.

        ???
    \end{proof}
\end{mthm}

There is a mapping from $\Fun{a}{b}$ to $\Relab{a}{b}$.
There is a bijection between $\Relab{a}{b}$ and $\Relab{b}{a}$.
There is a bijection between $\Relab{b}{a}$ and $\Fun{b}{(\Set{a})}$.
This means that there is a bijection between $\Fun{a}{(\Set{b})}$ and $\Fun{b}{(\Set{a})}$.

\section{Machines}

\section{Machine types}

We define the \emph{type of a deterministic machine with configuration type $c$} as
\[ \DMach{c} = \Pair{(\Set{c})}{(\Fun{c}{c})} \]
Compare this to the type of a nondeterministic machine
with the same configuration type,
which differs only in the type of the second element of the pair
by replacing $\sfFun$ with $\sfRel$:
\[ \NMach{c} = \Pair{(\Set{c})}{(\Relab{c}{c})}. \]
We can generalize this by parametrization to
the \emph{type of a machine with configuration type $c$
and transition kind $t$},
\[
    \Mach{t}{c} = \Pair{(\Set{c})}{(t\,c\,c)},
\]
so that we can define in hindsight that
\begin{align*}
       \sfDMach &= \Macha{\sfFun},
    \\
       \sfNMach &= \Macha{\sfRel}.
\end{align*}

\section{Inhabitants of the machine types}

A \emph{deterministic machine $D$ with configuration type $c$}
(an inhabitant of $\DMach{c}$) is
\begin{align*}
    D &= (I,\beta) : \DMach{c}
\end{align*}
where
\begin{itemize}
    \item the set $I : \Set{c}$ is the set of all \emph{initial configurations} of the machine, and
    \item the function $\beta : \Fun{c}{c}$ is the \emph{transition function} of the machine.
\end{itemize}

\section{Turing machines}

The machine we have been talking about
can be more general than a deterministic Turing machine.
To make a machine that is equivalent to a Turing machine,
the configuration type must be countable
and the transition function must be recursive.
\begin{align*}
    \DTM &= \Pair{(\Set{\Bits})}{\RecFun}
    \\
    \NTM &= \Pair{(\Set{\Bits})}{\RecRel}
    \\
    \RecFun &< \Fun{\Bits}{\Bits}
    \\
    \RecRel &< \Relab{\Bits}{\Bits}
\end{align*}

If and only if $|\RecFun| = |\RecRel|$,
nondeterminism does not let the machine compute any more function.

Recall that a predicate is a set and a set is a predicate.
The set $I$ is a subset of $C$.
Let $C$ be the set of all inhabitants of the configuration type $c$.

\section{Introduction}

A \emph{configuration} of a machine is a state of that machine at a certain time.
For example, a configuration of a Turing machine is a tuple of its state,
its head positions, and the contents of its tapes.
A \emph{machine} is a \emph{transition function}.
This transition function depends on the set of primitive operations of the machine.
Seen the other way around, this transition function
determines the set of primitive operations of the machine.

A machine is an embodiment of an algorithm.

A machine performs computation by repeatedly
making a transition from its current configuration
according to its transition function
until it reaches a terminal configuration.

A \emph{primitive operation} maps a configuration to a configuration.
Every primitive operation represents a computation that the machine can do in one unit time.
An \emph{architecture} is a set of primitive operations
and a set of rules for evaluating expressions built using those primitive operations.
A \emph{machine} is an architecture and a configuration representing its current state.

\section{Deterministic machines and graph theory}

We can see the tuple $(C,\beta)$ as a \emph{directed graph}
where each configuration is a vertex in the graph
and there is an edge from $x$ to $y$ if and only if $\beta(x) = y$.
An \emph{initial configuration} is a possible configuration from which the machine starts computing.
An initial configuration contains an input for the algorithm running on the machine.
A deterministic machine always starts computing from an initial configuration
and always either goes into infinite cycle or ends at terminal configuration.

\paragraph{Termination}
A configuration $x$ is \emph{terminal} iff
there is no $y$ satisfying $\beta x = y$, that is iff there is no $y$ such that $(x,y) \in \beta$.
In the graph, such configuration $x$ is terminal iff it has zero out-degree,
that is iff there is no edge from that configuration in the graph.

When there is ambiguity about which machine we are discussing,
we will write something like `terminal $D$-configuration'
or `$\beta$-terminal configuration' instead of just `terminal configuration'.

We say that a configuration $x$ \emph{eventually terminates}
iff there exists $n$ such that $\beta^n(x)$ is terminal.
Such configuration $x$ eventually terminates iff repeated application of $\beta$ to $x$
eventually produces a terminal configuration.
In the graph, such configuration $x$ eventually terminates iff
there is a path from $x$ to a terminal configuration.

\paragraph{Computation}
A \emph{computation} is a path in the graph.
More precisely, a computation from $x_0$ to $x_{n-1}$
is a path $[x_0, x_1, \ldots, x_{n-1}]$
such that $x_{k+1} = \beta x_k$ for each natural number $k$ from $0$ to $n-1$.
We can see by induction that $x_k = \beta^k x_0$ for each natural number $k$ from $0$ to $n-1$.

A \emph{complete computation} is a computation that begins at an initial configuration
and ends at a terminal configuration.
Subcomputation is to computation as subpath is to path.
The graph can be seen as a set of complete computations.

We say that the deterministic machine $D = (I,\beta)$ \emph{computes} a function
$f : A \to B$
that is isomorphic to
$f_N : \mathbb{N} \to \mathbb{N}$
that is in turn also isomorphic to
$f_D : I \to T$
where $T$ is the set of all terminal configurations of the machine
where $f_D$ is defined as the following function:
\begin{equation}
    f_D = \{ (x,y) \,|\, x \in C \,\wedge\, x \text{ eventually terminates at } y \}
\end{equation}
Formally we say $f_D x = y$ iff there exists
a natural number $n$ such that $\beta^n x = y$ and $y$ is terminal.
The functions $p$ and $q$ are the \emph{input encoding} function
and the \emph{output encoding} function.
If we ignore the time used by the machine,
we can see the machine as a function from $I$ to $T$.
$q \circ f = f_D \circ p$.

State every natural number $n$ as the sum of $p$ and $q$
where $p$ is the biggest prime less than $n$.

The encoding and decoding functions allow us to make
a distinction between a natural number
and the binary representation of natural number.

\section{A universal machine to simulate other machines?}

\[
    \UDMach{c} = \DMach{(\Pair{(\Fun{c}{c})}{c})}
\]

Machine or algorithm can be encoded as string.
\emph{Universal machines} simulate every machine.
A program corresponds to a \emph{partial computable function}.
Partial is not total.
Total function is a function defined for each element in its domain.
Partial function is a function that can be undefined for any number of elements in its domain.

The \emph{$c$-universal machine} of type $\UDMach{c}$ can compute every inhabitant of $\DMach{c}$.
The configuration type of the universal machine is $\Pair{(\Fun{c}{c})}{c}$.
\[
    I_u = \{ (\beta,x) \,|\, (I,\beta) : \DMach c \,\wedge\, x \in I \}
\]
Can a universal machine simulate all universal machines?
Let $t = (\Fun{c}{c},c)$.
Is $|t| = |(\Fun{t}{t},t)|$?
If and only if yes, then a universal machine can simulate all universal machines.

A universal deterministic machine is $U = (C_u, I_u, \beta_u)$
where the universal configuration set is $C_u = (C \to C, C)$.
A configuration of that universal machine is a tuple $(\beta, x)$.
This machine can compute everything every other machine can compute.
The universal machine transitions from $(\beta, x)$ to $(\beta, y)$
if and only if the simulated machine transitions from $x$ to $y$.
Let $u$ be the pairing function, that is $uab = (a,b)$.
Then
\begin{equation}
    u \beta \circ \beta = \beta_u \circ u \beta
\end{equation}
The universal transition function $\beta_u$ can be seen as a set:
\begin{equation}
    \beta_u = \{ ((\beta,x), (\beta,y)) \,|\, (x,y) \in \beta \}
\end{equation}
The inverse of the partially applied pairing function $u a$ is $r$
where $r (a,b) = b$.

\section{Recursive Function over Bitstrings}

Every inhabitant of $\Bit \to \Bit$ is recursive.

Every inhabitant of $\Bit \to \Bit \to \Bit$ is recursive.

For all $a$, the \emph{head} function $f [x 0, x 1, \ldots] = x 0$,
where $f : \List{a} \to a$, is recursive.

For all $a$, the \emph{tail} function $f [x 0, x 1, \ldots] = [x 1, \ldots]$,
where $f : \List{a} \to \List{a}$, is recursive.

Every well-formed well-typed lambda expression
that involves only recursive functions is recursive?

\section{Nondeterminism from two deterministic machines}

The author got the idea of defining a nondeterministic machine
using two deterministic transition functions
from Arora and Barak \cite[p.~40]{Arora2009}.
We can form a nondeterministic machine $N$
from two deterministic machines $D_0 = (C,I,\beta_0)$ and $D_1 = (C,I,\beta_1)$
(both machines must have the same configuration set and the same initial configuration set)
as follows:
\begin{enumerate}
    \item The sets of initial configurations are the same.
    \item If $x$ is a configuration of $D_0$ then $(0,x)$ is also a configuration of $N$.
    \item If $x$ is a configuration of $D_1$ then $(1,x)$ is also a configuration of $N$.
    \item Nothing else is a configuration of $N$.
    \item $\beta = P 0 \beta_0 \cup P 1 \beta_1$ where $P c X = \{ (c,x) \,|\, x \in X \}$.
    \item $x$ is $\beta$-terminal iff $x$ is $\beta_0$-terminal or $\beta_1$-terminal or both.
    \item Nothing else is $\beta$-terminal.
\end{enumerate}

Sum types: $\Either{a}{b} = \Left a | \Right b$.
Iff $x : a$ then $\Left x : \Either{a}{b}$.
Iff $y : b$ then $\Right y : \Either{a}{b}$.
The configuration type is $\Either{c_0}{c_1}$.
\[
    I =
    \{ \Left x \,|\, x \in I_0 \}
    \cup
    \{ \Right x \,|\, x \in I_1 \}
\]
\begin{align*}
    (\Left x, \Left y) \in \beta \iff (x,y) \in \beta_0
    \\
    (\Right x, \Right y) \in \beta \iff (x,y) \in \beta_1
\end{align*}

$\NMach c$.
$\beta : \Relab{c}{c}$.
Does $\Fun{c}{c} < \Relab{c}{c}$ strictly?

We can also form a deterministic machine $D$ from a nondeterministic machine $N$.

A machine computes a function $f : X \to Y$ as follows:
For each $x$ such that $f x$ is defined, the string $\hat x$ (the string encoding of $x$)
is an initial configuration of the machine.
For each $y = fx$, the configuration $\hat y$ (the configuration encoding of $y$)
is a terminal configuration of the machine.
There is a path from $\hat x$ to $\hat y$.
There is a machine that computes this $f$ in unit time: the oracle of $f$.
Then there is another machine that computes it asymptotically slower.
Then there is another machine that computes it even slower asymptotically.
This goes on and on.
We can always invent a slower machine.

\begin{enumerate}
    \item
        Every configuration of $N$ can branch to \emph{at most two} configurations.
        Formally for each configuration set $X$, it holds that $\mu(B X) \le 2 \cdot \mu X$
        where $\mu X$ is the size of $X$.
\end{enumerate}

$\NMach c$.

\section{Nondeterminism}

$N = (I,\beta) : \NMach c$
where $I : \Set{c}$ and $\beta : \Relab{c}{c}$.

Let $N$ be a nondeterministic machine $(C,I,\beta)$ where
Let $\beta \subseteq C \times C$ is the transition relation of $N$.
The tuple $(C,\beta)$ forms a graph.
There is an edge from $x$ to $y$ iff $(x,y)$ is in $\beta$.
with the following properties:
\begin{align}
    B^n X &= \{ (x,y) \,|\, x \in X \wedge (x,y) \in \beta^n \}
\end{align}
where $B X$ means $B^1 X$.

The relation $\beta$ can also be seen as an equivalent function $B : \powerset C \to \powerset C$
where $\powerset C$ is the power set of $C$.
Thus for every nondeterministic machine $(C, I, \beta)$
there is a deterministic machine $(\powerset C, \powerset I, B)$
that computes the same function at the same time.
But this deterministic machine is not countable.
Contradiction.
So nondeterminism bestows some power that deterministic machines cannot have.
There exists nondeterministic machine that cannot be simulated in equal time by deterministic machine.
(Is this a jump in logic? This seems wrong.)

But every deterministic machine can simulate
every nondeterministic machine by using breadth-first search.
So nondeterminism does not add any power?

Every deterministic machine is trivially a
nondeterministic machine since every transition function is a transition relation.

Is the set of functions computable by the nondeterministic machines
the same as that of deterministic machines?
Does it add any power?

\section{Uncomputable functions}

Each machine has infinitely many functions that it \emph{cannot compute}.
We prove this by diagonalization.
Suppose that the set of functions computable by a machine is $F = \{ f_0, f_1, f_2, \ldots \}$.
Suppose also that there is no function outside that set.
Alas, we can define a function $f$
such that $f0 \neq f_0 0$, $f1 \neq f_1 1$, $f2 \neq f_2 2$, and so on
such that $fn \neq f_n n$ for each natural number $n$.
This $f$ is not equal to any function in $F$ so it cannot be in that set.
Contradiction.

Nondeterministic machine is a $\sigma$-algebra of deterministic machine?
Can we relate nondeterminism to topology?

We can make it a topological space?
Nondeterministic reversible computation?
Remember that a configuration of a nondeterministic machine is a set.
Allow the no-computation: for each configuration $x$, make $x \in N x$.
Then make it reversible:
if $y \in N x$ then $x \in N y$.
Then make each subset of $B x$ to be in $N x$.
We can $(C,N)$ where $C$ is the set of all configurations of the machine
and $N : C \to 2^C$ is the neighborhood function.

Ullman? Sipser? defines NTM as DTM with transition relation instead of transition function.

We define $\beta^n$ as a composition of $n$ instances of $\beta$:
$\beta^0 = \text{id}$ and $\beta^1 = \beta$ and $\beta^n = \beta \circ \beta^{n-1}$.
The time complexity of a computation $x$
is the smallest $n$ such that $\beta^{n+1} x = \beta^n x$.

\section{Major theme in computability theory}

The idea is to encode almost \emph{everything}
(machines, algorithms, configurations, computations, numbers, graphs, you name it)
as a \emph{string} and then establish a bijection between strings and natural numbers.

Time-limited simulation of another machine:
simulate the machine as long as the number of steps do not exceed $f n$.
If the number of step exceeds $f n$ and the machine still does not terminate,
output $0$.
Truncated language?

A particular kind of machine called the \emph{recursive machines}
is equivalent to Turing machines with polynomial-time speedup/slowdown.

For each machine with transition function $t$ there exists another machine
with transition function $t' = t^p$
providing polynomial speedup.
Space usage of configuration $x$.
Length of program plus length of input.

\section{Differential topology and computation?}

A configuration is a point.

The transition function.

Unit step is in differential time?

What are we trying to say?
