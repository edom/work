\chapter{Artificial intelligence}

\section{Convex}

\paragraph{Set}
Let \(p, q \in S\) be two points
and \(L(p,q) \subseteq S\) be the line segment from \(p\) to \(q\).
The set \(S\) is
\index{convex set}%
\emph{convex} iff \(\forall p,q \in S : L(p,q) \subseteq S\).

\paragraph{Function}
A function \(f : \Real \to \Real\) is
\index{convex function}%
\emph{convex}
iff the area above its graph is a convex set.
That area is \(\{(x,y) ~|~ x \in \Real, ~ y > f(x)\}\).

\section{Machine learning}

Why are there so many machine learning algorithms?

Machine learning is finding a function fitting a data list,
minimizing error on unseen data.
Machine learning is about how program improves with experience.

Find a function fitting the data and minimizing the
\index{loss function}%
\emph{loss function}.

Given \([(x_1,y_1),\ldots,(x_n,y_n)]\),
find \(f\) minimizing \(\sum_k \norm{f(x_k) - y_k}^2\).

A
\index{model}%
\emph{model} is a constrained optimization problem:
Given \(C\),
compute \(\min_{x \in C} f(x)\) or \(\argmin_{x \in C} f(x)\).
If \(C\) is discrete, use dynamic programming.
If \(C\) is continuous, use gradient descent.

\section{Predictor}

Let \(a\) be the input type, \(b\) be the output type, and \(g : a \to b\).
A
\index{predictor}%
\emph{predictor} is a function.
Iff \(b\) is finite, then \(f\) is a
\index{classifier}%
\emph{classifier}.
A
\index{feature}%
\emph{feature} inhabits \(a \to \Real\).
A
\index{data}%
\emph{data} or an
\index{example}%
\emph{example}
is a tuple \((x,y) : (a,b)\).

A
\index{linear predictor}%
\index{predictor!linear}%
\emph{linear predictor} is the equation
\(y = w \cdot f(x)\) where \(w\) is the \emph{weight vector},
\(f(x) = (f_1(x),\ldots,f_n(x))\) is the \emph{feature vector} of \(x\),
\(f_k(x)\) is the \(k\)th feature,
\(x\) is the input,
and \(y\) is the predicted output.
The predictor is linear in \(w\).

\section{Classifier}

\index{binary classifier}%
\index{classifier}%
\index{classifier!binary}%
The type of a \emph{classifier} is \(a \to b\) where \(b\) is countable.
Iff \(|b| = 2\), the classifier is \emph{binary}.
Iff \(|b|\) is finite, the classifier is \emph{multi-class}.

A \emph{quasiclassifier} is an inhabitant of \(\Real^\infty \to \Real\).
A \emph{predicate} \(p\) turns a quasiclassifier \(q\) into a classifier \(c~x = p~(q~x)\).

A multiclassifier can be made from binary classifiers.

\index{maximum-margin hyperplane}%
\index{hyperplane!maximum-margin}%
The \emph{maximum-margin hyperplane} separating
the lower training set \(L\) and the upper training set \(U\)
is the hyperplane \(h\)
such that
\(\forall a \in U : h~a > 0\),
\,\(\forall b \in L : h~b < 0\),
and \(\dist~h~(U \cup L)\) is maximal.

\section{Learner}

A
\index{learner}%
\emph{learner} inhabits \([(a,b)] \to (a \to b)\).

A
\index{loss function}%
\emph{loss function} inhabits \((a,b,\Real^\infty) \to \Real\).

The
\index{training loss}%
\emph{training loss} of \(g(x) = w \cdot f(x)\) with respect to \(D\)
is \(\frac{1}{|D|} \sum_{(x,y) \in D} L(x,y,w)\)
where \(L\) is the loss function.

Learning is finding \(w\) that minimizes the training loss.

Let \(y \in \{-1,+1\}\).
The
\index{score}%
\emph{score} of \(f\) for \((x,y)\) is \(f(x)\).
The
\index{margin}%
\emph{margin} of \(f\) for \((x,y)\) is \(f(x) \cdot y\).

Binarization of \(f\) is \(\sgn \circ f\).

Least-squares linear regression

Minimize training loss

Gradient descent training with initial weight \(w_1\), iteration count \(T\), and step size \(\eta\):
Let \(K : \Real^n \to \Real\) be the training loss function.
Let \(\nabla K\) be the gradient of \(K\).
The weight update equation is \(w_{t+1} = w_t - \eta \cdot (\nabla K)(w_t)\)
where \(w_1\) may be random.
The training result is \(w_T\).

Stochastic gradient descent (SGD) training:
\(w_{t+1} = w_t - \eta \cdot (\nabla(L~x_t~y_t))(w_t)\).
Note the usage of the loss function \(L\)
instead of the training loss function \(K\).

SGD is \emph{online} or \emph{incremental} training.

Classification is regression with zero-one loss function.
Every classification can be turned into regression
by using \emph{hinge loss} or \emph{logistic regression}.

The
\index{logistic function}%
\emph{logistic function} is \(f(x) = \frac{1}{1 + e^{-x}}\).

Nearest neighbor with training data list \(D\):
\(g(x') = y\) where \((x,y) \in D\) minimizing \(\norm{f(x') - f(x)}^2\).

\section{University courses}

For a course with computer science background, see
Stanford University CS221 (Artificial Intelligence: Principles and Techniques) Autumn 2016 \cite{LiangCs221}.
For a course with mathematics background, see
Massachusetts Institute of Technology 18.657 (Mathematics of Machine Learning) Fall 2015 \cite{rigollet2015ocw}.

\section{Bibliography}

\cite{DeepArch}

\cite{DeepLearning}

\cite{RepLearn}

\cite{SuttonBartoRein}

Algorithmic information theory
\cite{AlgoInfTh}

\section{Other resources}

Corpuses, datasets, training sets:
MNIST handwritten digit dataset.

OpenAI.
Let an AI learn in an accurate-enough physical simulation,
then move it into the real world.

OpenCog \url{http://opencog.org/about/}
