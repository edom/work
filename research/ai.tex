\chapter{Artificial intelligence}

(Summarize \cite{LiangCs221}. Shuffle around later.)

A
\index{model}%
\emph{model} is a constrained optimization problem:
Given \(C\),
compute \(\min_{x \in C} f(x)\) or \(\argmin_{x \in C} f(x)\).
If \(C\) is discrete, use dynamic programming.
If \(C\) is continuous, use gradient descent.

Let \(a\) be the input type, \(b\) be the output type, and \(g : a \to b\).
A
\index{predictor}%
\emph{predictor} is a function.
Iff \(b\) is finite, then \(f\) is a
\index{classifier}%
\emph{classifier}.
A
\index{feature}%
\emph{feature} inhabits \(a \to \Real\).
A
\index{data}%
\emph{data} is a tuple \((x,y) : (a,b)\).

A
\index{linear predictor}%
\index{predictor!linear}%
\emph{linear predictor} is \(y = w \cdot f(x)\) where \(w\) is the weight vector,
\(f(x) = (f_1(x),\ldots,f_n(x))\) is the feature vector of \(x\),
\(f_k(x)\) is the \(k\)th feature,
\(x\) is the input,
and \(y\) is the predicted output.

A
\index{learner}%
\emph{learner} inhabits \([(a,b)] \to (a \to b)\).

A
\index{loss function}%
\emph{loss function} inhabits \((a,b,\Real^\infty) \to \Real\).

Let \(y \in \{-1,+1\}\).
The
\index{score}%
\emph{score} of \(f\) for \((x,y)\) is \(f(x)\).
The
\index{margin}%
\emph{margin} of \(f\) for \((x,y)\) is \(f(x) \cdot y\).

Binarization of \(f\) is \(\sgn \circ f\).

Least-squares linear regression

Minimize training loss
