\chapter{Computation}

\section{Endofunction}

\index{endofunction}%
The type of an \emph{endofunction} is \(a \to a\).

\section{Unary algebra}

\index{unary algebra}%
A \emph{unary algebra} is \((S,f)\) where \(f : S \to S\).

\section{String}

\index{language}%
A \emph{language} is a set of strings.

Two
\index{string}%
\emph{strings}
\(x\) and \(y\) can be
\index{string concatenation}%
\index{concatenation!string}%
\emph{concatenated} to \(xy\).

Two languages \(A\) and \(B\) can be
\index{language concatenation}%
\index{concatenation!of languages}%
\emph{concatenated} to
\(AB = \{ ab ~|~ a \in A, ~ b \in B \}\).

\index{alphabet}%
An \emph{alphabet} is a finite set whose each element is a string of length one.

\section{Kleene closure}

Let \(A\) be a language.
The
\index{Kleene closure}%
\index{Kleene operator}%
\index{Kleene star}%
\emph{Kleene closure}
of \(A\) is \(A^* = A^0 \cup AA^*\).
Another nonrecursive definition is
\( A^* = \bigcup_{n \in \Nat} A^n \)
where \( A^0 = \{\emptystr\} \),
\( A^{n+1} = A A^n \),
and \(\epsilon\) is the
\index{empty string}%
\index{string!empty}%
empty string.

\(A^*\) is the smallest superset of \(A\)
closed under string concatenation.

\section{Finite automaton}

\section{Finite-state machine}

\section{Turing machine}

A \emph{Turing machine} is a finite-state machine with read-write memory.
How do we model finite-state machine?
How do we model read-write memory?

Let \(m\) be a Turing machine.

Let \(A\) be the alphabet.
It is finite.

Let \(S\) be the state type.
It is finite.

Let \(P = (S,T)\) be the phase type.

Let \(T = (A^\infty,A,A^\infty)\) be the tape type.

Define the \emph{phase} of \(m\) as \((s,t)\)
where \(s\) is the \emph{state} and \(t\) is the \emph{tape}.

A tape is \((L,h,R) : (A^\infty,A,A^\infty)\).

Define the transition function \(\after : P \to P\).

The pluggable parts of the machine are
the alphabet type \(A\),
the state type \(S\),
the next-state function \(\jump : (S,A) \to S\),
the output-symbol function \(\fwrite : (S,A) \to A\),
and the direction function \(\dir : (S,A) \to \{0,1\}\).
The \(\jump\) computes the next state.
The \(\fwrite\) function computes the symbol to write.
The \(\dir\) function computes where the head should move after writing that symbol.

The tape is \((L,h,R)\).
The string \(L\) is the \emph{reverse} of the string on the left of the head.
The symbol \(h\) is the symbol at the head.
The string \(R\) is the string on the right of the head.

Here comes an equational description of a Turing machine.
For readability, we don't use the minimum number of equations.

\section{The fixed parts}

A Turing machine must satisfy all of these.
These are the parts we cannot change.

The types.
\begin{align}
    P &= (S,T)
    \\
    T &= (\InfList~A, ~ A, ~ \InfList~A)
    \\
    \after &: P \to P
\end{align}

The logical constraints.
\begin{align}
    S &\text{ is finite}
    \\
    A &\text{ is finite}
\end{align}

The value definitions:
\begin{align}
    \after~(s,t) &= (s',t')
    \\
    t &= (L,h,R)
    \\
    s' &= \jump~i
    \\
    t' &= \dir~\stay~\fleft~\fright~i~(L,w,R)
    \\
    i &= (s,h)
    \\
    w &= \foutput~i
    \\
    \stay~(L,h,R) &= (L,h,R)
    \\
    \fleft~(cL,h,R) &= (L,c,hR)
    \\
    \fright~(L,h,cR) &= (hL,c,R)
\end{align}

We try to avoid pattern matching:
\begin{align}
    t' &= (L',h',R')
    \\
    w &= \foutput~i
    \\
    L' &= \dir~\stay_0~\fleft_0~\fright_0~L
    \\
    h' &= \dir~\stay_1~\fleft_1~\fright_1~L~w~R
    \\
    R' &= \dir~\stay_2~\fleft_2~\fright_2~R
    \\
    \stay~(L,h,R) &= (L,h,R)
    \\
    \fleft~(L,h,R) &= (\tail~L, \head~L, \cons~h~R)
    \\
    \fright~(L,h,R) &= (\cons~h~L, \head~R, \tail~R)
    \\
    \stay_0~L &= L
    \\
    \fleft_0~L &= \tail~L
    \\
    \fright_0~L &= \cons~L
    \\
    \stay_1~h &= h
    \\
    \fleft_1~L &= \head~L
    \\
    \fright_1~R &= \head~R
    \\
    \stay_2~R &= R
    \\
    \fleft_2~R &= \cons~R
    \\
    \fright_2~R &= \tail~R
    \\
    \stay_0 &= \stay_2
    \\
    \fleft_0 &= \fright_2
    \\
    \fright_0 &= \fleft_2
    \\
    \fleft_1 &= \fright_1
\end{align}

Some note about the notations:
We write \(A^\infty\) for the set of all infinite strings of \(A\).

\section{The pluggable parts}

There are the parts we can change.
\begin{align}
    s_0 &= \text{the initial state}
    \\
    S &= \text{the set of states}
    \\
    A &= \text{the set of symbols}
    \\
    \jump &= \text{a function of type \((S,A) \to S\)}
    \\
    \foutput &= \text{a function of type \((S,A) \to A\)}
    \\
    \dir &= \text{a function of type \(\forall a ~.~ a \to a \to a \to (S,A) \to a\)}
\end{align}

If you have the pluggable parts,
the equations will give you a Turing machine.

\section{Some definitions}

The input is the initial phase.

Let \(x = (s,t)\).
We say that \(x\) is an \emph{initial} phase iff \(s = s_0\).
We say that \(x\) is a \emph{halting} phase iff \(\after~x = x\).

The \emph{phase graph} is defined as follows.
Its vertex set is the set of all phases.
An edge \((x,y)\) is in the graph iff \(\after~x = y\).

We say that \(x\) \emph{eventually leads} to \(y\) iff
there is a path from \(x\) to \(y\) in the phase graph.

We say that the machine \emph{halts for input \(t\)} iff
\((s_0,t)\) eventually leads to a halting phase.

Let the alphabet be \(A = \{b,0,1\}\) where \(b\) is the blank symbol.
Let \(B = \ldots bbb \ldots\) be an infinite string of blank symbols.
We say that the machine \emph{accepts the input \(t\)} iff
the phase \((s_0,t)\) eventually leads to the phase \((s, (B,1,B))\)
where \(s\) can be any state.

\section{Turing machine as constrained unary algebra}

A Turing machine is a unary algebra with a limited transition function.
It can only read and write at the head.

We can encode an infinite stream of \(a\)
as the infinite type \(S~a = (a \to b) \to (S~a \to b) \to b\).

\emph{A Turing machine is a finite state machine with memory.}

A finite state machine has input, but no memory.

We can encode memory as infinite feedforward.

A finite state machine step function is a function \((S,I) \to (S,O)\).

\section{Machine}

A machine that can only change one cell at a time.
A machine is
\((P, f)\)
where \(P = (S,A^\infty)\)
and
\(d~x~(f~x) \le 1\)
for all \(x : P\).
The state set \(S\) is finite.
The alphabet \(A\) is finite.
Define the distance between two phases as
\begin{align}
    d~(s,x)~(t,y) = \sum_{k : \Nat} ~ [x_k \neq y_k].
\end{align}
It ignores the state.
It only cares about the memory.

But unlike Turing machines, this machine cannot approach an infinite number of 1s...

The machine is free to change its state as long as it only changes at most one cell at a time.
If we remove the restriction, we get a nondeterministic machine,
or even an oracle.

If \(S\) has exactly \(n\) elements,
we can encode \(f : S \to A\) as \(g : A^n\)
where \(f~s_k = g_k\) where \(g = (f~s_1, \ldots, f~s_n)\).

\section{Logical formulation of Turing machine}

We can encode a function \(f : A \to B\) as a predicate \(f' : A \to B \to Bool\) such that \(f'~a~b\) iff \(f~a = b\).

If you have the predicates \(\jump\), \(\fwrite\), \(\stay\), \(\fleft\), and \(\fright\),
and an initial state and initial tape,
then the Turing machine generator will give you the following inference rules.
You can rewrite the rules in Prolog.
Prolog will then simulate that Turing machine.
\begin{align}
    \inferrule
    {\fwrite~s~h~w \\ \stay~s~h}
    {\tape~s~(L,h,R)~(L,w,R)}
    \\
    \inferrule
    {\cons~h'~L'~L \\ \cons~w~R~R' \\ \fwrite~s~h~w \\ \fleft~s~h}
    {\tape~s~(L,h,R)~(L',h',R')}
    \\
    \inferrule
    {\cons~w~L~L' \\ \cons~h'~R'~R \\ \fwrite~s~h~w \\ \fright~s~h}
    {\tape~s~(L,h,R)~(L',h',R')}
    \\
    \inferrule
    {\jump~s~h~s' \\ \tape~s~(L,h,R)~(L',h',R')}
    {\phase~(s,(L,h,R))~(s',(L',h',R'))}
\end{align}

The computation path beginning from \(p\).
\begin{align}
    \inferrule
    {\phase~p~q \\ p \neq q \\ \fpath~q~r \\ \cons~p~r~r'}
    {\fpath~p~r'}
    \\
    \inferrule
    {\halting~p}
    {\fpath~p~[p]}
\end{align}

\begin{align}
    \halting~x &\equiv \phase~x~x
\end{align}

\begin{align}
    \inferrule
    {\forall s ~ \forall h ~ | \{ w ~|~ \fwrite~s~h~w \} | \le 1 \\ \forall s ~ \forall h ~ |[\stay~s~h] + [\fleft~s~h] + [\fright~s~h]| \le 1}
    {\text{the machine is deterministic}}
\end{align}

\section{What}

Turing machine as mono-unary algebra of states (configuration-tape pairs)

Universal Turing machine

Lambda calculus

Recursive functions

\section{Regex, cfg, Brzozowsky}

If \(a\) is a type, then \(\Regex~a\) is a type.
Usually we call the \(a\) in \(\Regex~a\) the \emph{alphabet}.

\begin{align}
    &\vdash \fnull : \Regex~a
    \\
    &\vdash \fempty : \Regex~a
    \\
    x : a &\vdash x : \Regex~a
    \\
    x : \Regex~a, ~ y : \Regex~a &\vdash xy : \Regex~a
    \\
    x : \Regex~a, ~ y : \Regex~a &\vdash x|y : \Regex~a
    \\
    x : \Regex~a &\vdash x^* : \Regex~a
\end{align}

Sometimes we parenthesize. The expression \(x|y^*\) means \(x|(y^*)\),
which is different from \((x|y)^*\).

Let there be a function \(\match : \Regex~a \to \List~a \to \Bool\).
\begin{align}
    \match~\fnull~x &\vdash
    \\
    &\vdash \match~\fempty~[]
    \\
    x : a, ~ y = [x] &\vdash \match~x~y
    \\
    \match~r~x, ~ \match~s~y &\vdash \match~(rs)~(xy)
    \\
    \match~r~x \vee \match~s~x &\vdash \match~(r|s)~x
    \\
    &\vdash \match~r^*~[]
    \\
    \match~r~x, \match~(r^*)~y &\vdash \match~(r^*)~(xy)
\end{align}

Recall that a language is a \emph{set} of strings.
We define multiplication slightly differently.
We replace ordered pair with string concatenation:
\begin{align}
    Q \times D &= \{ qd ~|~ q : Q, ~ d : D \}.
\end{align}

\section{Laziness}

Complete laziness, full laziness, interaction sets

Compile Java to lambda calculus

High level intermediate language


\section{(Begin old computation book draft)}

\theoremstyle{definition}
\newcounter{thmctr}
\newtheorem{mdef}[thmctr]{Definition}
\newtheorem{mque}[thmctr]{Question}
\newtheorem{mcon}[thmctr]{Conjecture}
\newtheorem{msco}[thmctr]{Strong Conjecture}
\newtheorem{mcor}[thmctr]{Corollary}
\newtheorem{mlem}[thmctr]{Lemma}
\newtheorem{mthm}[thmctr]{Theorem}
\newcommand\sno{\ensuremath{\mathcal N}}
\newcommand\syes{\ensuremath{\mathcal Y}}
\newcommand\Fin{\ensuremath{\operatorname{Fin}}}
\newcommand\powerset{\ensuremath{\mathcal{P}}}
\newcommand\fa[1]{\forall#1\,}
\newcommand\Fa[1]{\forall#1~}
\newcommand\FA[1]{\forall#1~~}
\newcommand\sfT{\mathsf{T}}
\newcommand\Typ{\mathsf{Typ}}
\newcommand\Either[2]{\mathsf{Either}~#1~#2}
\newcommand\Left[1]{\mathsf{Left}~#1}
\newcommand\Right[1]{\mathsf{Right}~#1}
\newcommand\Void{\mathsf{Void}}
\newcommand\Pair[2]{\mathsf{Pair}~#1~#2}
\newcommand\UDMach[1]{\mathsf{UDMach}~#1}
\newcommand\DMach[1]{\mathsf{DMach}~#1}
\newcommand\NMach[1]{\mathsf{NMach}~#1}
\newcommand\Mach[2]{\mathsf{Mach}~#1~#2}
\newcommand\Macha[1]{\mathsf{Mach}~#1}
\newcommand\RecRel{\mathsf{RecRel}}
\newcommand\Kleene[1]{\mathsf{Kleene}~#1}
\newcommand\RecFun{\mathsf{RecFun}}
\newcommand\DTM{\mathsf{DTM}}
\newcommand\NTM{\mathsf{NTM}}
\newcommand\sfDMach{\mathsf{DMach}}
\newcommand\sfNMach{\mathsf{NMach}}
\newcommand\sfFun{\mathsf{Fun}}
\newcommand\sfSet{\mathsf{Set}}
\newcommand\sfRel{\mathsf{Rel}}
\newcommand\sfVec{\mathsf{Vec}}
\newcommand\Vect[2]{\sfVec~#1~#2}
\newcommand\Relab[2]{\sfRel~#1~#2}
\newcommand\Fun[2]{\sfFun~#1~#2}
\newcommand\sfPred{\mathsf{Pred}}
\newcommand\Pred[1]{\sfPred~#1}
\newcommand\setB{\mathbb B}
\newcommand\decset{\mathcal D}
\newcommand\langset{\mathcal L}
% time complexity
\newcommand\TC{\mathcal{T}}
\newcommand\leTC{\ensuremath{\le_\TC}}
% time equivalence class
\newcommand\Teq{\mathbb{T}}
\newcommand\Teqsum{\mathbf{T}}
% space equivalence class
\newcommand\Seq{\mathbb{S}}
\newcommand\Seqsum{\mathbf{S}}
\newcommand\SC{\mathcal{S}}
% \newcommand\hom{\operatorname{hom}}
\newcommand\fite[3]{\text{if}~#1~\text{then}~#2~\text{else}~#3}
\newcommand\id{\ensuremath{\operatorname{id}}}
\newcommand\mE{\ensuremath{\mathcal E}}
\newcommand\mP{\ensuremath{\mathcal P}}
\newcommand\mM{\ensuremath{\mathcal M}}
\newcommand\mL{\ensuremath{\mathcal L}}
\newcommand\bB{\ensuremath{\mathbb B}}
\newcommand\amb{\operatorname{amb}}
\newcommand\ambc{\operatorname{ambc}}
\newcommand\fhead{\ensuremath{\operatorname{head}}}
\newcommand\ftail{\ensuremath{\operatorname{tail}}}
\newcommand\fcons{\ensuremath{\operatorname{cons}}}
\newcommand\aTIME{\operatorname{\alpha-TIME}}
\newcommand\idTIME{\operatorname{id-TIME}}
\newcommand\mPTIME{\operatorname{\mP-TIME}}
\newcommand\TIME{\operatorname{\mathsf{TIME}}}
\newcommand\DTIME{\operatorname{\mathsf{DTIME}}}
\newcommand\NTIME{\operatorname{\mathsf{NTIME}}}
\newcommand\EXPTIME{\ensuremath{\mathsf{EXP}}}
\newcommand\PTIME{\ensuremath{\mathsf{P}}}
\newcommand\NPTIME{\ensuremath{\mathsf{NP}}}
\newcommand\NSPACE{\operatorname{NSPACE}}
\newcommand\DSPACE{\operatorname{DSPACE}}
\newcommand\SDP{\ensuremath{\operatorname{SD}}}

\section{TODO}

cite CMI problem description;
understand Baker-Gill-Solovay (?) theorem and relativization;
read Rogers \cite{Rogers1987};
read Arora and Barak \cite{Arora2009};
read Marek and Remmel \cite{Marek2009};
read Boolos, Burgess, and Jeffrey \cite{Boolos2002}

Descriptive complexity: Fagin's theorem.
Neil Immerman's book.
NP equivalent to existential second-order logic.
% http://en.wikipedia.org/wiki/Fagin%27s_theorem

\section{Diary}

In this document, I use the term \emph{predicate} to always mean
a total function that takes a bit string and gives a bit.
Furthermore, if $f$ is a predicate, then
I may write ``$x$ \emph{satisfies} $f$'' to mean $f~x = 1$, and
I may write ``$f$ is \emph{satisfiable}'' to mean
that there exists $x$ that satisfies $f$.

\newcommand\encoding[1]{\langle#1\rangle}

I write $\encoding{x}$ to mean a binary encoding of $x$.
I write $\encoding{x,y}$ to mean a binary encoding of $x$
concatenated with a binary encoding of $y$.

The ATMB problem is ``Given a machine $m$, a string $x$,
and a number $n$,
does $m$ accept $x$ in $n$ steps or less?''
This problem is exptime-complete with respect to the length of
$\encoding{m,x,n}$. (cite?)

The bounded halting problem is:
``Given a machine $m$ and a number $n$,
does $m$ accept any string that is at most $n$ bits long?''
This is np-complete? (cite?)
% http://www.ics.uci.edu/~eppstein/161/960312.html
% sat-t-icalp.ps

\begin{enumerate}
\item Show that it is in exptime.
\item Show that it is in exptime-hard.
(This implies that it is not in p.
Show that it is polynomial-time many-one reducible to ATMB?
Use diagonalization perhaps?)
\item Show that it is in np.
\item Congratulations.
\end{enumerate}

\section{Defining computation}

Difficulty is because the tape head may move in two directions.
Graph reduction always proceeds in one direction.
Post-Turing machine is simpler than Turing machine.
Wang B-machine is even simpler.

Here I introduce the macroscopic model of computation:
a computation is a repeated application of a function $f$
until the output does not change anymore.
The computation begins with the input $x$ already in place.
I write that such computation produces the output $y$
in $n$ steps iff $n$ is the smallest number such that $f^{n-1}~x = f^n~x = y$
where $f^n~x$ means $n$ times applying $f$ to $x$.
This function can be thought a transition function that describes
how the entire system (machine state and tape content) changes.
A machine can be seen as a function.

This is a special case of fractal? Chaotic function?

The zero function $f~x = 0$ corresponds to a machine
that always accepts its input.

The function $f~x = 2 \cdot x$ corresponds to a machine
that appends a zero bit to the memory, one by one.

One-instruction-set computer.
Subleq.

What is the essence of computation?
Is it recursion?

Using graph reduction as computational model
may make the problem easier to solve.

This has to do with time-constructibility?

\section{Finding the problem}

Now I try to craft a problem that nondeterministic machines
can solve exponentially faster than deterministic machines can best do.
I am looking for a problem whose best deterministic solution
is an exhaustive search.
Formally, I am looking for a problem that is dexptime-hard but also in np.

I feel that this problem is promising:
``Find a string that satisfies a given satisfiable ptime-predicate,''
where I use the term \emph{ptime-predicate} to mean that
a machine can compute the predicate in an amount of time
that is a polynomial of the number of bits in the input string.

However, there is also an exptime-complete problem:
given a machine and a number $k$, determine if the machine
halts in at most $k$ steps.

What if I combine those problems into this problem:
``Given a ptime-predicate $p$ and a natural number $n$,
determine whether there is an $n$-bit string satisfying $p$?''
Showing that this problem is bounded below by dexptime
and bounded above by np would prove that p and np are not equal.

The input is $(p,n)$.
The output is a bit.

If the problem is to be in ptime,
the length of the string representation of $p$
must be a polynomial of $n$.

Suppose there exists $k$ such that the length of the input is in $\Theta(n^k)$.

Let the input be $(k,p,n)$?

The following question is a generalization of the p vs np problem:
Given the same amount of resources (space and time),
what is the maximum ratio of the number of functions that NTMs can compute
to the number of functions than DTMs can compute?
Informally, what is the maximum speedup
that nondeterminism can give a TM?
The answer of this question will answer the p vs np question.

How do I prove that a problem doesn't have better algorithm than brute force?
I have to see the paper that proves something is dexptime-complete.
Pebble game problem? Kolaitis and Panttaja?
Does that a problem can only be brute-forced at best
depend on the model of computation?
Show that the existence of faster algorithm raises a contradiction?

Should I switch from DTM and NTM to ATM (alternating Turing machine)?
Should I abandon TMs and use pebble games instead?
How about Boolean circuits?

The following is a nondeterminstic algorithm that
builds the string that will satisfy such predicate.
\begin{enumerate}
\item Let the current string be empty.
\item Either skip this step,
or pick a bit and append it to the current string.
\item If the current string satisfies the predicate, halt.
\item Otherwise go to step 2.
\end{enumerate}
The running time of that algorithm is a function of the length
of the shortest string that satisfies the predicate.
Let $s~p$ be the length of the shortest string that satisfies the predicate $p$.
We have just shown that the problem is in $\NTIME(O(s~p))$.

Now, if we can show that for that problem
there is no better deterministic algorithm than the exhaustive search
(that is if the problem is exptime-hard),
then we prove that P and NP are not equal.

The size of the search space of all deterministic algorithms
that run in $n$ steps is smaller than...
$\DTIME(2^{s~p})$.

Can the algorithm use any information from
the string representation of the predicate?

Suppose that there is a deterministic algorithm that can
always find an answer in polynomial time.

How many satisfiable predicates are there?
Let $c~n$ be the number of predicates that can be satisfied
by an input that is not longer than $n$ bits.
\begin{align}
  c~0 &= 2^{2^0} - 1 = 1
  \\
  c~1 &= 2^{2^0 + 2^1} - 1 = 7
  \\
  c~2 &= 2^{2^0 + 2^1 + 2^2} - 1 = 127
  \\
  c~3 &= 2^{2^0 + 2^1 + 2^2 + 2^3} - 1 = 32,767
  \\
  c~n &= 2^{2^{n+1}-1} - 1
\end{align}

I use the term \emph{$n$-least predicate} to mean that the predicate
can be satisfied by a string that is no longer than $n$ bits.

There exists an $n$-least predicate.

If two predicates differ, then their encodings will also differ.

A predicate can be encoded as a base-2 representation real number
between 0 inclusive and 1 exclusive.
The uncountability of real number implies that there is no algorithm
for determining whether any arbitrary predicate is satisfiable.

However, a predicate can also be encoded as a natural number
(by G\"odel numbering for example).
Natural numbers are countable.

\section{Introduction}

A relation between two sets is a subset of the Cartesian product of those sets.
A \emph{relation} from $A$ to $B$ is a subset of $A \times B$.
A \emph{function} is a single-valued relation:
an injection:
If $a = b$ then $f a = f b$.
If $(a,b) \in f$ and $(a,c) \in f$ then $b = c$.

There are infinitely many functions.
What does it mean for a function to be \emph{computable} or \emph{recursive}?

Constant function $\forall x ~ f x = a$.

Roughly speaking, we say that a function is computable
if and only if it can be stated as a finite arrangement
of certain primitive operations we allow.

For example, if we only allow the constant function,
then the machine will either accept all input or reject all input.

The Turing machine is too much distraction
from the more abstract nature of computation itself
that might be more conveniently be described in.
It gives too much emphasis on \emph{how} to perform a computation.

\section{First attempt}

A \emph{recursive predicate} is a function having type $\mathbb N \to \{0,1\}$.
There is a bijection between $\{0,1\}^*$ and $\mathbb N$
so every function having type $\{0,1\}^* \to \{0,1\}$ can also be considPred{a} recursive predicate.
We say that $x$ \emph{satisfies} $p$ iff $px = 1$.
We say that $p$ is \emph{satisfiable} iff there exists $x$ that satisfies $p$.

The \emph{$p$-satisfaction problem}
and asks for the smallest $x$ that satisfies a given predicate $p$.
A related decision problem is the \emph{$p$-satisfiability problem} that
asks whether a given predicate $p$ is satisfiable.
This satisfiability problem allows us to show that $\PTIME \neq \NPTIME$
by constructing a satisfiable recursive predicate
$p$ in $\DTIME(\Theta n)$ such that
the corresponding $p$-satisfiability is in both $\NTIME(On)$ and $\DTIME(\Omega 2^n)$
where $n$ is the length of the shortest string that satisfies $p$.

An \emph{alphabet} is a finite countable non-empty set.

Many things can be \emph{recursive} or \emph{computable}: sets, functions, languages.

Blum \cite{Blum1967} defined a machine-independent complexity measure?
Blum speedup theorem implies $\EXPTIME = \PTIME$?

Rabin \cite{Rabin1977}?

Chow \cite{Chow1976} introduces the theory concisely.

Who? shows that partial computable functions are isomorphic to natural numbers.
This allows category theory to be used on computability theory.
An algorithm is a natural number.
An algorithm for simulating an algorithm: $\mathbb N \to \mathbb N$.
G\"odel numbering of partial recursive functions.
Formal systems.
Relates completeness, consistency, computability, decidability.

We use typed lambda calculus as our model of computation.

Informally, we construct a function $f$ such that $f$ is easy but $\SDP f$ is hard
such that there is no faster deterministic algorithm
than trying every possible subsequence.
A list of length $n$ has at most $2^n$ subsequences.

There is a bijection between $X^*$ and $\mathbb N$.
the empty string, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, 100, 101, 110, 111
\begin{align}
    \varphi [] &= 0
  \\ \varphi x &= \sum_{k=0}^{\mu x - 1} 2^k + \sum_{k=0}^{\mu x - 1} x_k \cdot 2^k
            \\ &= 2^{\mu x} - 1 + \sum_{k=0}^{\mu x - 1} x_k \cdot 2^k
\end{align}

Time hierarchy theorem?

The deterministic algorithm can use iterative deepening depth-first search:
\begin{align}
    sf &= \text{any $($map $f$ $\xi)$}
    \\
    \xi &= \text{fix} n []
    \\
    nx &= \text{map} (0:) x ++ \text{map} (1:) x
    \\
\end{align}
The nondeterministic algorithm:
\begin{align}
    sf &= gf[]
    \\
    gfx &= fx \vee \ambc (gf(0:x)) (gf(1:x))
\end{align}

\section{Type theory}

\section{Motivation}

The type theory described here is inspired by
Martin-L\"of type theory
and the Haskell programming language.
The type theory is constructive (intuitionistic) and dependent.
This theory shall help us elucidate ideas that
would be otherwise cumbersome using set theory.
Perhaps this theory can translate the $\PTIME$ vs $\NPTIME$ problem
into something easier.
This theory will be the foundation
of the next chapters.

\section{Notation}

Throughout this book, we will use the notation
\[ a : b \]
to mean that \emph{the type of $a$ is $b$}.
We can also say that $a$ \emph{inhabits} $b$
or that $a$ is an \emph{inhabitant} of $b$.
Every \emph{term} has a type.

If $a$ is a type then we write $|a|$ to mean the \emph{cardinality} of that type.
Informally, we can say that the cardinality of a type
is the size of a type, that is the number of inhabitants of that type.
This is slightly trickier when the type has infinitely many inhabitants.

The juxtaposition (concatenation) $f x$ means the \emph{application} of $f$ to $x$.
Functions are \emph{curried} and \emph{partially applicable}.
The expression $f x y$ means $(f x) y$:
apply to $y$ the result of applying $f$ to $x$.
Function application associates to the left.
Function type associates to the right.
The type $a \to b \to c$ means $a \to (b \to c)$.
If $f : a \to b \to c$ and $x : a$ then $f x : b \to c$,
just like in typed lambda calculus.
Do not confuse this with the function composition
\[ f \circ g = \lambda x \to f (g x). \]
This associates to the right?
Does associativity matter here?
\[
    f \circ g \circ h = f \circ (g \circ h)
\]


\section{Meet the types}

\begin{itemize}
    \item
        $\Void$ is the empty type.
        It has no inhabitant.
        We will rarely have anything to do with this type.
    \item
        $\Bool$ is the type of boolean values.
        This type has two inhabitants: $\true$ and $\false$.
    \item
        $\Bit$ has two inhabitants: $0$ and $1$.
    \item
        $\Fun{a}{b}$ is the type of functions from $a$ to $b$.
        The function can be partial.
        This type is also be written $a \to b$.
    \item
        $\Pred{a}$ is the type of
        logical predicates about objects of type $a$.
        We define
        \[ \Pred{a} = \Fun{a}{\Bool}. \]
    \item
        $\Nat$ is the type of natural numbers.
        This type is defined using Peano axioms.
    \item
        $\Set{a}$ is the type of the set of elements of type $a$.
    \item $\List{a} = \Kleene{a}$ is the Kleene closure of $a$.
        A list of $a$ is an ordered collection of elements of type $a$ with duplicates allowed.
        so we can write $[x,y,z] 1 = y$.
    \item $\Bits$ is the type of bitstrings.
        \[
            \Bits = \Kleene{\Bit}
        \]
    \item $\Either{a}{b}$ is the sum type or the union type
        that consists of $\Left{x}$ for all $x : a$
        and $\Right{y}$ for all $y : b$.
    \item $\Pair{a}{b}$ is the product type
        that consists of $(x,y)$ for all $x : a$ and $y : b$.
    \item
        $\Vect{a}{n}$ is the type of vectors
        whose element type is $a$ and whose length is $n : \Nat$:
        \[ \Vect{a}{n} = \Fun{(I n)}{a} \]
        where
        \[ I n = \{ 0, 1, 2, \ldots, n - 1 \} \]
        or alternatively using predicate logic
        \[ \Fa{x} (x : \Nat \wedge x < n \iff x : I n) \]
    \item $\Typ$ is the type of types.
        This implies
        \[ \Typ : \Typ \]
        (Does $\Typ : \Typ$ imply inconsistency?
        Russell's paradox?
        Unrestricted comprehension?)

        This means that $\sfSet$ and $\sfRel$ can be thought as the \emph{type functions}
        \begin{align*}
            \sfSet &: \Typ \to \Typ,
            \\
            \sfRel &: \Typ \to \Typ \to \Typ.
        \end{align*}
\end{itemize}

The cardinality of $\Nat$ is $\aleph_0$ (aleph-null).

\section{Cardinality of types}

See also: cardinality, cardinal arithmetics,
aleph number, beth number, transfiniteness, Georg Cantor,
New Foundation, axiomatic set theory, Willard Van Orman Quine, type theory.

$|\List{a}| = |\Fun{\Nat}{a}|$.

To compute the cardinality of $\Fun{a}{b}$,
imagine that there are $|a|$ boxes.
Each box has exactly one out of $|b|$ possible contents.
The number of possible functions is thus $|b|^{|a|}$.

If a type has finitely many inhabitants,
then the cardinality of that type is the number of its inhabitants.

If there is a bijection between two types,
then those types have the same cardinality.

Type-theoretic restatement of Cantor's theorem?
There is no bijection between $a$ and $\Set{a}$.
$|a| < |\Set{a}|$.

Kind of ordering on cardinalities:
\begin{itemize}
    \item Iff there is an injection from $a$ and $b$, then $|a| \le |b|$.
    \item Iff there is an surjection from $a$ and $b$, then $|a| \ge |b|$.
    \item Iff there is a bijection between $a$ and $b$, then $|a| = |b|$.
\end{itemize}

Two sets are equinumerous (have the same cardinality) if and only if there is a bijection between them.
Given the cardinality of $A$, what is the cardinality of $A^*$?
There is a bijection between $\{0,1\}^*$ and $\mathbb{N}$?
Cantor's theorem?

$|\sfT a| = |\sfT (\Set{a})|$?

\section{Cardinality theorems}

\begin{mthm}
    \[
        |a| \lneq |\Set{a}|
    \]
\begin{proof}
    Has been proved by Georg Cantor using Cantor's diagonal argument
    that the cardinality of a set is strictly less than its power set.
    Beth numbers.
    $\beth_n \lneq \beth_{n+1}$ for each natural number $n$.
\end{proof}
\end{mthm}

\begin{mthm}[Equinumerosity among one-parameter types]
    For each $a$, all these types have the same cardinality:
    $\Pred{a}$, $\Set{a}$.
    \begin{proof}
        Let $p : \Pred{a}$ be a predicate and $s : \Set{a}$ be a set.
        We define $p$ and $s$ such that each object that satisfies the predicate $p$ is an element of the set $s$
        and also such that each element of the set $s$ satisfies the predicate $p$.
        \begin{align*}
            F p &= \{ x \,|\, p x \}
            \\
            G s &= \lambda x \to x \in s
        \end{align*}
        The relationship is
        \[ \FA{x} (p x \iff x \in s) \]

        But what if $p x = x \not\in S$.
        Or what if $p x = \neg\exists S ~ x \in S$?
        Or what if $p x = \Fa{S} x \in S$?
        Or what if $p x = x \in x$?
        What if $p x = \neg (p x)$?
        Isn't this prone to Russell's paradox?
        Unrestricted comprehension?
        FIXME?

        Or is this not prone?
        $p$ cannot refer to $s$?
        Can it?
    \end{proof}
\end{mthm}

Thus a predicate is a set and a set is a predicate.
It turns out that there is a name for this concept:
that set is the \emph{extension} of that predicate.
If $p$ is a predicate, then $p$ is also a set,
so we can write $x \in p$ to mean that $p x$ is true.
What if we assume that a predicate is equal to its own extension?
Now we make a bold but reasonable claim:
a predicate \emph{is} a set and a set \emph{is} a predicate.
This has some interesting consequences.

If we assume the equality, then $p$ becomes a fixed point of $\phi \mu$.
To see this, we have to define several functions.
Let $\phi$ be the flip combinator, that is $\phi f x y = f y x$.
Let $\mu$ be the set membership function, that is $\mu x y = x \in y$.
Recall that the $\eta$-reduction transforms $p x = q x$ to $p = q$.
\begin{align*}
    p x &= x \in s
    \\
    &= \mu x s
    \\
    &= \phi \mu s x
    \\
    p &= \phi \mu s
    \\
    p &= s
    \\
    p &= \phi \mu p
    \\
    p &= \phi \mu (\phi \mu p)
    \\
    &= \phi \mu (\phi \mu (\phi \mu p))
    \\
    &= \ldots
\end{align*}
That implies that we can write strange but provable things like these:
\begin{align*}
    1 \in \{0,1,2\} &= \{0,1,2\} 1 = \true
    \\
    3 \in \{0,1,2\} &= \{0,1,2\} 3 = \false
    \\
    (\lambda x \to x = 1) 1 &= 1 \in (\lambda x \to x = 1) = \true
\end{align*}
but this can be confusing at first.
Should we distinguish predicate and set?
Should we treat them as the same thing?
The membership operator $\in$ becomes swapped function application.

We can even generalize the notation $f x = x \in f$ to every function $f : a \to b$, not just predicates.
Let $f x = x + 1$. Then $f 0 = 0 \in f = 1$.
This may need some effort and time to get accustomed to,
but once you master it, you will be another mathematician.

\begin{mthm}[Equinumerosity among two-parameter types]
    All these types have the same cardinality:
    \begin{itemize}
        \item $\Relab{a}{b}$, $\Relab{b}{a}$
        \item $\Pred(a,b)$, $\Pred(b,a)$
        \item $\Set{(a,b)}$, $\Set{(b,a)}$
        \item $\Fun{a}{(\Set{b})}$, $\Fun{(\Set{a})}{b}$
    \end{itemize}
    \begin{proof}
        Proving $|\Relab{a}{b}| = |\Relab{b}{a}|$ is simple.

        Proving $|\Pred(a,b)| = |\Pred(b,a)|$ is simple.

        $r : \Relab{a}{b}$ and $p : \Pred(a,b)$ and $f : a \to b \to \Bool$.
        $r$ relates $x$ to $y$ if and only if $p(x,y)$ is true.
        \begin{align*}
            p z &= z \in r
             \\ &= \mu z r
             \\ &= \phi \mu r z
            \\
            p &= \phi \mu r
        \end{align*}
        Then let $p = r$.

        Since there is a bijection between $\Pred{(a,b)}$ and $\Relab{a}{b}$
        and between $\Pred{a}$ and $\Set{a}$,
        there is a bijection between $\Relab{a}{b}$ and $\Set{(a,b)}$.

        To prove that there is a bijection between $\Relab{a}{b}$ and $\Fun{a}{(\Set{b})}$,
        we choose any $r : \Relab{a}{b}$ that is a relation
        from objects of type $a$ to objects of type $b$.
        Define the \emph{image of $x$ in $r$} as
        $i r x = \{ y \,|\, \text{$r$ relates $x$ to $y$} \}$
        where the type of $i$ is $\Relab{a}{b} \to a \to \Set{b}$.
        We define the \emph{relation functionization} function $F$ as
        \[ F r = \{ (x,Y) \,|\, i r x = Y \} \]
        we capitalize $Y$ to highlight the fact that it is a set.
        $G : \Fun{a}{(\Set{b})} \to \Relab{a}{b}$ is the \emph{function relationization} function.
        \[ G f = \{ (x,y) \,|\, y \in f x \} \]
        $G f$ relates $x$ to $y$ iff $y \in f x$.
        We can see that $F(G f) = f$ and $G(F r) = r$.
        Thus $F \circ G$ is the identity of $\Fun{a}{(\Set{b})}$
        and $G \circ F$ is the identity of $\Relab{a}{b}$.
        Thus $F$ and $G$ are inverses of each other.

        ???
    \end{proof}
\end{mthm}

There is a mapping from $\Fun{a}{b}$ to $\Relab{a}{b}$.
There is a bijection between $\Relab{a}{b}$ and $\Relab{b}{a}$.
There is a bijection between $\Relab{b}{a}$ and $\Fun{b}{(\Set{a})}$.
This means that there is a bijection between $\Fun{a}{(\Set{b})}$ and $\Fun{b}{(\Set{a})}$.

\section{Relationship between type theory and graph theory}

A type $a$ with an endorelation $r : \Relab{a}{a}$
is isomorphic to a graph $(V, E)$
where $V$ is the set of all inhabitants of $a$
and $E : \Relab{a}{a}$.

Graph with uncountably infinite vertices.
Almost a topological space?

\section{Machines}

\section{Machine types}

We define the \emph{type of a deterministic machine with configuration type $c$} as
\[ \DMach{c} = \Pair{(\Set{c})}{(\Fun{c}{c})} \]
Compare this to the type of a nondeterministic machine
with the same configuration type,
which differs only in the type of the second element of the pair
by replacing $\sfFun$ with $\sfRel$:
\[ \NMach{c} = \Pair{(\Set{c})}{(\Relab{c}{c})}. \]
We can generalize this by parametrization to
the \emph{type of a machine with configuration type $c$
and transition kind $t$},
\[
    \Mach{t}{c} = \Pair{(\Set{c})}{(t\,c\,c)},
\]
so that we can define in hindsight that
\begin{align*}
       \sfDMach &= \Macha{\sfFun},
    \\
       \sfNMach &= \Macha{\sfRel}.
\end{align*}

\section{Inhabitants of the machine types}

A \emph{deterministic machine $D$ with configuration type $c$}
(an inhabitant of $\DMach{c}$) is
\begin{align*}
    D &= (I,\beta) : \DMach{c}
\end{align*}
where
\begin{itemize}
    \item the set $I : \Set{c}$ is the set of all \emph{initial configurations} of the machine, and
    \item the function $\beta : \Fun{c}{c}$ is the \emph{transition function} of the machine.
\end{itemize}

\section{Turing machines}

The machine we have been talking about
can be more general than a deterministic Turing machine.
To make a machine that is equivalent to a Turing machine,
the configuration type must be countable
and the transition function must be recursive.
\begin{align*}
    \DTM &= \Pair{(\Set{\Bits})}{\RecFun}
    \\
    \NTM &= \Pair{(\Set{\Bits})}{\RecRel}
    \\
    \RecFun &< \Fun{\Bits}{\Bits}
    \\
    \RecRel &< \Relab{\Bits}{\Bits}
\end{align*}

If and only if $|\RecFun| = |\RecRel|$,
nondeterminism does not let the machine compute any more function.

Recall that a predicate is a set and a set is a predicate.
The set $I$ is a subset of $C$.
Let $C$ be the set of all inhabitants of the configuration type $c$.

\section{Introduction}

A \emph{configuration} of a machine is a state of that machine at a certain time.
For example, a configuration of a Turing machine is a tuple of its state,
its head positions, and the contents of its tapes.
A \emph{machine} is a \emph{transition function}.
This transition function depends on the set of primitive operations of the machine.
Seen the other way around, this transition function
determines the set of primitive operations of the machine.

A machine is an embodiment of an algorithm.

A machine performs computation by repeatedly
making a transition from its current configuration
according to its transition function
until it reaches a terminal configuration.

A \emph{primitive operation} maps a configuration to a configuration.
Every primitive operation represents a computation that the machine can do in one unit time.
An \emph{architecture} is a set of primitive operations
and a set of rules for evaluating expressions built using those primitive operations.
A \emph{machine} is an architecture and a configuration representing its current state.

\section{Deterministic machines and graph theory}

We can see the tuple $(C,\beta)$ as a \emph{directed graph}
where each configuration is a vertex in the graph
and there is an edge from $x$ to $y$ if and only if $\beta x = y$.
An \emph{initial configuration} is a possible configuration from which the machine starts computing.
An initial configuration contains an input for the algorithm running on the machine.
A deterministic machine always starts computing from an initial configuration
and always either goes into infinite cycle or ends at terminal configuration.

\section{Termination}

We say that a configuration $x$ is \emph{terminal} iff
there is no $y$ satisfying $\beta x = y$, that is iff there is no $y$ such that $(x,y) \in \beta$.
In the graph, such configuration $x$ is terminal iff it has zero out-degree,
that is iff there is no edge from that configuration in the graph.

When there is ambiguity about which machine we are discussing,
we will write something like `terminal $D$-configuration'
or `$\beta$-terminal configuration' instead of just `terminal configuration'.

We say that a configuration $x$ \emph{eventually terminates}
iff there exists a natural number $n$ such that $\beta^n x$ is terminal.
Such configuration $x$ eventually terminates iff repeated application of $\beta$ to $x$
eventually produces a terminal configuration.
In the graph, such configuration $x$ eventually terminates iff
there is a path from $x$ to a terminal configuration.

\section{What do we mean by computation?}

A \emph{computation} is a path in the graph.
More precisely, a computation from $x_0$ to $x_{n-1}$
is a path $[x_0, x_1, \ldots, x_{n-1}]$
such that $x_{k+1} = \beta x_k$ for each natural number $k$ from $0$ to $n-1$.
We can see by induction that $x_k = \beta^k x_0$ for each natural number $k$ from $0$ to $n-1$.

A \emph{complete computation} is a computation that begins at an initial configuration
and ends at a terminal configuration.
Subcomputation is to computation as subpath is to path.
The graph can be seen as a set of complete computations.

We say that the deterministic machine $D = (I,\beta)$ \emph{computes} a function
$f : A \to B$
that is isomorphic to
$f_N : \mathbb{N} \to \mathbb{N}$
that is in turn also isomorphic to
$f_D : I \to T$
where $T$ is the set of all terminal configurations of the machine
where $f_D$ is defined as the following function:
\begin{equation}
    f_D = \{ (x,y) \,|\, x \in C \,\wedge\, x \text{ eventually terminates at } y \}
\end{equation}
Formally we say $f_D x = y$ iff there exists
a natural number $n$ such that $\beta^n x = y$ and $y$ is terminal.
The functions $p$ and $q$ are the \emph{input encoding} function
and the \emph{output encoding} function.
If we ignore the time used by the machine,
we can see the machine as a function from $I$ to $T$.
$q \circ f = f_D \circ p$.

State every natural number $n$ as the sum of $p$ and $q$
where $p$ is the biggest prime less than $n$.

The encoding and decoding functions allow us to make
a distinction between a natural number
and the binary representation of natural number.

\section{A universal machine to simulate other machines?}

\[
    \UDMach{c} = \DMach{(\Pair{(\Fun{c}{c})}{c})}
\]

Machine or algorithm can be encoded as string.
\emph{Universal machines} simulate every machine.
A program corresponds to a \emph{partial computable function}.
Partial is not total.
Total function is a function defined for each element in its domain.
Partial function is a function that can be undefined for any number of elements in its domain.

The \emph{$c$-universal machine} of type $\UDMach{c}$ can compute every inhabitant of $\DMach{c}$.
The configuration type of the universal machine is $\Pair{(\Fun{c}{c})}{c}$.
\[
    I_u = \{ (\beta,x) \,|\, (I,\beta) : \DMach c \,\wedge\, x \in I \}
\]
Can a universal machine simulate all universal machines?
Let $t = (\Fun{c}{c},c)$.
Is $|t| = |(\Fun{t}{t},t)|$?
If and only if yes, then a universal machine can simulate all universal machines.

A universal deterministic machine is $U = (C_u, I_u, \beta_u)$
where the universal configuration set is $C_u = (C \to C, C)$.
A configuration of that universal machine is a tuple $(\beta, x)$.
This machine can compute everything every other machine can compute.
The universal machine transitions from $(\beta, x)$ to $(\beta, y)$
if and only if the simulated machine transitions from $x$ to $y$.
Let $u$ be the pairing function, that is $uab = (a,b)$.
Then
\begin{equation}
    u \beta \circ \beta = \beta_u \circ u \beta
\end{equation}
The universal transition function $\beta_u$ can be seen as a set:
\begin{equation}
    \beta_u = \{ ((\beta,x), (\beta,y)) \,|\, (x,y) \in \beta \}
\end{equation}
The inverse of the partially applied pairing function $u a$ is $r$
where $r (a,b) = b$.

\section{Recursive Function over Bitstrings}

Every inhabitant of $\Bit \to \Bit$ is recursive.

Every inhabitant of $\Bit \to \Bit \to \Bit$ is recursive.

For all $a$, the \emph{head} function $f [x 0, x 1, \ldots] = x 0$,
where $f : \List{a} \to a$, is recursive.

For all $a$, the \emph{tail} function $f [x 0, x 1, \ldots] = [x 1, \ldots]$,
where $f : \List{a} \to \List{a}$, is recursive.

Every well-formed well-typed lambda expression
that involves only recursive functions is recursive?

\section{Nondeterminism from two deterministic machines}

The author got the idea of defining a nondeterministic machine
using two deterministic transition functions
from Arora and Barak \cite[p.~40]{Arora2009}.
We can form a nondeterministic machine $N$
from two deterministic machines $D_0 = (C,I,\beta_0)$ and $D_1 = (C,I,\beta_1)$
(both machines must have the same configuration set and the same initial configuration set)
as follows:
\begin{enumerate}
    \item The sets of initial configurations are the same.
    \item If $x$ is a configuration of $D_0$ then $(0,x)$ is also a configuration of $N$.
    \item If $x$ is a configuration of $D_1$ then $(1,x)$ is also a configuration of $N$.
    \item Nothing else is a configuration of $N$.
    \item $\beta = P 0 \beta_0 \cup P 1 \beta_1$ where $P c X = \{ (c,x) \,|\, x \in X \}$.
    \item $x$ is $\beta$-terminal iff $x$ is $\beta_0$-terminal or $\beta_1$-terminal or both.
    \item Nothing else is $\beta$-terminal.
\end{enumerate}

Sum types: $\Either{a}{b} = \Left a | \Right b$.
Iff $x : a$ then $\Left x : \Either{a}{b}$.
Iff $y : b$ then $\Right y : \Either{a}{b}$.
The configuration type is $\Either{c_0}{c_1}$.
\[
    I =
    \{ \Left x \,|\, x \in I_0 \}
    \cup
    \{ \Right x \,|\, x \in I_1 \}
\]
\begin{align*}
    (\Left x, \Left y) \in \beta \iff (x,y) \in \beta_0
    \\
    (\Right x, \Right y) \in \beta \iff (x,y) \in \beta_1
\end{align*}

$\NMach c$.
$\beta : \Relab{c}{c}$.
Does $\Fun{c}{c} < \Relab{c}{c}$ strictly?

We can also form a deterministic machine $D$ from a nondeterministic machine $N$.

A machine computes a function $f : X \to Y$ as follows:
For each $x$ such that $f x$ is defined, the string $\hat x$ (the string encoding of $x$)
is an initial configuration of the machine.
For each $y = fx$, the configuration $\hat y$ (the configuration encoding of $y$)
is a terminal configuration of the machine.
There is a path from $\hat x$ to $\hat y$.
There is a machine that computes this $f$ in unit time: the oracle of $f$.
Then there is another machine that computes it asymptotically slower.
Then there is another machine that computes it even slower asymptotically.
This goes on and on.
We can always invent a slower machine.

\begin{enumerate}
    \item
        Every configuration of $N$ can branch to \emph{at most two} configurations.
        Formally for each configuration set $X$, it holds that $\mu(B X) \le 2 \cdot \mu X$
        where $\mu X$ is the size of $X$.
\end{enumerate}

$\NMach c$.

\section{Nondeterminism}

$N = (I,\beta) : \NMach c$
where $I : \Set{c}$ and $\beta : \Relab{c}{c}$.

Let $N$ be a nondeterministic machine $(C,I,\beta)$ where
Let $\beta \subseteq C \times C$ is the transition relation of $N$.
The tuple $(C,\beta)$ forms a graph.
There is an edge from $x$ to $y$ iff $(x,y)$ is in $\beta$.
with the following properties:
\begin{align}
    B^n X &= \{ (x,y) \,|\, x \in X \wedge (x,y) \in \beta^n \}
\end{align}
where $B X$ means $B^1 X$.

The relation $\beta$ can also be seen as an equivalent function $B : \powerset C \to \powerset C$
where $\powerset C$ is the power set of $C$.
Thus for every nondeterministic machine $(C, I, \beta)$
there is a deterministic machine $(\powerset C, \powerset I, B)$
that computes the same function at the same time.
But this deterministic machine is not countable.
Contradiction.
So nondeterminism bestows some power that deterministic machines cannot have.
There exists nondeterministic machine that cannot be simulated in equal time by deterministic machine.
(Is this a jump in logic? This seems wrong.)

But every deterministic machine can simulate
every nondeterministic machine by using breadth-first search.
So nondeterminism does not add any power?

Every deterministic machine is trivially a
nondeterministic machine since every transition function is a transition relation.

Is the set of functions computable by the nondeterministic machines
the same as that of deterministic machines?
Does it add any power?

\section{Uncomputable functions}

Each machine has infinitely many functions that it \emph{cannot compute}.
We prove this by diagonalization.
Suppose that the set of functions computable by a machine is $F = \{ f_0, f_1, f_2, \ldots \}$.
Suppose also that there is no function outside that set.
Alas, we can define a function $f$
such that $f0 \neq f_0 0$, $f1 \neq f_1 1$, $f2 \neq f_2 2$, and so on
such that $fn \neq f_n n$ for each natural number $n$.
This $f$ is not equal to any function in $F$ so it cannot be in that set.
Contradiction.

Nondeterministic machine is a $\sigma$-algebra of deterministic machine?
Can we relate nondeterminism to topology?

We can make it a topological space?
Nondeterministic reversible computation?
Remember that a configuration of a nondeterministic machine is a set.
Allow the no-computation: for each configuration $x$, make $x \in N x$.
Then make it reversible:
if $y \in N x$ then $x \in N y$.
Then make each subset of $B x$ to be in $N x$.
We can $(C,N)$ where $C$ is the set of all configurations of the machine
and $N : C \to 2^C$ is the neighborhood function.

Ullman? Sipser? defines NTM as DTM with transition relation instead of transition function.

We define $\beta^n$ as a composition of $n$ instances of $\beta$:
$\beta^0 = \text{id}$ and $\beta^1 = \beta$ and $\beta^n = \beta \circ \beta^{n-1}$.
The time complexity of a computation $x$
is the smallest $n$ such that $\beta^{n+1} x = \beta^n x$.

\section{Major theme in computability theory}

The idea is to encode almost \emph{everything}
(machines, algorithms, configurations, computations, numbers, graphs, you name it)
as a \emph{string} and then establish a bijection between strings and natural numbers.

Time-limited simulation of another machine:
simulate the machine as long as the number of steps do not exceed $f n$.
If the number of step exceeds $f n$ and the machine still does not terminate,
output $0$.
Truncated language?

A particular kind of machine called the \emph{recursive machines}
is equivalent to Turing machines with polynomial-time speedup/slowdown.

For each machine with transition function $t$ there exists another machine
with transition function $t' = t^p$
providing polynomial speedup.
Space usage of configuration $x$.
Length of program plus length of input.

\section{Complexity}

\section{Usage of computational resources}

There are two computational resources we concern ourselves with:
time and space.
The unit of time is arbitrary.
Number of steps. Each step takes the same amount of time.
Each configuration transition uses the same amount of time.

The time used by that computation is the length of the corresponding path.
Formally the time required by the machine
to move from configuration $a$ to $b$ is
the smallest natural number $n$ such that $\beta^n a = b$.
By definition, the machine \emph{moves in one unit time} from configuration $x$ to $y$ iff $\beta x = y$
so $\beta$ depends on the set of primitive operations we equip the machine with.

The \emph{time-to-termination} of a configuration $x$,
written $\TC \beta x$,
is the smallest natural number $n$ such that $\beta^n x$ is terminal.
The time-to-termination of $x$ is the length of the shortest path from $x$ to a terminal configuration.

The maximum space usage of a computation is
the maximum of the space usages of the vertexes of the corresponding path.
Now suppose that we write $s x$ as the \emph{space usage} of configuration $x$.
The space usage of an initial configuration is the size of the input.
We define $\SC \beta x$,
the \emph{maximum-space-to-termination} of a computation from $x$
as follows:
\begin{equation}
    \SC \beta x = \max_{c \in C \beta x} s c
\end{equation}
where we define
$C \beta x = \{ \beta^k x \,|\, k \in \mathbb N \}$
as the set of all configurations
that can be reached from $x$ by following $\beta$.

\section{Time class}

What can a machine compute in a given number of steps?
Given more time, does a machine compute more?

What is the relationship between the size of the input and the running time?
What is the relation between the size of
an initial configuration and its time-to-termination?
How fast does the time-to-termination grow?

We are talking about a special kind of problems
whose time usage depends only on the length of the input.

A \emph{time class} is a set of all initial configurations that have the same time-to-termination.
Define the \emph{time class $n$ of machine $D$},
written $\Teq D n$ (or just $\Teq n$ when the machine is clear in the context),
as the set of every initial $D$-configuration
whose time-to-termination equals $n$.
Then define $\Teqsum n$, the \emph{max-time class $n$},
as the set of every initial configuration whose time-to-termination is less than $n$:
\begin{equation}
    \Teqsum n = \bigcup_{k = 0}^{n - 1} \Teq k
\end{equation}

For example, if the machine ignores its input and runs in constant time,
$\Teqsum 1 = I$.

\newcommand{\Cset}{\mathbf{C}}
Define $\Cset n$ as the set of every initial configuration whose size is less than $n$.
The \emph{time simplicity function of the machine} is
\begin{equation}
    K n = \frac{\mu(\Teqsum n)}{\mu(\Cset n)}
\end{equation}
(Find a better name?)
Higher $K n$ means faster computation.
The \emph{space simplicity function} of the machine is defined analogously:
\begin{equation}
    J n = \frac{\mu(\mathbf S n)}{\mu(\Cset n)}
\end{equation}

If the alphabet used is the binary alphabet $\{ 0, 1 \}$
and the input is a natural number encoded using the usual binary positional encoding,
then $\mu (C n) = 2^n$ and thus $\mu (\Cset n) = 2^{n + 1} - 1$.

If every instance $x$ of size $\mu x$ is solved by machine
(if the time-to-termination of each initial configuration $x$ of size $\mu x$ is less than $f(\mu x)$)
then we say that the problem is size-bounded by $f$.

Is there an $m$ such that for all $n > m$, we have $\Teq n = \varnothing$?
Only if the machine does not read all its input?

Now we examine the relation between the size of the initial configuration
and the time to termination of that configuration.
Intuitively, when the problem is non-trivial, as the initial configuration grows larger,
the machine should require more time to arrive at the related terminal configuration.
If the problem is non-trivial then a bigger initial configuration
should imply a higher time to termination of that configuration.

The minimum time-to-termination of every initial configuration of size $n$.

The maximum size of every initial configuration with time-to-termination of $n$.

The sets of functions computable by deterministic and nondeterministic machines are the same?

$\Teq n$ may be empty?

$\Teq n$ and the length of.

Define space equivalence class $n$ as $\Seq n$ as the set of
every configuration that has a maximum-space-to-termination of $n$.

Both $\TC$ and $\SC$ are functions.
Are they computable?
$C \beta$ is also a function.

Every configuration following $x_0$ is fully determined by $x_0$.
The computation is fully determined by its first configuration.
A machine is deterministic in the sense that given a configuration,
the computation that proceeds from that configuration is always the same.
What can graph theory say about computation?
What result in graph theory helps us discover something in computation theory?
Weighted graph can model computation with nonuniform transition time.
But for what?
Modeling modern machines with speculative execution?

We say that an algorithm $a$ is in $\TIME(O(f n))$ where $n$ is the length of the input iff
for infinitely many (but not necessarily every) $i$ in each initial configuration
$x \equiv (a,i)$ that eventually terminates, we have $\TC x \in O(f(s x))$.

Determining if a number is divisible by $m$ is a constant-time operation in a machine with alphabet of size $m$,
but is it also constant-time operation in a machine with another base that is coprime to $m$?

\section{Fair coding scheme}

How to code almost anything into a bit string?

How to code members of type $t$ into bits?

How do we constrain the coding function so that meticulous mathematicians
cannot cheat by shifting the computation into the coding function?

Let $\mu t : t \to \Nat$ be the \emph{canonical measure} of an element of the countable type $t$.

Let $c t : t \to \Bits$ be the coding function.
This coding function must be measure-preserving, size-preserving.
Formally the coding function must satisfy
\[
    \mu t x \le \mu t y \iff \mu \Bits (c t x) \le \mu \Bits (c t y)
\]
for each $x : t$ and $y : t$.

\section{Terminology and notation}

\begin{mdef}[list, string]
    A \emph{list} $x$ of $X$ of length $n$ is $[x_0,x_1,\ldots,x_{n-1}]$
    where each $x_k$ has the type $X$.
    The list $x$ has type $X^*$ (the Kleene closure of $X$).
    The empty list is written $[]$.
    Lists are also known as \emph{strings}.
\end{mdef}

\begin{mdef}[problem, instance, language]
    An \emph{instance} is a string.
    A \emph{problem} is a set of instances.
    A problem is also called a \emph{language}.
\end{mdef}

\begin{mdef}[complexity class]
    A \emph{complexity class} is a set of problems.
\end{mdef}

\begin{mdef}[time complexity of an expression]
    We write $\TC_M e$ to mean the number of time units required by machine $M$
    to evaluate expression $e$.
    Sometimes $M$ is omitted so we write only $\TC e$
    such as when the machine is not too relevant
    or is clear in the context.

    To evaluate an expression is to reduce it until it becomes a value.
    A value is an expression that reduces to itself.
\end{mdef}

\begin{mdef}[decider]
An \emph{$X$-decider} is an implementation of a function having the type $X \to \mathbb B$.
We say that the decider \emph{accepts} the input $x$ iff $fx$ is true.
The \emph{language} recognized by the decider is the set
\begin{align*}
    \langset f = \{ x ~|~ x : X, ~ fx \}
\end{align*}
\end{mdef}

\begin{mdef}[lower bound]
    We say that $b$ is a time usage lower bound of decider $f$ iff $fx \in \Omega b$.
    We say that $b$ is the time usage infimum of that decider iff
    for all $c$ that is a time usage lower bound of $f$,
    $b$ is a time usage lower bound of $f$ and $b \in \Omega c$.
\end{mdef}

\begin{mdef}[decider set]
    The \emph{decider set} of a language $L$, written $\decset L$, is
    the set of all deciders whose language is $L$.
    \begin{align}
        \decset L = \{ f ~|~ \langset f = L \}
    \end{align}
\end{mdef}

\begin{mdef}[decider set time usage ordering]
    We say $f \leTC g$ iff $\forall x \in X^* ~ \TC(fx) \in O(\TC(gx))$.
\end{mdef}

\begin{mdef}[minimum-time decider]
    $f$ is a \emph{minimum-time decider} of $L$ iff for each $g \in \decset L$,
    it holds that $\forall x \in X^* ~ \TC(fx) \in O(\TC(gx))$.
    In other words, such $f$ is an infimum of $\decset L$ according to $\leTC$.
\end{mdef}

\begin{msco}[nonexistence of time usage supremum]
    $\decset L$ has no supremum according to $\leTC$.
    For each decider $f \in \decset L$, there exists an asymptotically slower $g \in \decset L$.
    \begin{align}
        \forall f \in \decset L ~ \exists g \in \decset L ~ \forall x \in X^* ~ \TC(fx) \in o(gx)
    \end{align}
\end{msco}

\begin{mcor}[decider set equivalence class]
    We can define two deciders as $f$ and $g$
    as equivalent iff for all input $x$,
    their time usage is asymptotically similar.
    \begin{align}
        \mathcal E m p L = \{ f ~|~ \langset f = \decset L, ~ \forall x \in X^* ~ \TC_m(fx) \in \Theta(\TC_m(px)) \}
    \end{align}
\end{mcor}

\begin{msco}[NTIME upper bound]
    Let $N$ be a nondeterministic machine.
    The number of time units required by $N$
    to evaluate $\SDP fx$ is bounded above as follows
    where $n$ is the length of $x$:
    \begin{align}
        \TC_N(\SDP fx) \in O\left(n + \max_{s \sqsubseteq x} (\TC(fs))\right)
    \end{align}
\end{msco}

\begin{msco}[DTIME upper bound]
    Let $D$ be a deterministic machine.
    The number of time units required by $D$
    to evaluate $\SDP fx$ is bounded above as follows
    where $n$ is the length of $x$:
    \begin{align}
        \TC_D(\SDP fx) \in O \left( \sum_{s \sqsubseteq x} \TC(fs) \right)
    \end{align}
\end{msco}

\begin{mdef}[$g$-transformer]
    A \emph{$g$-transformer} of a decider $f$ is a function $g : X^* \to X^*$
    such that $fx = f(gx)$ for all $x : X^*$.
    If $\TC(fx) \in o(\TC(f(gx)))$ then
    we call the $g$-transformer a \emph{$g$-speedup}.
    On the other hand if $\TC(fx) \in \omega(\TC(f(gx)))$ then
    we call the $g$-transformer a \emph{$g$-slowdown}.
\end{mdef}

\begin{mcon}[DTIME lower bound]
    For each deterministic machine $D$,
    $\TC_D(\SDP fx) \in \Omega(m^n)$ for all $f : X^* \to \mathbb B$
    and $x : X^*$
    where $|X| = m$
    and $|x| = n$.
\end{mcon}

\begin{mcon}[existence of a certain decider]
There exists an alphabet $X$
and an $X^*$-decider $f$ such that
for each list $x : X^*$ of length $n$,
the following two statements hold
\begin{align}
    \TC_D(\SDP fx) &\in \Omega 2^n
    \\
    \TC_N(\SDP fx) &\in O n
\end{align}
where $D$ and $N$ are the deterministic machine
and the nondeterministic machine from the two previous conjectures.
If such $f$ exists, then $\PTIME \neq \NPTIME$.
\end{mcon}

\begin{mcon}
    If $f = f \circ g$ for all $f$ then $g = \id$.
\end{mcon}

\begin{mcon}[existence of a PNP decider]
Such decider cannot have overlapping subproblem
and cannot have optimal substructure.
There is no mathematical identity that allows decider to be computed faster.
If
\begin{align}
    \neg\exists p ~ \forall x \in \{ a | a \in X^*, p a \} ~ \exists y ~ fx = fy
\end{align}
then $f$ exists.
\end{mcon}

Proof by contradiction:
suppose that for every decider,
there exists a mathematical identity that allows it to be computed faster.
\begin{align}
    \forall f ~ \exists p ~ \forall x \in \{ a | a \in X^*, p a \} ~ \exists y ~ fx = fy
\end{align}
Then what?

\section{Machines}

\begin{mdef}[standard machine]
    The standard machine with alphabet $X$
    has the following primitive operation set:

    Every constant of type $X$.

    Every unary operation of type $X \to X$.

    Every binary operation of type $X \to X \to X$.

    Every ternary operation of type $X \to X \to X \to X$.
    One of these ternary operations is the three-way branch operation:
    $pctf$ is $t$ iff $c \neq 0$; $f$ otherwise.

    Function application.

    Function composition.

    Some list operations: $\fnull : X^* \to X$,
    $\fhead : X^* \to X$,
    $\ftail : X^* \to X^*$,
    and $\fcons : X \to X^* \to X^*$.

    A fixed-point combinator $y : \forall a ~ (\alpha \to \alpha) \to \alpha$ satisfying $yf = f(yf)$
    such that the expression $yf$ reduces to $f(yf)$ in one time unit.
\end{mdef}

\section{Recursion}

\begin{mlem}
    The type of the Y-combinator is $\forall a ~ (\alpha \to \alpha) \to \alpha$.
    \begin{proof}
        The definition is $yf = f(yf)$.
        \begin{align}
            f &: \alpha
            \\
            r &: \alpha \to \beta
            \\
            yf &: \beta
            \\
            f &: \beta \to \gamma
            \\
            f(yf) &: \gamma
            \\
            yf : \beta, \ f(yf) : \gamma, \ yf = f(yf) &\implies \beta = \gamma
            \\
            f : \alpha, \ yf : \beta, \ f(yf) : \gamma &\implies f : \beta \to \gamma, \ \alpha = \beta \to \gamma
        \end{align}
    \end{proof}
\end{mlem}

Recursive has to do with fixpoints.

\section{Nondeterminism}

In a nondeterministic machine, $\ambc x y$
has the same effect as reducing $x$ and $y$ in parallel in one unit time.
Let the accepting computation be $z$ where $z$ is either $x$ or $y$.
\begin{align}
    \TC(\ambc xy) &= \TC z
\end{align}

\section{Differential topology and computation?}

A configuration is a point.

The transition function.

Unit step is in differential time?

What are we trying to say?
