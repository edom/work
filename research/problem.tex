\chapter{Open problems}

Can a supervised learning algorithm always be made into an unsupervised learning algorithm?

Can an approximation scheme always be made into an optimization scheme?

Semi-supervised learning?

Optimal clustering:
Given a set of points, what is the optimal clustering/partition?

Optimal approximation:
Given a set of points
\(\{(x_1,y_1),\ldots,(x_n,y_n)\}\)
(samples of a function),
what is the function that optimally approximates those samples?
The approximation error is \(\sum_k y_k - f~x_k \).
Let \(F\) be the set of all integrable real-to-real functions.
Define \(M~f = \int_{-\infty}^\infty f~x~dx\) as the infinite integral of \(f\).
Define the complexity of \(f\)
as \(C~f = \sum_{k=1}^\infty M~(D_k~f)\)
where \(D\) is the derivative operator.

Metaapproximation:
Given set of points
\(D = \{(x_1,y_1),\ldots,(x_n,y_n)\}\),
find \(g\) that finds \(f\) that approximates \(D\).

Let \(F\) be the set of all real-to-real functions.
Can we craft a measure on \(F\)?
Can we craft a probability measure on \(F\)?
Can we craft a universal prior for \(F\) like Solomonoff did for bitstrings?

What is the best way to update the approximator using the approximation error?
