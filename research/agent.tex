%\title{An agent in a discrete world}
%\author{Erik Dominikus}
%\date{2017-02-26}

\chapter{Agent models}

\newcommand\Sta{\fun{Sta}}
\newcommand\law{\fun{law}}

\section{Explicit-world mono-unary algebra}

A \emph{world} \(W\) is a mono-unary algebra \((\Sta~W, ~ \law~W)\)
where \(\Sta~W\) is the \emph{world state type}
and \(\law~W : \Sta~W \to \Sta~W\) is the \emph{world law}.

An \emph{agent} \(A\) is a mono-unary algebra \((\Sta~A, ~ \law~A)\)
where \(\Sta~A\) is the \emph{agent state type}
and \(\law~A : \Sta~A \to \Sta~A\) is the \emph{agent law}.

The \emph{environment} is the world minus the agent.
Something is a part of the agent if and only if
the agent can directly control that part.
Otherwise it is a part of the environment.

\newcommand\fpenalty{\fun{penalty}}
\newcommand\reward{\fun{reward}}
\newcommand\sense{\fun{sense}}
\newcommand\actuate{\fun{actuate}}

The function \(\sense : \Sta~W \to \Sta~A\)
defines how the agent perceives the world.
This function is not invertible
because \(\Sta~W \supset \Sta~A\).
This means that \emph{there exists part of the world that the agent cannot sense.}

An agent exists in a world.
The \(\sense\) function must be a homomorphism from the world to the agent.
The \(\sense\) function must satisfy this equation:
\begin{equation}
    \sense \circ \law~W = \law~A \circ \sense.
\end{equation}

That equation relates the actual world law
and what the law looks like from the agent's point of view.
The agent can never know the world law.
The agent can only discover something homomorphic to that law.
That means \emph{we can never know the laws of nature}.
We will never know the reality.
We can only know something homomorphic to the laws of nature.

\newcommand\orbit{\fun{orbit}}
\newcommand\Orbit{\fun{Orbit}}
\newcommand\InfSeq{\fun{InfSeq}}
\newcommand\iterate{\fun{iterate}}
\newcommand\judge{\fun{judge}}

Starting from a state \(x\), the agent forms the sequence
\( \orbit~A~x = \iterate~(\law~A)~x = (x, ~ f~x, ~ f^2~x, \ldots) \) where \(f = \law~A\).
We define such \(\orbit~A~x\) to be a member of the type \(\Orbit~A\).
We use the notation \(\InfSeq~A\) to mean
the space of infinite sequences
where each element has type \(A\).
We have \(\Orbit~A \subset \InfSeq~A\).
We consider the type \(\Orbit~A = \{ \orbit~A~x ~|~ x : \Sta~A \}\),
the set of all orbits of \(A\).
We have a judge function that judges an orbit.
This function is \(\judge : \Orbit~A \to \Real\).

Now we assume that every \(x : \Sta~A\) is distributed uniformly.
Define \(p~r\) as
the probability of finding an \(x\) where \(\judge~x \le r\).
The shape of the distribution \(p\)
describes the intelligence of the agent.

The function \(\fpenalty : \Sta~A \to \Real\)
defines the undesirability of an agent state.
Alternatively, the function \(\reward : \Sta~A \to \Real\)
defines the desirability of an agent state.
The function measures how bad or how good the agent performs.
This is the agent's hidden objective function.
This is hardwired.
This is arbitrary.
The agent doesn't have to be aware of this.
An intelligent agent acts to make its
\(\fpenalty~x\) as close to zero as possible
in the long term for as many \(x\) as possible,
where \(x\) is an agent state.

The agent displays an intelligent behavior
if it can minimize the long-term penalty from lots of starting states.
The most intelligent agent is the one that minimizes its lifelong sum of penalty?

An agent has input and output.

An \emph{agent logic} is a function of type \((M,I) \to (M,O)\)
where \(M\) is the memory type,
\(I\) is the input type,
and \(O\) is the output type.
We assume that the world remembers the agent memory.

\section{Limitations}

The agent is not omniscient.
The agent does not know everything.
The agent can only perceive a small part of the world.
The agent has physical limitations.
The agent cannot know the whole world.

\section{Implicit-world discrete dynamical system model}

Let \(w\) be a world.
Let \(a\) be an agent in world \(w\).
Let \(x~t\) be the input of the agent at time \(t\).
Let \(y~t\) be the output of the agent at time \(t\).
Let \(m~t\) be the memory of the agent at time \(t\).

We assume that the agent needs one time step to compute the output.
\begin{align}
    y~(t+1) &= Y~(x~t)~(m~t)~t
    \\
    m~(t+1) &= M~(x~t)~(m~t)~t
    \\
    x~(t+1) &= X~(x~t)~(y~t)~t
\end{align}

\section{Dynamical systems}

\section{Measuring the intelligence of a phase space trajectory}

We can think of a human as a dynamical system.

Given two phase space trajectories,
which is more intelligent?
Why?
The most intelligent is the most homeostatic, the most stabilizing, the most controlling.
